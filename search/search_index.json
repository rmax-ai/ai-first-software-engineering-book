{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AI-First Software Engineering","text":"<p>Welcome to the public documentation for the AI-first software engineering book. This site publishes the current working drafts, glossaries, patterns, and operational governance that power the book\u2019s ongoing iteration loop.</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Preface \u2013 foundational motivation and thesis for the AI-first engineering approach.</li> <li>Chapters \u2013 structured research chapters covering paradigm shifts, harness engineering, autonomous kernels, memory systems, evaluation, governance, and production infrastructure.</li> <li>Patterns \u2013 reusable engineering patterns curated from the repository.</li> <li>Glossary \u2013 operational definitions and terminology used throughout the book.</li> </ul>"},{"location":"#publishing-workflow","title":"Publishing workflow","text":"<ol> <li>Chapters are drafted in <code>book/chapters/</code> with deterministic loops enforced by <code>state/kernel.py</code>.</li> <li>The MkDocs configuration references the book files directly so every build reflects the latest chapter revisions.</li> <li>Use <code>mkdocs build</code> to produce the static site or <code>mkdocs serve</code> for a preview server before deployment.</li> <li>Keep governance documents (<code>CONSTITUTION.md</code>, <code>AGENTS.md</code>) and evaluation rules (<code>evals/</code>) aligned with the book\u2019s stated principles.</li> </ol>"},{"location":"#quick-commands","title":"Quick commands","text":"<pre><code>mkdocs build          # Generate the static site\nmkdocs serve          # Start a local preview server\nmkdocs gh-deploy      # Publish to GitHub Pages (configure remote)\n</code></pre>"},{"location":"#project-tree","title":"Project tree","text":"<pre><code>mkdocs.yml                    # Site configuration for the book\ndocs/index.md                 # Site homepage (this file)\nbook/chapters/*.md           # Chapter drafts consumed by MkDocs navigation\nbook/patterns/               # Referenced pattern library\nbook/glossary.md             # Detailed glossary definitions\nCONSTITUTION.md, AGENTS.md    # Governance that shapes the book process\nevals/*.yaml                 # Evaluation contracts enforced via state/kernel.py\n</code></pre>"},{"location":"book/glossary/","title":"Glossary","text":"<p>This glossary defines terms as used in this book. Definitions are intentionally operational: they describe what a term does in a system.</p>"},{"location":"book/glossary/#acceptance-criteria","title":"Acceptance criteria","text":"<p>Explicit, testable conditions that define \u201cdone\u201d for a task.</p> <p>In practice, acceptance criteria should map to checks (tests, lint, build, schema validation, golden diffs) or to a bounded human-review checklist.</p>"},{"location":"book/glossary/#agent-loop","title":"Agent loop","text":"<p>A control loop that repeatedly: (1) observes state, (2) chooses a next action (often a tool call), (3) executes, (4) updates state, (5) decides whether to stop.</p> <p>Typical loop variables include a step budget, an allowlist of tools, and a termination condition.</p>"},{"location":"book/glossary/#allowlist","title":"Allowlist","text":"<p>A set of explicitly permitted actions or tools.</p> <p>Allowlists reduce accidental capability expansion: adding a new tool is a deliberate change that can be reviewed, tested, and governed.</p>"},{"location":"book/glossary/#budget","title":"Budget","text":"<p>A hard limit enforced by the kernel (or orchestration layer) on steps, time, tokens, or cost.</p> <p>Budgets turn \u201ctry until it works\u201d into a bounded process.</p>"},{"location":"book/glossary/#context-window","title":"Context window","text":"<p>The maximum amount of text (messages + tool outputs + retrieved memory) that a model can condition on at once.</p> <p>Design implication: systems must choose what to include, summarize, or omit.</p>"},{"location":"book/glossary/#determinism","title":"Determinism","text":"<p>The degree to which the system produces the same outcome given the same inputs.</p> <p>In agentic systems, determinism is usually approached through constraints, typed tools, and verification rather than assumed.</p>"},{"location":"book/glossary/#drift","title":"Drift","text":"<p>Unintended change in behavior over time relative to a baseline.</p> <p>Drift can be caused by model updates, prompt edits, tool/API changes, data changes, environment changes, or accumulating memory. Drift is detected by comparing traces or eval results across versions.</p>"},{"location":"book/glossary/#evidence","title":"Evidence","text":"<p>Artifacts that support a claim about system behavior.</p> <p>Examples: a test run output, a checksum of an applied patch, a trace segment showing tool inputs/outputs, a golden diff.</p>"},{"location":"book/glossary/#eval","title":"Eval","text":"<p>A repeatable test that measures whether a system meets a target behavior under defined inputs and constraints.</p> <p>Evals can be automated (unit tests, golden files, scripted scenarios) or human-scored, but they must be versioned and runnable.</p>"},{"location":"book/glossary/#failure-mode","title":"Failure mode","text":"<p>A specific way the system can fail, including its trigger and observable symptoms.</p> <p>Failure modes drive targeted mitigations (tool contracts, guardrails, tests) rather than general caution.</p>"},{"location":"book/glossary/#golden-file","title":"Golden file","text":"<p>A stored \u201cexpected output\u201d used for regression testing.</p> <p>Golden tests are useful for CLI help text, formatted outputs, and traces; they must be reviewed carefully because updating them can mask regressions.</p>"},{"location":"book/glossary/#governance","title":"Governance","text":"<p>Rules, controls, and escalation paths that constrain and audit agent behavior.</p> <p>Governance commonly includes: tool allowlists, budgets, approval gates, logging requirements, data handling policies, incident response, and rollback procedures.</p>"},{"location":"book/glossary/#harness","title":"Harness","text":"<p>The engineered environment around a model that makes behavior reliable and useful in a specific context.</p> <p>A harness typically includes: prompts, tool schemas, routing policies, memory strategy, eval suite, tracing/telemetry, and release discipline. Most production reliability lives in the harness rather than in the model.</p>"},{"location":"book/glossary/#idempotency","title":"Idempotency","text":"<p>A property of an operation where repeating it produces the same final state as running it once.</p> <p>Idempotency matters for retries: without it, transient failures can cause duplicated side effects.</p>"},{"location":"book/glossary/#kernel","title":"Kernel","text":"<p>The minimal execution substrate that runs the agent loop and mediates interaction with the outside world.</p> <p>A kernel is responsible for: step control, tool invocation, persistence boundaries, cancellation/timeouts, and trace logging. It should be small enough to audit.</p>"},{"location":"book/glossary/#memory","title":"Memory","text":"<p>Persisted or semi-persisted state used to condition future behavior.</p> <p>Memory may be transient (within a run), session-scoped, or long-lived. It may be explicit (structured records) or implicit (retrieval index). Memory is a mechanism for reintroducing prior information into context; it is not automatically correct.</p>"},{"location":"book/glossary/#non-determinism","title":"Non-determinism","text":"<p>Variation in outcomes across runs due to sampling, tool timing, nondeterministic tests, external APIs, or changing environments.</p> <p>Managing nondeterminism is a core engineering task in agentic systems.</p>"},{"location":"book/glossary/#policy","title":"Policy","text":"<p>An explicit rule set that constrains behavior.</p> <p>Policies can be encoded in prompts, tool routers, allowlists, budgets, and approval workflows. Effective policies are testable and observable.</p>"},{"location":"book/glossary/#prompt","title":"Prompt","text":"<p>The structured input (system/developer/user messages and other context) used to condition the model.</p> <p>Prompting is part of the harness; changes to prompts should be versioned and evaluated like code.</p>"},{"location":"book/glossary/#retrieval","title":"Retrieval","text":"<p>Selecting external information (documents, traces, code snippets) to include in context for a step.</p> <p>Retrieval must be treated as a hint: critical claims still require confirmation against source-of-truth artifacts.</p>"},{"location":"book/glossary/#rag-retrieval-augmented-generation","title":"RAG (retrieval-augmented generation)","text":"<p>A pattern where the system retrieves relevant documents and includes them in the prompt before generating output.</p> <p>In engineering systems, RAG commonly retrieves code, docs, tickets, and prior traces.</p>"},{"location":"book/glossary/#side-effect","title":"Side effect","text":"<p>Any operation that changes state outside the model\u2019s messages.</p> <p>Examples: writing files, calling APIs, creating tickets, merging PRs, or modifying databases.</p>"},{"location":"book/glossary/#tool-interface","title":"Tool interface","text":"<p>The contract between the agent and an external capability.</p> <p>A tool interface specifies: inputs, outputs, errors, side effects, idempotency, latency expectations, and permission scope. Good tool interfaces make failure explicit and reduce ambiguity.</p>"},{"location":"book/glossary/#trace","title":"Trace","text":"<p>A structured record of an agent run that is sufficient to reconstruct what happened and why.</p> <p>A trace typically includes: prompts/messages, tool calls and results, intermediate decisions, budgets, timing, and final outputs. Traces are used for debugging, eval attribution, and governance.</p>"},{"location":"book/glossary/#verification","title":"Verification","text":"<p>Objective checks that the system runs (or produces as artifacts) to validate that an output meets acceptance criteria.</p> <p>Verification is stronger than self-review: it produces evidence.</p>"},{"location":"book/preface/","title":"Preface","text":"<p>This book treats AI-first software engineering as an engineering discipline rather than a product feature.</p>"},{"location":"book/preface/#scope","title":"Scope","text":"<p>This book focuses on system design for AI-assisted and agentic development:</p> <ul> <li>Harness design: tool contracts, constraints, budgets, evaluation gates, and traces.</li> <li>Operational reliability: reproducibility, attribution, rollback, and incident response.</li> <li>Governance: permissions, protected surfaces, and enforcement via tooling/CI.</li> <li>Memory as an engineered subsystem: provenance, retention, correction, and drift control.</li> </ul> <p>This book does not attempt to:</p> <ul> <li>Train foundation models or discuss model internals beyond what is necessary to reason about system behavior.</li> <li>Provide a survey of all agent frameworks; patterns are described in terms of interfaces and invariants.</li> <li>Substitute evaluation with plausibility; \u201cdone\u201d requires evidence.</li> </ul>"},{"location":"book/preface/#key-distinction-model-vs-harness","title":"Key distinction: model vs harness","text":"<ul> <li>Model: the reasoning component that proposes plans and edits.</li> <li>Harness: the execution and control environment (tools, policies, evaluation, tracing, state).</li> </ul> <p>A recurring hypothesis in the chapters is that many reliability gains in practice are harness-induced: schema design, verification discipline, and traceability change outcomes even when the model is unchanged.</p>"},{"location":"book/preface/#what-the-repository-demonstrates","title":"What the repository demonstrates","text":"<p>The repository is structured to make book development itself a reproducible agent loop:</p> <ul> <li>Governance is defined in <code>CONSTITUTION.md</code> and <code>AGENTS.md</code>.</li> <li>Chapter quality, drift signals, and style guardrails are declared in <code>evals/</code>.</li> <li>Iteration state is recorded in <code>state/</code>.</li> </ul> <p>The intention is to make each chapter a testable unit: clear thesis, system breakdown, concrete examples, trade-offs, failure modes, and research directions.</p>"},{"location":"book/preface/#how-to-read","title":"How to read","text":"<ol> <li>Start with the chapter that matches your immediate constraint (evaluation, governance, infra).</li> <li>Use <code>book/glossary.md</code> to disambiguate terms.</li> <li>Treat pattern documents in <code>book/patterns/</code> as reusable design primitives.</li> </ol>"},{"location":"book/chapters/01-paradigm-shift/","title":"Chapter 01 \u2014 Paradigm Shift","text":""},{"location":"book/chapters/01-paradigm-shift/#thesis","title":"Thesis","text":"<p>AI-first software engineering is an architectural inversion. Machine reasoning becomes a primary execution substrate. The harness\u2014tools, constraints, evaluation, and traceability\u2014becomes the primary design surface.</p> <p>This inversion is practical. Reliability comes from constraints, evaluations, and traces that turn generated changes into a repeatable loop.</p> <p>A concrete, testable implication (holding the model constant):</p> <ol> <li>A stronger harness should reduce iterations-to-pass.</li> <li>A stronger harness should reduce time-to-green.</li> <li>A stronger harness should increase attribution rate (more failures have a primary cause you can act on).</li> </ol> <p>Operational definition:</p> <ul> <li>Model capability changes when you swap models while holding tools, constraints, and evaluation constant.</li> <li>Harness capability changes when you keep the model constant but alter tools, policies, evaluation gates, or trace capture.</li> </ul> <p>In this framing, attribution rate is a harness outcome. It depends on what evidence you capture and which evaluation gates you run. It is not just a function of model fluency.</p> <p>This chapter\u2019s claim is a hypothesis: some observed \u201ccapability\u201d gains in practice are attributable to harness engineering rather than model changes.</p>"},{"location":"book/chapters/01-paradigm-shift/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Without a clear boundary between model capability and harness capability, teams misattribute failures and waste effort.</li> <li>Reliability depends on reproducible loops (plan \u2192 act \u2192 verify) rather than isolated prompts.</li> <li>Production constraints (auditability, security, cost, regression control) require system design, not \u201cprompting.\u201d</li> </ul>"},{"location":"book/chapters/01-paradigm-shift/#system-breakdown","title":"System Breakdown","text":"<ul> <li>Actors: human governor, agent loop, tools/runtime, evaluation/CI.</li> <li>Artifacts: specs, plans, diffs, traces, eval results, decision records.</li> <li>Invariants (hypotheses to test):</li> <li>Every non-trivial change is traceable to a plan and verified by checks.</li> <li>The system can attribute regressions to a layer (prompt, tool, code, eval).</li> <li>Autonomy is gated by evaluations and budgets.</li> </ul> <p>A diagram helps here because the distinction (model vs harness) changes how evidence moves. The diagram makes the handoffs explicit. It also shows where trace capture happens. Focus on two points. First: where the trace is recorded. Second: where the trace is used for attribution and the next plan.</p> <pre><code>flowchart TB\n  P[\"Plan&lt;br&gt;(spec + intent)\"]\n  A[\"Act&lt;br&gt;(propose patch)\"]\n  T[\"Tools&lt;br&gt;(apply + run)\"]\n  V[\"Verify&lt;br&gt;(evals/CI)\"]\n  D{Checks pass?}\n  S[Stop&lt;br&gt;ship/merge]\n  X[Attribute&lt;br&gt;root cause]\n  TR[\"(Trace)\"]\n\n  P --&gt; A\n  A --&gt; T\n  T --&gt; V\n  V --&gt; D\n  D -- yes --&gt; S\n  D -- no --&gt; X\n  X --&gt; P\n\n  A -. record .-&gt; TR\n  T -. record .-&gt; TR\n  V -. record .-&gt; TR\n  X -. use .-&gt; TR\n</code></pre> <p>Legend:</p> <ul> <li>Solid arrows are the operational loop (plan \u2192 act \u2192 verify).</li> <li>Dashed arrows are trace capture and trace usage.</li> </ul> <p>Takeaway: without a trace, attribution is guesswork. You will not know if the fix is spec/prompt, tool/runtime, code, or eval/CI. The same three measurable signals below should show up consistently in how you run (and later audit) the loop.</p> <ul> <li>Measurable signals (to separate model vs harness effects):</li> <li>Iterations-to-pass: number of propose\u2192verify cycles until all required checks pass.</li> <li>Time-to-green: wall-clock time from first attempt to passing evaluation gates.</li> <li>Attribution rate: fraction of failures with a clear primary cause.<ul> <li>Bucket: prompt/spec vs tool/runtime vs code vs eval.</li> </ul> </li> <li>Attribution checklist (what evidence makes a failure \u201cbelong\u201d to a layer):</li> <li>Spec/prompt:<ul> <li>Requirement is ambiguous or contradictory.</li> <li>Reasonable interpretations change expected output.</li> <li>Clarifying text resolves the failure without code changes.</li> </ul> </li> <li>Tool/runtime:<ul> <li>Tool errors, timeouts, missing permissions, or flaky environment.</li> <li>Identical reruns yield different results.</li> </ul> </li> <li>Code:<ul> <li>Deterministic failing tests or typechecks tied to a specific diff.</li> <li>Reverting the diff restores the previous behavior.</li> </ul> </li> <li>Eval/CI:<ul> <li>What is asserted does not match what is intended.</li> <li>Tests are incorrect, overly strict, or missing a required case.</li> <li>Fixing the test changes outcomes without changing product behavior.</li> </ul> </li> </ul>"},{"location":"book/chapters/01-paradigm-shift/#concrete-example-1","title":"Concrete Example 1","text":"<p>Refactor a small library function using an agent loop.</p> <ul> <li>Inputs: failing unit test + desired behavior specification (e.g., a short \u201cGiven/When/Then\u201d note checked into the repo).</li> <li> <p>Loop: propose patch \u2192 run tests \u2192 inspect diff \u2192 record trace (commands + outputs) \u2192 stop on pass.</p> </li> <li> <p>Minimal trace record (copyable):</p> </li> </ul> Field Value Spec note path <code>docs/specs/parse-date.md</code> (example) Failing test <code>tests/test_parse_date.py::test_rejects_empty</code> Commands (in order) <code>pytest -q</code> Commands (in order) <code>ruff check .</code> (example) Diff identifier commit SHA or patch ID (e.g., <code>abc1234</code>) Evaluation outputs failing test names Evaluation outputs exit codes Evaluation outputs first failing assertion (or minimal log excerpt) Attribution decision one of <code>{spec/prompt, tool/runtime, code, eval/CI}</code> Evidence 1\u20132 sentences tied to the outputs above <p>Example attribution:   - code \u2014 the same test fails deterministically after the diff; reverting the diff restores pass.</p> <ul> <li>Measured outputs:</li> <li>Iterations-to-pass.</li> <li>Time-to-green.</li> <li>Diff size (files touched, lines changed).</li> <li>Locality: changes stay within the intended function/surface area.</li> <li> <p>Attribution per iteration using the checklist above (recorded in the trace).</p> </li> <li> <p>Stop rule:</p> </li> <li>Stop when the original failing test passes and the full unit test suite passes.</li> <li>Also require the diff to stay constrained to the intended surface area.</li> <li>If you hit a fixed budget (N iterations or T minutes), stop and hand off:<ul> <li>the trace record</li> <li>the smallest reproducible failing case</li> </ul> </li> </ul>"},{"location":"book/chapters/01-paradigm-shift/#concrete-example-2","title":"Concrete Example 2","text":"<p>Ship a minor API change in a production service.</p> <ul> <li>Inputs: API contract + backward-compat constraints + staging environment + a defined rollout/rollback policy.</li> <li> <p>Loop: generate migration plan \u2192 implement \u2192 run contract tests \u2192 produce trace report (diff + commands + results) \u2192 human approve.</p> </li> <li> <p>Minimal trace report (copyable):</p> </li> </ul> Field Value Contract/version <code>openapi.yaml</code> (example) Compatibility window \u201ccompatible within v1.x\u201d Backward-compat constraints \u201cno required fields added\u201d Backward-compat constraints \u201cno behavior change on existing endpoints\u201d Staging target <code>staging-us-east-1</code> (example) Commands (in order) <code>make contract-test</code> Commands (in order) <code>npm run lint</code> Commands (in order) <code>npm run typecheck</code> Commands (in order) <code>./scripts/staging-smoke.sh</code> Diff identifier PR number + commit SHA (e.g., <code>PR #482</code>, <code>def5678</code>) Evaluation outputs failing checks Evaluation outputs log paths/links Evaluation outputs timestamps (supports time-to-green) Attribution decisions per failure: <code>{spec/prompt, tool/runtime, code, eval/CI}</code> + evidence <p>Example attribution:   - eval/CI \u2014 contract test rejects an allowed optional field; fixing the assertion changes outcomes without changing API behavior.</p> <ul> <li>Measured outputs:</li> <li>Iterations-to-pass and time-to-green (from first migration-plan draft to all required checks passing in staging).</li> <li>Attribution rate per iteration using the checklist above (spec/prompt vs tool/runtime vs code vs eval).</li> <li> <p>Backward-compat outcomes:</p> <ul> <li>contract-test failures introduced (required gate: 0 new failures in required checks)</li> <li>rollback verification in staging (required gate: exercise rollback successfully at least once)</li> <li>time-to-green (default target: \u2264 30 minutes from first implementation attempt to all required checks passing in staging; set per service)</li> </ul> </li> <li> <p>Guardrails:</p> </li> <li>Protected paths or modules that require explicit human review before edits (e.g., auth, billing, infra).</li> <li>Required checks (contract tests, integration tests, lint/typecheck, and a staging smoke test).</li> <li>Rollback plan defined up front (feature flag, config switch, or revert procedure) and verified in staging.</li> <li>Approval gate: no deploy until a human reviews the migration plan, the diff, and the evaluation results.</li> <li> <p>Mapping: Guardrails define what must be protected, required checks define what must be proven, and the approval gate defines who must accept the evidence before deploy.</p> </li> <li> <p>Stop rule:</p> </li> <li>Stop when all required checks pass in staging and the migration plan matches backward-compat constraints.</li> <li>Require that the trace report can explain every material change.</li> <li>If you hit a fixed budget (N iterations or T minutes), pause the rollout and escalate with:<ul> <li>the trace report</li> <li>the smallest reproducible failing case</li> </ul> </li> <li>If any guardrail is violated (protected file touched, required test skipped, rollback unclear), stop immediately and require human intervention.</li> </ul>"},{"location":"book/chapters/01-paradigm-shift/#trade-offs","title":"Trade-offs","text":"<ul> <li>Strong harness constraints reduce freedom (and sometimes speed) but increase reproducibility.</li> <li>More evaluation gates reduce regressions but add compute and latency.</li> <li>Trace-heavy workflows improve debugging but increase storage and privacy considerations.</li> </ul>"},{"location":"book/chapters/01-paradigm-shift/#failure-modes","title":"Failure Modes","text":"<ul> <li>Illusion of capability: improvements credited to the model when they come from better tooling/evals.</li> <li>Unbounded autonomy: loops run without budgets, causing tool thrash and unclear outcomes.</li> <li>Non-attributable failures: missing traces make regressions un-debuggable.</li> </ul> <p>Synthesis: treat machine reasoning as an execution substrate, and treat the harness as the primary lever for reliability. Track iterations-to-pass, time-to-green, and attribution rate to separate harness effects from model effects and to make failures actionable.</p>"},{"location":"book/chapters/01-paradigm-shift/#research-directions","title":"Research Directions","text":"<ul> <li>Metrics that separate model improvements from harness improvements.</li> <li>Minimal trace schema that supports attribution and replay.</li> <li>Formal definitions of autonomy envelopes and stop conditions.</li> </ul>"},{"location":"book/chapters/02-harness-engineering/","title":"Chapter 02 \u2014 Harness Engineering","text":""},{"location":"book/chapters/02-harness-engineering/#thesis","title":"Thesis","text":"<p>Harness engineering is the discipline of turning a general-purpose model into a predictable system. It does that by defining tool contracts, constraints, loop control, and evaluation gates.</p> <p>In this chapter, \u201cpredictable\u201d means three things:</p> <ol> <li>Repeatability: the same input context tends to produce the same classes of actions.</li> <li>Boundedness: the system\u2019s allowed changes are limited by explicit constraints.</li> <li>Verifiability: outputs can be accepted or rejected by checks, not judgment calls.</li> </ol> <p>Hypothesis (falsifiable): for a fixed set of tasks and repositories, tightening the harness reduces regression rate and rework more than swapping between models of similar capability.</p> <p>This is not a model-selection argument. The claim is that, when you hold the model and task conditions constant, harness design is the primary lever that changes reliability outcomes.</p> <p>To test that hypothesis, hold these constant:</p> <ul> <li>The task set (same prompts and acceptance criteria).</li> <li>The repositories and their starting commits.</li> <li>The agent instructions and tool availability (only harness strictness changes).</li> </ul>"},{"location":"book/chapters/02-harness-engineering/#why-this-matters","title":"Why This Matters","text":"<ul> <li>The same model can behave reliably or unreliably depending on tool schemas, budgets, and verification.</li> <li>Teams can standardize harness practices even when models change.</li> <li>Production safety and auditability primarily live in the harness layer.</li> </ul> <p>If you want evidence for the hypothesis, your experiments should explicitly keep the model constant (or compare models in the same capability band) and vary only harness strictness. Otherwise, it becomes difficult to attribute changes in regression rate and rework to harness design versus model behavior.</p>"},{"location":"book/chapters/02-harness-engineering/#system-breakdown","title":"System Breakdown","text":"<ul> <li>Control plane: prompts, policies, budgets, stop conditions.</li> <li>Tool plane: filesystem edits, build/test runners, linters, browsers, APIs.</li> <li>Evaluation plane: checks as gates; regression suites; quality rubrics.</li> <li>State plane: task ledger, traces, decisions, artifacts.</li> <li>Interfaces:</li> <li>Tool schemas and error contracts.</li> <li>Patch discipline (diff-only, small changes).</li> <li>Evaluation API (what constitutes pass/fail).</li> </ul> <p>A diagram helps here because the planes are easy to list but hard to reason about as a system. Focus on where constraints enter (control plane), where evidence is produced (tool/evaluation planes), and what gets recorded for replay (state plane).</p> <pre><code>flowchart TB\n  TB[Task brief&lt;br/&gt;+ repo state]\n\n  C[Control plane&lt;br/&gt;policies&lt;br/&gt;budgets&lt;br/&gt;stop conditions]\n  T[Tool plane&lt;br/&gt;patches&lt;br/&gt;commands&lt;br/&gt;structured errors]\n  E[Evaluation plane&lt;br/&gt;tests&lt;br/&gt;lint/typecheck&lt;br/&gt;build&lt;br/&gt;rubrics]\n  S[State plane&lt;br/&gt;ledger&lt;br/&gt;trace&lt;br/&gt;artifacts]\n\n  TB --&gt; C\n  C --&gt;|typed tool calls| T\n  T --&gt;|check inputs| E\n  E --&gt;|pass/fail + evidence| C\n  T --&gt;|events + diffs + outputs| S\n  E --&gt;|gate results| S\n</code></pre> <p>Read the diagram as a loop with three distinct roles:</p> <ul> <li>Constraints enter in the control plane (policies, budgets, stop conditions).</li> <li>Evidence is produced by the tool and evaluation planes (outputs + pass/fail).</li> <li>Replay depends on the state plane capturing enough artifacts to reproduce decisions.</li> </ul> <p>Takeaway: predictability comes from making the loop explicit. The control plane must be able to stop based on evaluation, and the state plane must capture enough evidence to reproduce the decision.</p> <p>A useful way to operationalize the planes is to name what each plane consumes, what it produces, and one metric you can track:</p> Plane Responsibilities Key artifacts (inputs/outputs) One measurable metric Control plane Decide what the agent is allowed to do and when to stop Inputs: task brief; policy; iteration/time budget Outputs: chosen strategy; stop reason Iterations to first passing gate Tool plane Perform actions with bounded, typed interfaces Inputs: validated tool calls Outputs: patches; command outputs; structured errors Patch locality (files/lines per task) Evaluation plane Decide if work is acceptable based on checks Inputs: test/lint/typecheck/build results; rubrics Outputs: pass/fail + evidence Gate pass rate (per iteration) State plane Record what happened and enable recovery Inputs: events; diffs; tool outputs Outputs: trace; ledger; artifacts for replay Reproducibility rate (same pass/fail on rerun) <p>The \u201cminimal contract surface\u201d is the set of interfaces that must be stable for predictability. It includes tool schemas (including error codes), patch discipline, and evaluation semantics.</p> <p>Boundary:</p> <ul> <li>In scope: tool schemas, patch discipline, and gate semantics (including what evidence is recorded).</li> <li>Out of scope: standardizing runtime infrastructure details, as long as the gate semantics stay consistent.</li> </ul>"},{"location":"book/chapters/02-harness-engineering/#concrete-example-1","title":"Concrete Example 1","text":"<p>Design a tool contract for \u201capply patch\u201d operations.</p> <p>A minimal schema sketch can be made scannable by treating it like an interface spec. One goal is to make failures recoverable without expanding scope.</p> <p>Schema (fields + constraints to keep edits local and reviewable):</p> <ul> <li>Required fields: <code>path</code>, <code>old_str</code>, <code>new_str</code>.</li> <li>Optional field: <code>context</code> (only for disambiguation).</li> <li>Constraints:</li> <li>No unrelated whitespace changes outside <code>old_str</code>.</li> <li>No implicit multi-file edits per call.</li> <li>Size budget (for example: max files changed; max lines changed).</li> </ul> Category Field / rule Purpose Example failure mode Required fields <code>path</code> Identify the target file Wrong path \u2192 cannot apply patch Required fields <code>old_str</code> Provide an exact, unique match anchor Stale context \u2192 no match Required fields <code>new_str</code> Provide the replacement text N/A (validated as string) Optional fields <code>context</code> Disambiguate or narrow a match Without it, match may be non-unique Constraints No unrelated whitespace changes outside <code>old_str</code> Preserve locality and reviewability \u201cFormatting spill\u201d across file Constraints No implicit multi-file edits Bound scope per call Patch tool touches multiple files Constraints Size budget (e.g., max changed lines per call) Prevent runaway diffs Large diff for small task <p>Error contract (examples):</p> Error code Meaning What the harness should return <code>NOT_FOUND</code> <code>path</code> does not exist and <code>create</code> is false A clear message plus allowed path roots (if any) <code>NON_UNIQUE_MATCH</code> <code>old_str</code> matches multiple locations Candidate ranges or small snippets for each match <code>NO_MATCH</code> <code>old_str</code> matches zero locations A \u201cfresh context\u201d snippet around the closest match <code>BUDGET_EXCEEDED</code> Change size violates configured limits The computed line/file counts and configured limits <code>POLICY_VIOLATION</code> Edit touches forbidden paths or patterns The specific policy rule that triggered <p>Happy path flow:</p> <ol> <li>Agent proposes a patch using a unique <code>old_str</code> block with minimal scope.</li> <li>Harness validates: file exists, match is unique, diff stays within budgets.</li> <li>Tool applies the patch and returns a structured result: changed line counts, a before/after snippet, and a stable identifier for the diff.</li> </ol> <p>Conflict recovery (when the patch fails with <code>NO_MATCH</code> or <code>NON_UNIQUE_MATCH</code>):</p> <ol> <li>The harness returns the error code plus a short \u201cfresh context\u201d snippet around the closest match (or a list of candidate match ranges).</li> <li>The agent re-reads the relevant portion of the file and regenerates a smaller, more specific <code>old_str</code> (or narrows by adding <code>context</code>).</li> <li>If the second attempt fails, the harness forces a stop condition (\u201cneeds human review\u201d) rather than allowing a whole-file overwrite.</li> </ol> <p>Mini-example (connecting the contract to measurable outcomes):</p> <ul> <li>Task: \u201cUpdate <code>timeout_ms</code> default from 5000 to 8000.\u201d</li> <li>Without a strict contract: the agent might run a broad search/replace and touch 6 files, including docs and unrelated constants.</li> <li>With the contract and budgets:</li> <li>The edit is constrained to one file and a small <code>old_str</code> anchor.</li> <li>The harness rejects the attempt if it exceeds, for example, 1 file and 20 changed lines.</li> <li>Your tracked metrics should shift: diff locality stays low (1 file, a few lines), and revert rate should drop because the change is less likely to introduce incidental regressions.</li> </ul> <p>Evaluation: measure whether the tool contract is improving outcomes with two task-level metrics:</p> <ul> <li>Revert rate: fraction of tasks where the patch is reverted in the next N commits/PR updates.</li> <li>Diff locality: median number of files touched and lines changed per task, with an alert threshold for outliers.</li> </ul>"},{"location":"book/chapters/02-harness-engineering/#concrete-example-2","title":"Concrete Example 2","text":"<p>Add an evaluation gate to an agent loop.</p> <p>A diagram helps here because \u201cattempt \u2192 gate \u2192 log \u2192 decide\u201d is simple to say, but easy to implement incorrectly. Focus on the two decision points: whether the gate passed, and whether another iteration is allowed under budgets.</p> <pre><code>flowchart TB\n  A[Attempt&lt;br/&gt;smallest plausible change]\n  G[Gate&lt;br/&gt;tests + static checks]\n  P{Gate passes?}\n  L[Log&lt;br/&gt;diff id + outputs + exit codes]\n  B{Budget remains&lt;br/&gt;and failure actionable?}\n  R[PR-ready output]\n  S[Stop&lt;br/&gt;failure summary + evidence]\n\n  A --&gt; G --&gt; P\n  P -- yes --&gt; L --&gt; R\n  P -- no --&gt; L --&gt; B\n  B -- yes --&gt; A\n  B -- no --&gt; S\n</code></pre> <p>Takeaway: the harness should only permit another attempt when (a) budgets remain and (b) the failure is actionable. Otherwise, it should stop with evidence, not a guess.</p> <p>A minimal loop in prose looks like: attempt \u2192 gate \u2192 log \u2192 decide.</p> <ol> <li>Attempt: the agent makes the smallest change it believes will satisfy the task.</li> <li>Gate: the harness runs required checks (e.g., unit tests + static checks).</li> <li>Log: record the tool outputs, the diff identifier, and the gate result in the task ledger.</li> <li>Decide:</li> <li>If gate passes: allow \u201cPR-ready\u201d output.</li> <li>If gate fails: allow another iteration only if budgets remain and the failure is actionable.</li> <li>If repeated failures occur: stop with a concrete failure summary and evidence.</li> </ol> <p>Gate configuration checklist (minimal defaults):</p> <ol> <li>Define the full gate: a fixed set of commands, in order (for example: unit tests, then lint/typecheck, then build).</li> <li>Define time and iteration budgets: max gate time per iteration and max iterations per task.</li> <li>Define evidence requirements: always store exit codes, stdout/stderr (or paths to logs), and the diff identifier.</li> <li>Define fallback eligibility: when tests are missing or non-runnable, use a narrower fallback gate and tighten patch budgets.</li> </ol> <p>Pass/fail criteria should be explicit:</p> <ul> <li>Pass = all configured checks return success (exit code 0) and no policy violations were triggered.</li> <li>Fail = any check fails, any policy violation occurs, or budgets are exceeded.</li> </ul> <p>Fallback gate policy when tests are missing:</p> <ul> <li>If unit tests are absent or non-runnable, the harness should not silently accept \u201clooks good.\u201d</li> <li> <p>Use a compact decision rule:</p> </li> <li> <p>If the repo\u2019s documented test command exists and runs in this environment, use <code>full_gate</code>.</p> </li> <li>If tests cannot be invoked here (missing command, missing dependency, or failures that prevent tests from running), try to fix within budget.</li> <li>If you cannot make tests runnable within budget, switch to <code>fallback_gate</code> and tighten patch budgets.</li> <li> <p>Record which gate was used (<code>full_gate</code> vs <code>fallback_gate</code>) so metrics remain comparable.</p> </li> <li> <p>A minimal <code>fallback_gate</code> is narrower and stricter:</p> </li> <li>typecheck/lint/build must pass (whatever subset is available), and</li> <li>a targeted command or script (documented in the repo) must run successfully, and</li> <li>the change must be limited by stricter patch budgets (smaller allowed diffs).</li> </ul> <p>Mini-example (gate policy with metrics):</p> <ul> <li>Task: \u201cRename <code>UserID</code> to <code>user_id</code> in a Python module.\u201d</li> <li>Budgets: max 2 iterations; max 10 changed lines per iteration.</li> <li>Repo state: a documented test command exists, but it fails immediately due to a missing system dependency.</li> <li>Gate choice: use <code>fallback_gate</code> plus stricter patch budgets.</li> <li>Iteration 1:</li> <li>Attempt: change one file and update one reference.</li> <li>Gate: lint/typecheck passes; targeted script fails with an import error.</li> <li>Iteration 2:</li> <li>Attempt: add the missing import in the same file.</li> <li>Gate: lint/typecheck passes; targeted script passes.</li> <li>Recorded metric: iterations to first passing gate = 2.</li> </ul>"},{"location":"book/chapters/02-harness-engineering/#trade-offs","title":"Trade-offs","text":"<ul> <li>Richer tool schemas reduce ambiguity but raise integration cost.</li> <li>Strict budgets prevent runaway loops but can truncate legitimate work.</li> <li>Strong gates improve safety but may block progress on tasks lacking tests.</li> </ul> <p>Decision checklist (recommended defaults and when to relax):</p> <ul> <li>Tool schemas:</li> <li>Default: prefer typed, validated arguments plus explicit error codes for recovery.</li> <li>Relax when: prototyping a new tool where integration speed matters more than repeatability, but only in non-production contexts.</li> <li>Patch budgets:</li> <li>Default: small, diff-only edits with per-call and per-task size limits.</li> <li>Relax when: performing mechanical, reviewable migrations (e.g., dependency rename) where change breadth is intentional and measurable.</li> <li>Evaluation gates:</li> <li>Default: require passing tests and static checks (a minimal green CI run) before any final output.</li> <li>Relax when: the repo cannot run tests in the current environment; use an explicit fallback gate and tighten patch budgets rather than skipping evaluation.</li> <li>Stop conditions:</li> <li>Default: stop on repeated failure modes (same check failing twice) with a structured report.</li> <li>Relax when: failures are due to flaky infrastructure and you can rerun deterministically (record rerun count as part of the trace).</li> </ul>"},{"location":"book/chapters/02-harness-engineering/#failure-modes","title":"Failure Modes","text":"<ul> <li>Schema underspecification: tools accept ambiguous inputs, producing inconsistent outcomes.</li> <li>How you notice: frequent <code>NO_MATCH</code>/<code>NON_UNIQUE_MATCH</code>-style failures, large diffs for small tasks, and high variance in outcomes across reruns.</li> <li>Harness fix: tighten required fields, add validation (uniqueness checks, size budgets), and return structured error codes plus fresh context to guide recovery.</li> <li>Over-permissive harness: agents change broad parts of the repo with weak verification.</li> <li>How you notice: many files touched per task, drift into unrelated directories, and regressions discovered after \u201ccompletion.\u201d</li> <li>Harness fix: enforce patch locality budgets, add path allowlists/denylists, and require passing gates before accepting final output.</li> <li>Gate bypass: humans accept outputs without running checks, breaking the feedback loop.</li> <li>How you notice: \u201cmerged without green checks,\u201d missing logs/evidence in the ledger, and recurring regressions that gates would have caught.</li> <li>Harness fix: make gates non-optional for PR-ready output (policy), require artifacts (logs, diff id, check results) attached to the task, and surface \u201cstop reason\u201d when evidence is missing.</li> </ul>"},{"location":"book/chapters/02-harness-engineering/#research-directions","title":"Research Directions","text":"<ul> <li>Harness quality metrics (iteration efficiency, regression rate, reproducibility).</li> <li>Open question: which small set of metrics best predicts long-term reliability across repos?</li> <li>Evaluation approach: track iterations to first passing gate, revert rate, and reproducibility rate using the plane metrics table, then compare distributions before/after harness changes.</li> <li>Hypothesis link: if harness tightening drives reliability more than model swaps, these distributions should improve (fewer iterations, fewer reverts, higher reproducibility) even when the model is held constant.</li> <li>Tool error taxonomies that guide automated recovery.</li> <li>Open question: what error codes enable the highest \u201cself-repair\u201d rate without encouraging risky retries?</li> <li>Evaluation approach: measure recovery success rate per error code (e.g., <code>% resolved within 2 retries</code>) and the resulting diff locality, using structured tool returns.</li> <li>Hypothesis link: a tighter harness should shift failures from \u201csilent bad edits\u201d toward typed, recoverable errors, increasing recovery success without increasing diff locality.</li> <li>Portable harness templates across languages and repo types.</li> <li>Open question: what parts of the contract surface are truly portable (schemas, budgets, gates) versus language-specific?</li> <li>Evaluation approach: apply a template to multiple repos, then compare reproducibility rate and gate pass rate changes while holding model choice constant.</li> <li>Hypothesis link: if harness design is the primary lever, a portable template should produce consistent improvements across repos without requiring a different model.</li> </ul>"},{"location":"book/chapters/03-autonomous-kernels/","title":"Chapter 03 \u2014 Autonomous Kernels","text":""},{"location":"book/chapters/03-autonomous-kernels/#thesis","title":"Thesis","text":"<p>An autonomous kernel is a minimal, well-specified control loop for bounded work: plan, apply tool actions, verify, and stop. It is designed for short-horizon execution with explicit budgets and explicit exit criteria, so its behavior is inspectable and repeatable. It is not a substitute for long-horizon project management or open-ended exploration.</p> <p>What this is (and is not):</p> <ul> <li>A kernel executes one bounded objective and stops; it does not \u201ckeep going\u201d to find adjacent work.</li> <li>A kernel operates inside a declared safety envelope (budgets + permissions + evaluation gates); it does not expand scope implicitly. Here, \u201csafety envelope\u201d means the explicit combination of budgets, permissions, and evaluation gates. It bounds what actions are allowed and what evidence is required to declare success.</li> <li>A kernel treats verification as mandatory (or explicitly waived with recorded justification); it does not declare success from intent or plausibility.</li> </ul> <p>Definitions:</p> <ul> <li>Autonomous kernel: a control loop with explicit limits and explicit exit criteria; it is not \u201cgeneral autonomy,\u201d long-horizon project management, or open-ended exploration.</li> <li>Budget: a hard cap on resources (iterations, elapsed time, tool calls, diff size) that prevents runaway behavior and forces escalation when progress stalls.</li> <li>Evaluation gate: a required check whose result must be recorded and must be satisfied (or explicitly waived with justification) before the kernel can declare success.</li> </ul> <p>Hypothesis: small, well-governed autonomous kernels (tight loops with explicit budgets and evaluation gates) outperform broad autonomy in stability and debuggability.</p>"},{"location":"book/chapters/03-autonomous-kernels/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Most failures in agentic work are operational: runaway loops, untraceable edits, and unverifiable outcomes.</li> <li>Kernels enable composability: multiple kernels can run with different permissions and evaluation profiles.</li> <li>\u201cKernel-first\u201d design makes autonomy a system property, not a prompt trick.</li> </ul>"},{"location":"book/chapters/03-autonomous-kernels/#system-breakdown","title":"System Breakdown","text":"<ul> <li>Kernel loop: intent \u2192 plan \u2192 act \u2192 verify \u2192 record trace \u2192 stop/iterate.</li> <li>Budgets: max iterations, time, tool calls, diff size.</li> <li>Permissions: read/write scopes, protected paths, allowed tools.</li> <li>Verification: mandatory checks per action class (e.g., tests for code changes).</li> <li>Persistence: ledger entries, trace logs, artifacts.</li> </ul> <p>The loop stages are the execution path. Budgets, permissions, and evaluation gates are the controls that constrain what can happen at each stage and what evidence is required to stop.</p> <p>Treat each loop step as a checkpoint with two requirements: (1) what must be recorded, and (2) what stop condition must be evaluated.</p> <ol> <li>Intent: state the task class and the success condition.</li> <li>Must record: intent string.</li> <li>Must record: success criteria (e.g., \u201crepro no longer fails\u201d).</li> <li> <p>Stop condition: if success criteria are ambiguous, stop and request clarification.</p> </li> <li> <p>Plan: enumerate the next 1\u20133 actions only, each tied to a gate and a budget slice.</p> </li> <li>Must record: planned steps.</li> <li>Must record: named gates.</li> <li>Must record: allocated budget slice.</li> <li> <p>Stop condition: if the plan requires tools/paths outside permissions, stop and request escalation.</p> </li> <li> <p>Act: perform the minimal change that tests the current hypothesis.</p> </li> <li>Must record: files touched.</li> <li>Must record: diff stats (and any migration steps taken).</li> <li> <p>Stop condition: if diff budget or file-touch budget is exceeded, stop and escalate.</p> </li> <li> <p>Verify: run the smallest evaluation that is credible for the task class.</p> </li> <li>Must record: exact command(s).</li> <li>Must record: exit codes.</li> <li>Must record: summary lines (or pointers/hashes to full logs).</li> <li>Stop condition: if a gate fails, iterate.</li> <li>Stop condition: if no credible gate exists, stop and escalate.</li> <li>Gate waivers are acceptable only when the trace records:<ul> <li>why the gate is not runnable</li> <li>what alternative check was run</li> <li>what risk remains</li> <li>the waiver decision and its rationale</li> </ul> </li> <li> <p>A waiver is not a \u201cpass\u201d; it is a recorded exception.</p> </li> <li> <p>Record trace: persist replayable evidence for what happened and why.</p> </li> <li>Must record: commands executed.</li> <li>Must record: outputs (or pointers).</li> <li>Must record: budgets consumed.</li> <li>Must record: permissions exercised.</li> <li> <p>Stop condition: if persistence fails (cannot write trace), stop; do not continue \u201cblind.\u201d</p> </li> <li> <p>Stop/iterate: decide based on evidence and remaining budget.</p> </li> <li>Must record: decision.</li> <li>Must record: rationale.</li> <li>Must record: the next action (either \u201cdone\u201d or \u201cwhat to try next\u201d).</li> <li>Stop condition: stop on success.</li> <li>Stop condition: stop when a budget is exhausted.</li> <li>Stop condition: stop when permissions/scope are insufficient.</li> </ol> <p>A diagram is useful here because the kernel has a fixed stage order, while budgets, permissions, and gates constrain stages from the side. Read solid arrows as the stage sequence. Read dotted arrows as constraints and required checks. Takeaway: \u201cdone\u201d is a verification outcome, and trace artifacts are explicit outputs.</p> <p>Mermaid mapping of stages to controls and outputs.</p> <pre><code>flowchart LR\n  I[Intent] --&gt; P[Plan] --&gt; A[Act] --&gt; V[Verify] --&gt; R[Record trace] --&gt; S{Stop / iterate}\n\n  B[(Budgets\\niterations/time/tool calls/diff size)] -. constrains .-&gt; P\n  B -. constrains .-&gt; A\n  B -. constrains .-&gt; V\n  Perm[(Permissions\\nread/write scopes\\nprotected paths\\nallowed tools)] -. constrains .-&gt; A\n  Gate[(Evaluation gates\\nby action class)] -. required .-&gt; V\n  Persist[(Persistence\\nledger/trace logs/artifacts)] -. produced .-&gt; R\n\n  V --&gt;|pass| S\n  V --&gt;|fail| P\n  S --&gt;|iterate| P\n  S --&gt;|stop| End[Exit with summary]\n</code></pre> <p>How to read this diagram:</p> <ul> <li>Solid arrows show execution order; dotted arrows show constraints and required checks.</li> <li>Verify is where gates produce pass/fail evidence that drives stop vs iterate.</li> <li>Record trace produces the artifacts you need to replay what happened.</li> </ul> <p>A compact \u201cmust capture\u201d checklist (minimum viable trace).</p> <p>This checklist is the smallest set of fields that enables replay, audit, and debugging without relying on memory or \u201cit seemed fine.\u201d</p> Loop stage Budget signal Permission signal Verification signal Persistence artifact intent remaining iterations/time required read scope success criteria defined intent string + criteria plan tool-call budget allocation allowed tools list planned gates named plan steps + gate mapping act diff size consumed write scope used N/A patch/diff stats verify time/tool calls consumed execution permissions gate results (pass/fail) command + exit code + excerpt record trace N/A N/A N/A ledger entry + trace pointer stop/iterate budget exhausted? permission insufficient? gates satisfied? final summary + next action"},{"location":"book/chapters/03-autonomous-kernels/#concrete-example-1","title":"Concrete Example 1","text":"<p>Bug-fix kernel for a CLI tool.</p> <ul> <li>Input:</li> <li>failing test case: <code>tests/test_parse.py::test_rejects_empty_input</code></li> <li>repro command: <code>python -m mycli parse \"\"</code></li> <li>observed: exit code <code>0</code></li> <li>expected: non-zero exit code</li> <li>budgets:<ul> <li>max 3 iterations</li> <li>max 10 tool calls</li> <li>max 40 lines changed</li> </ul> </li> <li>permissions:<ul> <li>read: <code>src/</code></li> <li>write: <code>src/parser.py</code></li> <li>run: <code>pytest -k parse</code></li> </ul> </li> </ul> <p>Mini-runbook (a single bounded kernel run):</p> <ol> <li>Localize failure (evidence-first)</li> <li>Action: reproduce with the smallest credible check.<ul> <li>Command: <code>pytest -k rejects_empty_input</code></li> </ul> </li> <li>Gate: the reproduction must fail in a controlled way.</li> <li>Gate: the failure should match the same assertion and code path.</li> <li>Record:<ul> <li>command: <code>pytest -k rejects_empty_input</code></li> <li>exit code: <code>1</code></li> <li>failing excerpt (example):</li> <li><code>&gt;       assert parse(\"\") != 0</code></li> <li><code>E       AssertionError: assert 0 != 0</code></li> <li>test summary line (example): <code>1 failed, 42 deselected in 0.31s</code></li> </ul> </li> <li> <p>Stop rule: if the failure does not reproduce, stop and return a \u201ccannot reproduce\u201d trace (no edits).</p> </li> <li> <p>Patch minimal surface (hypothesis-driven)</p> </li> <li>Action: implement the smallest boundary check consistent with the hypothesis.</li> <li>Hypothesis: empty string is being treated as a valid token stream in <code>src/parser.py</code>.</li> <li>Change: reject empty input at the parse entrypoint.</li> <li>Change: avoid touching downstream call sites.</li> <li>Gate: stay within 40 changed lines.</li> <li>Gate: touch only <code>src/parser.py</code> (unless escalation is explicitly allowed).</li> <li>Record:<ul> <li>files touched: <code>src/parser.py</code></li> <li>diff stats (example): <code>src/parser.py | 7 ++++++-</code></li> </ul> </li> <li> <p>Stop rule: if a second file is required, stop and escalate (\u201cwrite scope too narrow\u201d).</p> </li> <li> <p>Run verification gate (tight but credible)</p> </li> <li>Action: rerun the failing test.<ul> <li>Command: <code>pytest -k rejects_empty_input</code></li> <li>Expected summary (example): <code>1 passed, 42 deselected in 0.28s</code></li> </ul> </li> <li>Action: run a small related slice.<ul> <li>Command: <code>pytest -k parse</code></li> <li>Expected summary (example): <code>12 passed, 0 failed, 31 deselected in 1.07s</code></li> </ul> </li> <li>Record:<ul> <li>command 1: <code>pytest -k rejects_empty_input</code></li> <li>exit code 1: <code>0</code></li> <li>summary 1 (example): <code>1 passed, 42 deselected in 0.28s</code></li> <li>command 2: <code>pytest -k parse</code></li> <li>exit code 2: <code>0</code></li> <li>summary 2 (example): <code>12 passed, 0 failed, 31 deselected in 1.07s</code></li> </ul> </li> <li> <p>Stop rule: if Gate 1 passes but Gate 2 fails, treat as \u201cnot fixed\u201d and iterate (the patch likely broke a nearby invariant).</p> </li> <li> <p>Record trace (auditable, replayable)</p> </li> <li>Action: persist a kernel trace and a ledger entry.</li> <li>Record (minimum):<ul> <li>budgets consumed: <code>iterations=1/3</code></li> <li>budgets consumed: <code>tool_calls=3/10</code></li> <li>budgets consumed: <code>diff_lines=7/40</code></li> <li>commands executed + exit codes</li> <li>final test summary lines:</li> <li><code>12 passed, 0 failed, 31 deselected in 1.07s</code></li> </ul> </li> <li> <p>Stop rule: if trace persistence fails, stop; do not continue with further edits.</p> </li> <li> <p>Stop criteria (explicit)</p> </li> <li>Stop success: Gate 1 and Gate 2 pass within budget.</li> <li>Stop failure: tool-call budget exhausted.</li> <li>Stop failure: diff budget exceeded.</li> <li>Stop failure: verification indicates a broader refactor is required.</li> <li>Stop escalation output: include the next action for a human.<ul> <li>Example: \u201ctokenization treats whitespace-only as empty.\u201d</li> <li>Example: \u201crequires editing <code>src/lexer.py</code>, outside current write scope.\u201d</li> </ul> </li> </ol> <p>This stays stable and debuggable because the kernel cannot declare success without passing named gates, and each iteration is bounded by explicit budgets.</p>"},{"location":"book/chapters/03-autonomous-kernels/#concrete-example-2","title":"Concrete Example 2","text":"<p>Dependency upgrade kernel.</p> <ul> <li>Input:</li> <li>target version: <code>libX 4.2.0 \u2192 4.3.0</code></li> <li>constraints:<ul> <li>Python <code>&gt;=3.10</code></li> <li>cannot change public API</li> <li>CI must stay green</li> </ul> </li> <li>upgrade guide note:<ul> <li>breaking rename <code>OldClient</code> \u2192 <code>Client</code></li> </ul> </li> <li>budgets:<ul> <li>max 4 iterations</li> <li>max 15 tool calls</li> <li>max 120 lines changed</li> </ul> </li> <li>permissions:<ul> <li>write: <code>pyproject.toml</code></li> <li>write: <code>src/</code></li> <li>run: <code>python -m compileall</code></li> <li>run: <code>pytest</code></li> </ul> </li> </ul> <p>Kernel steps with an explicit remediation branch:</p> <ol> <li>Update manifest (narrow scope)</li> <li>Action: bump version constraint in <code>pyproject.toml</code>.</li> <li>Record:<ul> <li>old constraint (example): <code>libX&gt;=4.2,&lt;4.3</code></li> <li>new constraint (example): <code>libX&gt;=4.3,&lt;4.4</code></li> <li>diff stats for manifest only (example): <code>pyproject.toml | 1 +-</code></li> </ul> </li> <li> <p>Stop/iterate rule:</p> <ul> <li>If the dependency resolver cannot produce a consistent lock, stop with resolver output.</li> <li>Do not attempt ad-hoc pinning unless that is explicitly in scope.</li> </ul> </li> <li> <p>Run a fast build/type gate before full tests</p> </li> <li>Gate A (fast): import/type/compile smoke check.<ul> <li>Command: <code>python -m compileall src</code></li> </ul> </li> <li>Record:<ul> <li>exit code</li> <li>compile summary lines (example):</li> <li><code>Listing 'src'...</code></li> <li><code>Compiling 'src/app.py'...</code></li> <li><code>compileall: success</code></li> </ul> </li> <li> <p>Interpretation:</p> <ul> <li>If Gate A fails, it is often a missing symbol or incompatible API.</li> <li>Remediate before running the full suite.</li> </ul> </li> <li> <p>Remediation branch (compile errors vs failing tests)</p> </li> <li> <p>If compile/import fails:</p> <ul> <li>Localize:</li> <li>Capture the first error site (file + symbol) from the compile output.</li> <li>Example line: <code>***   File \"src/integrations/libx.py\", line 12</code></li> <li>Example line: <code>ImportError: cannot import name 'OldClient' from 'libx'</code></li> <li>Patch:</li> <li>Apply the minimal mechanical fix in the smallest set of files.</li> <li>Example: rename <code>OldClient</code> to <code>Client</code>.</li> <li>Verify:</li> <li>Rerun Gate A only.</li> <li>Proceed only when it returns exit code <code>0</code>.</li> <li>Budget guard:</li> <li>If more than 5 files are touched, stop and escalate (\u201crequires broader refactor\u201d).</li> <li>If cumulative diff exceeds 120 lines, stop and escalate (\u201cexceeds change budget for this kernel\u201d).</li> </ul> </li> <li> <p>If compile passes but tests fail:</p> <ul> <li>Localize:</li> <li>Run the smallest failing unit (single file or single test) based on the first failure.</li> <li>Example command: <code>pytest tests/test_libx_integration.py -q</code></li> <li>Example summary: <code>1 failed, 18 passed in 4.92s</code></li> <li>Patch:</li> <li>Address the behavioral change with a targeted adjustment.</li> <li>Record the reason.</li> <li>Example: \u201clibX now defaults timeout=None; set explicit timeout=5.0 in wrapper\u201d.</li> <li>Verify:</li> <li>Rerun the failing tests.</li> <li>Proceed to the full suite gate only after the localized failure is cleared.</li> </ul> </li> <li> <p>Run full verification gate (credibility gate)</p> </li> <li>Gate B (full): run the test suite (or the project\u2019s standard verification command).<ul> <li>Command: <code>pytest</code></li> </ul> </li> <li>Record:<ul> <li>exit code</li> <li>test summary line (example): <code>219 passed, 0 failed in 38.41s</code></li> </ul> </li> <li> <p>Verification risk handling:</p> <ul> <li>A narrowed verification set is acceptable only as an explicit gate waiver.</li> <li>Record why the full gate is unavailable.</li> <li>Record what alternative gate was run.</li> <li>Record what risk remains.</li> <li>Do not label a waiver as \u201cgreen\u201d; label it as \u201cwaived.\u201d</li> </ul> </li> <li> <p>Stop criteria and outputs</p> </li> <li>Stop success: Gate A and Gate B pass within budget.</li> <li>Stop failure: repeated failures indicate the upgrade exceeds current permission/scope (e.g., requires API redesign), or budgets are exhausted.</li> <li>Required outputs on stop:<ul> <li>change summary:</li> <li>files touched</li> <li>primary reason</li> <li>verification summary:</li> <li>Gate A command + result (include summary lines)</li> <li>Gate B command + result (include summary lines)</li> <li>rollback plan:</li> <li>\u201crevert manifest bump and lockfile\u201d (or equivalent)</li> <li>exact files to revert</li> </ul> </li> </ol> <p>This avoids \u201cfix-forward\u201d drift by forcing an early gate (compile) to localize breakage, and by stopping when remediation expands beyond the change budget.</p>"},{"location":"book/chapters/03-autonomous-kernels/#trade-offs","title":"Trade-offs","text":"<ul> <li> <p>Smaller kernels reduce risk but may require orchestration for multi-step projects.</p> </li> <li> <p>Mitigation: use staged kernels (e.g., \u201cdiagnose-only\u201d kernel \u2192 \u201cpatch\u201d kernel \u2192 \u201crefactor\u201d kernel), each with separate budgets and permissions.</p> </li> <li> <p>Strict permissions reduce blast radius but can prevent necessary refactors.</p> </li> <li> <p>Mitigation: use permission escalation as an explicit step with a justification and a widened verification gate (e.g., requiring a broader test suite when write scope expands).</p> </li> <li> <p>Heavier tracing improves auditability but adds operational overhead.</p> </li> <li> <p>Mitigation: record a minimum viable trace by default (commands, diffs, gate results), and sample/expand traces only on failures or high-risk task classes.</p> </li> </ul>"},{"location":"book/chapters/03-autonomous-kernels/#failure-modes","title":"Failure Modes","text":"<ul> <li>Local minima: kernel makes safe micro-edits without addressing root cause.</li> <li>Tool thrash: too many actions with low information gain.</li> <li>False confidence: passing a narrow eval set while violating higher-level requirements.</li> </ul> <p>Detection signals should be tied to budgets and evaluation gates, not intuition.</p> Failure mode Detection signals (operational) Local minima Same verification failure repeats across iterations.Edits stay confined to the same small area.Diff size increases without new localized evidence (no tighter repro, no new failing test isolated).Iteration budget is consumed with no change in selected gates or outcomes. Tool thrash Tool-call count rises while the plan and hypothesis remain unchanged.The same command is rerun without an intervening change that could affect its result.Files touched increases despite a small, bounded intent. False confidence Gates get narrower over time without a recorded waiver and risk note.Fast gates pass, but the credibility gate is repeatedly deferred without an explicit waiver record.Success is declared without a trace artifact that includes exact gate commands, exit codes, and summary lines."},{"location":"book/chapters/03-autonomous-kernels/#research-directions","title":"Research Directions","text":"<ul> <li>Kernel composition patterns (delegation, staged permissions, multi-kernel workflows).</li> <li>Automatic stop-condition tuning based on task class.</li> <li>Replayable kernels for deterministic debugging of agent behavior.</li> </ul>"},{"location":"book/chapters/04-memory-systems/","title":"Chapter 04 \u2014 Memory Systems","text":""},{"location":"book/chapters/04-memory-systems/#thesis","title":"Thesis","text":"<p>Memory is an engineered data store for work artifacts, not a raw log of prior messages. It must be structured, queryable, and governed (with provenance) to improve long-horizon work.</p> <ul> <li>Structured: stored as records with explicit fields (not free-form chat logs), so constraints, sources, and updates are representable.</li> <li>Queryable: retrievable by filters (task, file, timeframe, decision id), with ranking that prefers fresh, high-provenance entries.</li> <li>Governed: subject to retention, redaction, and correction rules, so stale or wrong entries can be superseded, expired entries can be hidden by default, and sensitive traces can be minimized.</li> <li>Correction: add a new record that <code>supersedes</code> a prior id (no silent edits).</li> <li>Redaction: remove or mask sensitive content while preserving minimal provenance where policy allows.</li> <li>Retention: apply expiry and access rules over time (e.g., via <code>valid_through</code> or class-specific TTLs).</li> </ul> <p>Definition of done for \u201cgood memory\u201d:</p> <ul> <li>Structure: every entry has an id, timestamp, type, and source.</li> <li>Query: a future agent can retrieve the right entry with a bounded filter (\u201cshow decisions for X\u201d / \u201cshow traces for regression Y\u201d).</li> <li>Governance: entries can be corrected (superseded) and expire or be redacted under policy.</li> </ul> <p>Hypothesis: uncurated memory increases confidence without increasing correctness, by amplifying earlier mistakes.</p>"},{"location":"book/chapters/04-memory-systems/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Long projects exceed context windows; without memory, work becomes repetitive and inconsistent.</li> <li>Without provenance, persistent memory becomes a source of silent drift: older summaries can out-rank newer evidence.</li> <li>Production environments need data minimization and retention policies for stored traces and summaries.</li> <li>Those policies must connect to provenance (what the data came from) and correction (how wrong entries are handled).</li> </ul>"},{"location":"book/chapters/04-memory-systems/#system-breakdown","title":"System Breakdown","text":"<p>A diagram helps here because memory systems fail at the interfaces (write \u2192 retrieve \u2192 act), not inside a single component. As you read it, focus on two gates: (1) retrieval is bounded and ranked, and (2) action is gated by provenance and freshness checks.</p>  flowchart TB   W[Write event\\n(policy: when/what to store)] --&gt; R[Record\\n(min schema fields)]   R --&gt; I[Index + Rank\\n(Read policy)]   I --&gt; V{Verify before acting\\n(provenance + freshness)}   V --&gt;|pass| A[Apply next step\\n(change, fix, decision)]   V --&gt;|fail| Q[Refine query\\nor re-run tests]   R --&gt; G[Govern\\n(retention/redaction/correction)]   G --&gt; R    subgraph Schema[Minimum schema fields]     ID[id]     TS[timestamp / valid_through]     TY[type]     SO[source pointer]     CF[confidence]     SU[supersedes]   end  <p>The Schema subgraph is a minimum viable baseline for this chapter (intentionally incomplete); implementations can add fields as needed without changing the write/read/governance gates.</p> <p>Legend (one-line per gate):</p> <ul> <li>Write: decide when to store and enforce minimum fields (especially <code>source</code>, <code>confidence</code>, and optional <code>valid_through</code>).</li> <li>Index + Rank: retrieve with bounded filters, then sort with explicit rules (freshness, confidence, not-superseded).</li> <li>Verify: before acting, check provenance and freshness; if checks fail, refine the query or re-run tests.</li> <li>Govern: apply retention/redaction/correction; corrections create new records that update ranking via <code>supersedes</code>.</li> <li>Schema extensions are fine, but ranking and governance still hinge on provenance, freshness/expiry, and explicit superseding.</li> </ul> <p>Takeaway: the record schema makes policies enforceable. Write policy populates fields with provenance. Read policy uses those fields for bounded retrieval and ranking. Governance updates them to correct and expire entries.</p> <p>Rule-of-thumb for write policy (\u201cstore\u201d vs \u201cdon\u2019t store\u201d):</p> <ul> <li>Store: decisions, constraints, conventions, and reproducible traces that can be pointed to (command + output snippet + commit/diff hash).</li> <li> <p>Don\u2019t store: raw transcripts, personal data, or long tool logs without a bounded retrieval key and an explicit retention/redaction rule.</p> </li> <li> <p>Memory classes:</p> </li> <li>Episodic: traces of actions/tool I/O/diffs.</li> <li>Semantic: stable project facts and conventions.</li> <li>Decisions: recorded trade-offs and constraints.</li> <li>State: current plan, progress, open issues (kept memory-specific: pointers to the latest plan/progress, not a second project management system).</li> <li>Write policy: what gets stored, when, by whom, and with what validation.</li> <li>Enforces that each record has a <code>source</code> pointer and a calibrated <code>confidence</code>.</li> <li>Sets <code>valid_through</code> for entries that must be refreshed (e.g., \u201ccurrent deployment procedure\u201d).</li> <li>Read policy: retrieval filters, ranking, freshness, and provenance checks.</li> <li>Filters by <code>type</code>, time window via <code>timestamp</code>, and task keys stored in the record.</li> <li>Ranks by freshness (prefer newest <code>timestamp</code>; hide expired <code>valid_through</code> by default unless explicitly requested), then by <code>confidence</code>.</li> <li>Prefers \u201cnot superseded\u201d: if a record is listed in another record\u2019s <code>supersedes</code>, it should be down-ranked or hidden by default.</li> <li>Governance: retention, access control, redaction, and correction mechanisms.</li> <li>Uses <code>supersedes</code> for explicit corrections rather than silent edits.</li> <li>Applies retention/redaction rules to episodic traces, while keeping minimal decision/semantic records with strong provenance.</li> </ul> <p>Minimum memory record schema (applies to all classes):</p> <ul> <li><code>id</code>: stable identifier (e.g., <code>dec-2026-02-22-authz-approach</code>, <code>trace-&lt;task&gt;-&lt;iter&gt;</code>).</li> <li><code>timestamp</code>: when recorded (and optionally <code>valid_through</code> for expiry/refresh).</li> <li><code>type</code>: episodic | semantic | decision | state.</li> <li><code>source</code>: where it came from (tool output, diff, doc link, human note) and a pointer (path/URL/commit hash).</li> <li><code>confidence</code>: coarse signal (e.g., low/medium/high) tied to source quality (tests passing, verified by review, etc.).</li> <li><code>supersedes</code>: optional list of prior ids this record replaces (supports correction and avoids \u201cmemory poisoning\u201d via repetition).</li> </ul>"},{"location":"book/chapters/04-memory-systems/#concrete-example-1","title":"Concrete Example 1","text":"<p>Decision memory for an architecture choice.</p> <p>Write event (what triggers storage):</p> <ul> <li>After selecting an approach in a design discussion or PR, store a decision record as part of the change (or alongside it) before implementation diverges.</li> </ul> <p>Stored record (template; identifiers are illustrative).</p> <p>Core schema fields:</p> field value <code>id</code> <code>dec-2026-02-22-memory-store-backend</code> <code>timestamp</code> <code>2026-02-22T19:33Z</code> <code>type</code> <code>decision</code> <code>source</code> <code>docs/decisions/dec-2026-02-22-memory-store-backend.md + commit 0123abcd</code> <code>confidence</code> <code>medium (reviewed; not yet load-tested)</code> <code>supersedes</code> <code>[]</code> <p>Decision-specific fields:</p> field value <code>statement</code> \u201cStore project memory as append-only records with explicit superseding, not as an editable wiki page.\u201d <code>options_considered</code> \u201cEditable wiki page\u201d; \u201cAppend-only records + supersedes field\u201d; \u201cNo persistence; rely on context only\u201d <code>chosen</code> \u201cAppend-only records + supersedes field\u201d <code>constraints</code> \u201cMust support redaction; must record provenance pointers; must allow correction.\u201d <code>rationale</code> \u201cEditable pages hide drift; append-only preserves history and enables explicit correction.\u201d <p>Enforcement rule (how future work must behave):</p> <ul> <li>Any change that contradicts the decision must reference <code>dec-2026-02-22-memory-store-backend</code> and justify why it still holds, or create a new decision record.</li> <li>The new record must include <code>supersedes: [\"dec-2026-02-22-memory-store-backend\"]</code>.</li> <li>This prevents silent divergence where repeated but incorrect summaries turn into \u201cfacts\u201d (see Memory poisoning in ## Failure Modes).</li> </ul>"},{"location":"book/chapters/04-memory-systems/#concrete-example-2","title":"Concrete Example 2","text":"<p>Trace-indexed memory for debugging.</p> <p>Trace-indexed memory stores enough episodic detail to support \u201cfind similar failure, verify with evidence\u201d workflows.</p> <p>Capture \u2192 index \u2192 retrieve \u2192 verify:</p> <ol> <li>Capture: store reproducible evidence for the iteration.</li> <li>Test command and a short failing stack trace snippet.</li> <li>Files changed plus a diff hash.</li> <li> <p>Environment notes that affect reproducibility (OS, dependency lockfile hash).</p> </li> <li> <p>Index: attach keys that support bounded queries later.</p> </li> <li><code>task_id</code>, <code>iteration</code>, <code>files_touched</code>.</li> <li><code>error_signature</code> (e.g., exception type + top frame).</li> <li> <p><code>timestamp</code> and <code>source</code> pointers.</p> </li> <li> <p>Retrieve: query with tight filters, then rank for usefulness.</p> </li> <li>Example query: \u201cShow traces where <code>error_signature</code> matches \u2018KeyError: CONFIG\u2019 and <code>files_touched</code> includes <code>settings.py</code> from the last 30 days.\u201d</li> <li> <p>Ranking rule: prefer freshest <code>timestamp</code> (and not-expired <code>valid_through</code> when present), then higher <code>confidence</code> (e.g., traces tied to commits that later passed CI), and prefer \u201cnot superseded.\u201d</p> </li> <li> <p>Verify (provenance + freshness): gate action on checks from Read policy.</p> </li> <li>Does the trace point to exact tool output and a commit hash (<code>source</code>)?</li> <li>Is the trace superseded by a newer trace for the same signature (<code>supersedes</code>)?</li> <li>Do current tests reproduce the signature, or has the failure mode changed?</li> </ol> <p>Expected outcome (measurable):</p> <ul> <li>For recurring <code>error_signature</code>s, reduce median iterations-to-fix from 6 to 3.</li> <li>Keep the post-fix regression rate at or below 2% in the next 20 CI runs affecting the same files.</li> <li>Calibrate <code>confidence</code> against evidence: raise it only when the linked commit passes CI for the relevant test suite, and downgrade it if later CI runs reintroduce the signature.</li> </ul> <p>This flow turns memory into a debugging aid rather than a replay of stale context: retrieval is constrained, and action is gated by a provenance check.</p>"},{"location":"book/chapters/04-memory-systems/#trade-offs","title":"Trade-offs","text":"<ul> <li>More memory improves continuity but increases risk of stale or incorrect retrieval.</li> <li>Strong provenance improves trust but adds overhead to writing and updating memory.</li> <li>Aggressive retention helps debugging but increases privacy and storage costs.</li> <li>Governance often trades cost of being wrong (acting on stale memory, causing regressions) against cost of being slow (extra steps to write, verify, and supersede records).</li> <li>Practical target: minimize the cost of being wrong under bounded retrieval and explicit correction, even if it adds a small constant overhead to each iteration.</li> </ul>"},{"location":"book/chapters/04-memory-systems/#failure-modes","title":"Failure Modes","text":"<ul> <li>Stale retrieval dominance: old assumptions override new evidence. Primary control: Read policy. Detection/mitigation: include freshness signals in Read policy (time windows, \u201cprefer not-superseded\u201d), and require verification against current tests before applying a past fix.</li> <li>Summarization loss: key constraints disappear in compression. Primary control: Write policy. Detection/mitigation: enforce Write policy that decision/semantic records keep explicit constraint fields, and link summaries back to their <code>source</code> pointers so missing details are recoverable.</li> <li>Memory poisoning: incorrect conclusions become \u201cfacts\u201d through repetition. Primary control: Governance (correction) plus Read policy (ranking). Detection/mitigation: require corrections via Governance using <code>supersedes</code> (no silent edits), and down-rank low-provenance entries in Read policy so repetition does not outweigh evidence.</li> <li>Control map (scan aid): stale retrieval dominance \u2192 Read policy; summarization loss \u2192 Write policy; memory poisoning \u2192 Governance + Read policy.</li> </ul>"},{"location":"book/chapters/04-memory-systems/#research-directions","title":"Research Directions","text":"<ul> <li>Memory scoring with automated freshness and provenance signals.</li> <li>Mechanisms for correcting memory (retractions, superseding records).</li> <li>Evaluations for memory usefulness (measuring reduced iterations without increased regressions), e.g., for trace-indexed debugging: reduce median \u201citerations-to-fix\u201d for recurring <code>error_signature</code>s while holding the post-fix regression rate constant (or lower) across subsequent CI runs.</li> </ul>"},{"location":"book/chapters/05-evaluation-and-traces/","title":"Chapter 05 \u2014 Evaluation and Traces","text":""},{"location":"book/chapters/05-evaluation-and-traces/#thesis","title":"Thesis","text":"<p>Evaluation and traceability make AI-first engineering reproducible. Traces provide evidence that links outcomes to the correct layer (model, tool, harness, or missing tests). Evaluations turn that evidence into gates that prevent incorrect changes from being accepted.</p> <p>Hypothesis: without trace-first design, teams cannot reliably distinguish a model error (wrong edit suggestion) from a tool error (command failed). They also cannot distinguish harness errors (applied the wrong diff or wrong working directory) from missing tests (a real regression that was not checked).</p>"},{"location":"book/chapters/05-evaluation-and-traces/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Reproducibility is a prerequisite for iterative improvement.</li> <li>Evaluation gates define the autonomy envelope and prevent silent regressions.</li> <li>Traces enable post-incident analysis and systematic harness refinement.</li> </ul>"},{"location":"book/chapters/05-evaluation-and-traces/#system-breakdown","title":"System Breakdown","text":"<ul> <li>Trace schema (minimum viable):</li> <li>task id</li> <li>plan<ul> <li>intended steps</li> <li>stop conditions</li> </ul> </li> <li>tool calls<ul> <li>tool name</li> <li>arguments</li> <li>start/end timestamps</li> <li>exit status</li> </ul> </li> <li>tool outputs<ul> <li>stdout/stderr excerpts, or pointers to stored artifacts</li> </ul> </li> <li>diffs<ul> <li>patches applied</li> <li>file paths touched</li> </ul> </li> <li>evaluation results<ul> <li>which checks ran</li> <li>pass/fail</li> <li>key failure signatures</li> </ul> </li> <li>budgets<ul> <li>time</li> <li>iteration count</li> <li>token/cost limits (if applicable)</li> </ul> </li> <li>stop reason<ul> <li>completed</li> <li>blocked by gate</li> <li>permission denied</li> <li>budget exceeded</li> </ul> </li> <li>redaction policy<ul> <li>what is removed or hashed (secrets, tokens, PII, proprietary paths)</li> </ul> </li> <li>environment fingerprint<ul> <li>repo URL (if applicable)</li> <li>commit SHA</li> <li>branch</li> <li>tool versions</li> <li>OS/arch</li> </ul> </li> <li>retry/iteration counters<ul> <li>attempt number per step</li> <li>total loop count</li> </ul> </li> <li>Query examples (incident review):<ul> <li>\u201cshow all tasks that modified <code>pyproject.toml</code> and failed secret scanning\u201d</li> <li>\u201clist failures where <code>tests</code> failed but a patch was still applied\u201d</li> <li>\u201cgroup stop reasons by action class over the last N runs\u201d</li> </ul> </li> <li>Evaluation types:</li> <li>correctness: unit/integration/contract tests.</li> <li>safety: permission checks, protected paths, secret scanning.</li> <li>quality: lint, type checks, formatting, doc checks.</li> <li>performance: benchmarks, latency/cost budgets.</li> <li>Gating model: which evaluations are required for which action classes.</li> </ul> <p>A gating model is only useful if the trace explains the choice.   A reviewer should be able to see why a gate passed, failed, or was skipped.</p> <p>A diagram helps here because the logic is a decision flow.   Focus on the first safety decision and the first failing gate.</p> <pre><code>flowchart TB\n  A[Action class selected] --&gt; B[Safety gate(s)\\n(permission / protected-path / secret scan)]\n  B --&gt;|fail| S1[STOP\\nstop_reason set\\nrecord: failure_signature + touched_paths]\n  B --&gt;|pass| C[Required evaluations\\nquality + correctness + performance]\n  C --&gt; D{Any required eval missing?}\n  D --&gt;|yes| S2[STOP\\nstop_reason set\\nrecord: skipped_reason + selection_reason]\n  D --&gt;|no| E{Any eval failed?}\n  E --&gt;|yes| S3[STOP\\nstop_reason set\\nrecord: command + exit_status + failure_signature]\n  E --&gt;|no| F[CONTINUE / COMPLETE\\nrecord: evaluation_results + budgets\\nstop_reason=completed]\n</code></pre> <p>After you review the flow, the key takeaway is this: every STOP is a first-class outcome.   The trace must record enough to explain the stop and replay the same checks.</p> <p>Legend:</p> <ul> <li>Diamonds are harness decision points.</li> <li> <p>Every STOP must record enough fields to replay the same checks.</p> </li> <li> <p>Minimal gating matrix (example, stated as rules):</p> </li> <li> <p>read-only (grep/view/list):</p> <ul> <li>safety: require permission check for accessed path(s)</li> <li>quality: skip; record <code>skipped_reason: \"read_only_action\"</code></li> <li>correctness: skip; record <code>skipped_reason: \"read_only_action\"</code></li> <li>performance: skip; record <code>skipped_reason: \"read_only_action\"</code></li> </ul> </li> <li> <p>patch edit (apply diff to code/docs):</p> <ul> <li>safety:</li> <li>require protected-path check for all touched files</li> <li>if any touched file is protected:<ul> <li>stop with <code>stop_reason: \"permission_denied\"</code></li> </ul> </li> <li>quality:</li> <li>require lint/typecheck if repo has config</li> <li>otherwise record <code>skipped_reason: \"no_config\"</code></li> <li>correctness:</li> <li>require targeted tests for touched modules</li> <li>selection order:       1) if a module\u2192test mapping exists:          - run the mapped command          - record <code>selection_reason: \"mapping\"</code>       2) else if the plan declares <code>test_command</code>:          - run it          - record <code>selection_reason: \"plan.test_command\"</code>       3) else if the repo declares a default test command:          - run it          - record <code>selection_reason</code> with the source path       4) else if a stack can be detected:          - run the stack fallback command          - record <code>selection_reason: \"fallback.detected_stack\"</code>       5) else:          - record <code>skipped_reason: \"no_test_runner_detected\"</code>          - stop with <code>stop_reason: \"blocked_by_correctness_gate\"</code></li> <li>fallback by detected stack:</li> <li>Deterministic stack detection (precedence order):<ul> <li>Inputs:</li> <li>touched file paths from the diff (after patch application)</li> <li>repo evidence files (repo root only)</li> <li>Step 1 \u2014 candidates from touched paths:</li> <li>Python if any touched path ends with <code>.py</code></li> <li>Node if any touched path ends with <code>.js</code>, <code>.jsx</code>, <code>.ts</code>, or <code>.tsx</code></li> <li>Go if any touched path ends with <code>.go</code></li> <li>Step 2 \u2014 if no candidates, use evidence files:</li> <li>Python if <code>pyproject.toml</code>, <code>pytest.ini</code>, <code>setup.cfg</code>, or <code>requirements.txt</code> exists</li> <li>Node if <code>package.json</code> exists</li> <li>Go if <code>go.mod</code> exists</li> <li>Step 3 \u2014 tie-break precedence:</li> <li>Python \u2192 Node \u2192 Go</li> <li>Trace requirements:</li> <li>record <code>selection_reason</code> (example: <code>touched_ext:.py</code>)</li> <li>record <code>detected_stacks</code> before tie-break (example: <code>[python,node]</code>)</li> </ul> </li> <li>Python: <code>pytest -q</code></li> <li>Node: <code>npm test</code></li> <li>Go: <code>go test ./...</code></li> <li>performance:</li> <li>require only when applicable</li> <li>require if any touched path is under <code>deploy/</code></li> <li>require if any touched path is under <code>prod/</code></li> <li>require if the plan declares <code>latency_budget_ms</code> or <code>cost_budget_usd</code></li> <li>otherwise record <code>skipped_reason: \"not_applicable\"</code></li> </ul> </li> <li> <p>dependency install (lockfile changes, package adds):</p> <ul> <li>safety:</li> <li>require secret scan of the diff and updated lockfile(s)</li> <li>record scan tool name</li> <li>record any hit signatures</li> <li>safety:</li> <li>require protected-path check for touched files</li> <li>if blocked, stop with <code>stop_reason: \"permission_denied\"</code></li> <li>quality:</li> <li>require lint/typecheck if repo has config</li> <li>otherwise record <code>skipped_reason: \"no_config\"</code></li> <li>correctness:</li> <li>require tests that import the changed dependency</li> <li>if unknown, select a command using the patch-edit rule</li> <li>record <code>selection_reason</code> for the path taken</li> <li>performance:</li> <li>require only when runtime or deploy files change</li> <li>examples: <code>Dockerfile</code>, <code>deploy/**</code></li> <li>otherwise record <code>skipped_reason: \"not_applicable\"</code></li> </ul> </li> <li> <p>deploy/release (publish, migrate, prod config):</p> <ul> <li>safety:</li> <li>require explicit permission grant</li> <li>record grant artifact (id or prompt) in the trace</li> <li>safety:</li> <li>require secret scan</li> <li>stop on any forbidden hit</li> <li>record hit signatures</li> <li>quality:</li> <li>require lint/typecheck if configured</li> <li>otherwise record <code>skipped_reason: \"no_config\"</code></li> <li>correctness:</li> <li>require full suite or contract tests for release</li> <li>record suite name and command</li> <li>performance:</li> <li>require if a budget is declared in the plan</li> <li>stop if exceeded</li> <li>record measured values</li> </ul> </li> </ul>"},{"location":"book/chapters/05-evaluation-and-traces/#concrete-example-1","title":"Concrete Example 1","text":"<p>Tracing a refactor.</p> <ul> <li>Record each patch, each test run, and each failure signature.</li> <li>Also record the environment fingerprint and exact tool invocations.</li> <li>That combination lets a reviewer replay the same sequence.</li> </ul> <p>Pseudo-trace excerpt (illustrative):</p> <pre><code>    task_id: refactor-auth-2026-02-22-001\n\n    environment:\n      repo_sha: a1b2c3d\n      tool_versions:\n        pytest: 8.1.1\n        python: 3.12.1\n\n    plan:\n      - rename AuthClient -&gt; CopilotClient\n      - update imports\n      - run targeted tests\n      - stop if tests fail\n\n    tool_calls:\n      - tool: apply_patch\n        files:\n          - src/auth/client.py\n        exit_status: 0\n      - tool: bash\n        cmd: pytest -q tests/test_auth_client.py::test_retry\n        exit_status: 1\n        failure_signature: ImportError: cannot import name 'AuthClient'\n\n    diffs:\n      - path: src/auth/client.py\n        summary: renamed symbol\n      - path: tests/test_auth_client.py\n        summary: unchanged\n\n    evaluation_results:\n      correctness:\n        cmd: pytest -q tests/test_auth_client.py::test_retry\n        outcome: FAIL\n        signature: ImportError\n\n    stop_reason: blocked_by_correctness_gate\n</code></pre> <p>Query step using structured fields (one possible workflow):</p> <ul> <li>Query: <code>repo_sha == \"a1b2c3d\" AND failure_signature CONTAINS \"ImportError: cannot import name 'AuthClient'\" AND diffs.paths CONTAINS \"src/auth/client.py\"</code></li> <li>Result (summary):</li> <li>matching_tasks: 3</li> <li>last_success_task_id: <code>refactor-auth-2026-02-20-007</code></li> <li>last_success_repo_sha: <code>a1b2c3d</code></li> <li>common_missing_patch_pattern: diffs do not include any files under <code>src/auth/__init__.py</code> or <code>src/auth/*</code> besides <code>client.py</code></li> </ul> <p>Decision rule using the query result:</p> <ul> <li>If <code>last_success_repo_sha</code> matches the current <code>repo_sha</code>:</li> <li>If only <code>client.py</code> was changed, expand patch scope to import sites.</li> <li>Update <code>src/auth/__init__.py</code> exports and import usages.</li> <li>Rerun <code>pytest -q tests/test_auth_client.py::test_retry</code>.</li> <li>If <code>last_success_repo_sha</code> differs:</li> <li>Rebase/reset to the pinned <code>repo_sha</code> first.</li> <li>Apply the expanded patch.</li> <li>Rerun <code>pytest -q tests/test_auth_client.py::test_retry</code>.</li> </ul> <p>Attribution using the trace:</p> <ul> <li>Model vs tool is not ambiguous here.</li> <li>The patch tool succeeded, and the test runner executed.</li> <li>The failure signature is a stable ImportError.</li> <li>The diffs show a narrow touch pattern, so the likely cause is refactor incompleteness.</li> </ul>"},{"location":"book/chapters/05-evaluation-and-traces/#concrete-example-2","title":"Concrete Example 2","text":"<p>Drift detection for an agent loop.</p> <ul> <li>Maintain a stable eval suite and a small set of \u201cgolden\u201d tasks.</li> <li>A \u201cgolden task\u201d should include:</li> <li>fixed input prompt and any fixed attachments</li> <li>fixed repo state (commit SHA) or a pinned fixture repository</li> <li>expected outcome constraints (e.g., \u201ctouch only <code>src/foo.py</code>\u201d, \u201cno network\u201d, \u201cno new dependencies\u201d)</li> <li>expected evaluations (which checks must pass)</li> <li>budgets (max iterations, max wall time, max tool calls)</li> <li>Define a baseline run-set for each golden task.</li> <li>Example: \u201clast green run-set on release tag <code>vX.Y</code>\u201d.</li> <li> <p>Keep repo_sha and harness version pinned for the baseline.</p> </li> <li> <p>Detect changes in iteration counts, regression rate, and stop reasons over time.</p> </li> <li>iteration drift:<ul> <li>median loop iterations in the most recent 50 runs increases by \u2265 30%</li> <li>compare against the baseline\u2019s 50-run median</li> </ul> </li> <li>regression rate:<ul> <li>correctness-gate failure rate in the most recent 50 runs increases by \u2265 5 percentage points</li> <li>compare against the baseline\u2019s 50-run rate</li> </ul> </li> <li>stop-reason shift:<ul> <li>in the most recent 50 runs, \u201cbudget exceeded\u201d becomes the most frequent stop_reason</li> <li>baseline window had \u201ccompleted\u201d as the most frequent stop_reason</li> </ul> </li> </ul> <p>The point is not to predict every future failure. The point is to detect that something changed (model, tool, harness, or repo) and to have enough trace evidence to localize the change.</p>"},{"location":"book/chapters/05-evaluation-and-traces/#trade-offs","title":"Trade-offs","text":"<ul> <li>More evaluation increases confidence but costs time and compute.</li> <li>Rich traces help debugging but create storage and privacy burdens.</li> <li>Overly rigid gates can block progress on codebases with weak test coverage.</li> </ul>"},{"location":"book/chapters/05-evaluation-and-traces/#failure-modes","title":"Failure Modes","text":"<ul> <li>Eval gaming: optimizing for metrics while harming real-world quality.</li> <li>Detect: compare \u201cpasses gates\u201d with downstream signals in traces (reverts, follow-up bugfix tasks, repeated failures on adjacent golden tasks); watch for large diffs with minimal test coverage.</li> <li>Respond: add adversarial or regression tests, strengthen gate definitions (e.g., require touched-module tests), and record coverage/selection rationale in the trace.</li> <li>Blind spots: evaluations do not cover critical behaviors.</li> <li>Detect: incidents where traces show \u201call gates passed\u201d but production-like behavior fails; repeated similar failures without corresponding eval signatures.</li> <li>Respond: add contract tests or invariants to the eval suite; introduce risk-based gating (e.g., stricter checks for auth/payment paths) and make the gating decision explicit in the trace.</li> <li>Un-actionable traces: logs exist but lack structure, making search and attribution hard.</li> <li>Detect: inability to answer basic questions (\u201cwhat changed?\u201d, \u201cwhat ran?\u201d, \u201cwhy did it stop?\u201d) without reading raw logs; missing environment fingerprint or missing diff records.</li> <li>Respond: enforce a minimal trace schema, store normalized fields for queries, and treat \u201cmissing trace fields\u201d as an evaluation failure for non-trivial action classes.</li> </ul>"},{"location":"book/chapters/05-evaluation-and-traces/#research-directions","title":"Research Directions","text":"<ul> <li>Standard trace formats for portability across tools and models.</li> <li>Risk-based gating (stricter checks for higher-risk diffs).</li> <li>Low-cost evaluations that correlate with production outcomes.</li> </ul>"},{"location":"book/chapters/06-agent-governance/","title":"Chapter 06 \u2014 Agent Governance","text":""},{"location":"book/chapters/06-agent-governance/#thesis","title":"Thesis","text":"<p>Governance defines the safe operating envelope for autonomy. It is not optional once agents can change code, invoke tools, or affect production systems. The \u201coperating envelope\u201d defines what an agent is allowed to do. It also defines when those actions are allowed and what evidence must exist afterward to support review and rollback.</p> <p>Hypothesis: autonomy without enforceable governance increases throughput in the short term. Over time, defect rate and operational risk rise. Changes can outpace review, exceed intended access scopes, or ship without traceable accountability.</p> <p>What governance is / is not:</p> <ul> <li>Governance is enforced constraints + auditability (tool/CI checks, logs, approvals), not a document that can be ignored.</li> <li>Governance is risk-shaped (stricter rules for higher-risk operations), not uniform friction applied everywhere.</li> <li>Governance is measurable (blocked unsafe changes, incidents, review latency), not a \u201cbest effort\u201d policy.</li> </ul>"},{"location":"book/chapters/06-agent-governance/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Agents can operate faster than human review cycles; governance prevents unsafe acceleration.</li> <li>Production environments require audit trails and controlled access to sensitive operations.</li> <li>Governance makes behavior consistent across models and team members.</li> </ul>"},{"location":"book/chapters/06-agent-governance/#system-breakdown","title":"System Breakdown","text":"<ul> <li>Policy artifacts: constitution (principles), agent rules (operational constraints), CI policies (enforcement).</li> <li>Enforceable rules:<ul> <li>\u201cNo writes to protected paths without approval token.\u201d</li> <li>\u201cAll tool calls must declare intent and target files.\u201d</li> </ul> </li> <li>Enforcement:<ul> <li>Pre-commit or tool-layer policy checks reject disallowed patches.</li> <li>CI re-validates policies on the merge commit.</li> </ul> </li> <li>Permissions: read/write scopes, protected files, tool allowlists.</li> <li>Enforceable rules:<ul> <li>\u201cRead-only by default; write requires explicit scope.\u201d</li> <li>\u201cNetwork access disabled unless allowlisted per job.\u201d</li> </ul> </li> <li>Enforcement:<ul> <li>Runtime tool allowlist/denylist.</li> <li>Repository protected-branch rules and CODEOWNERS coverage for specific paths.</li> </ul> </li> <li>Budgets: time, iterations, tool calls, diff size, cost ceilings.</li> <li>Enforceable rules:<ul> <li>\u201cMax N iterations per task.\u201d</li> <li>\u201cMax diff size for autonomous edits.\u201d</li> <li>\u201cStop on repeated failures.\u201d</li> </ul> </li> <li>Enforcement:<ul> <li>Harness-level counters.</li> <li>CI policy fails if changes exceed thresholds or if required checkpoints were skipped.</li> </ul> </li> <li>Review: mandatory human checkpoints for specific risk classes.</li> <li>Enforceable rules:<ul> <li>\u201cHuman approval required for security configs, auth paths, dependency upgrades.\u201d</li> <li>\u201cTwo-person review for rollback scripts.\u201d</li> </ul> </li> <li>Enforcement:<ul> <li>CODEOWNERS + branch protection.</li> <li>CI classifies changes and blocks merge unless approvals match the risk class.</li> </ul> </li> <li>Audit: trace retention, searchable logs, change attribution.</li> <li>Enforceable rules:<ul> <li>\u201cAll tool executions emit structured logs.\u201d</li> <li>\u201cEach patch links to an agent run id and input prompt.\u201d</li> </ul> </li> <li>Enforcement:<ul> <li>Log collection in CI artifacts or a central store.</li> <li>Merge requires a commit trailer field <code>Agent-Run-Id: &lt;run_id&gt;</code>.</li> <li>CI validates this trailer case-sensitively.</li> <li>The key must be exactly <code>Agent-Run-Id</code>.</li> <li><code>&lt;run_id&gt;</code> must be lowercase and match <code>run_[a-z0-9]{12}</code>.</li> <li>Valid: <code>Agent-Run-Id: run_3f9c2a1b7d4e</code>.</li> <li>Invalid: <code>agent-run-id: run_3f9c2a1b7d4e</code>, <code>Agent-Run-Id: RUN_3F9C2A1B7D4E</code>.</li> </ul> </li> </ul> <p>Governance loop (artifacts in parentheses):</p> <ol> <li>Policy definition (constitution / agent rules / CI policies)</li> <li>Enforcement at execution and merge time (tool allowlists / protected-path checks / CI gates)</li> <li>Telemetry and audit capture (structured traces / searchable logs / change attribution)</li> <li>Review routing and approvals (risk classification / CODEOWNERS / mandatory checkpoints)</li> <li>Policy update after incidents or near-misses (rule changes + new tests/evals)</li> </ol> <p>A diagram helps here because governance is a loop with repeated checkpoints. In the flow below, focus on (a) where enforcement blocks unsafe actions and (b) where evidence is captured. Those points determine what reviewers can verify and what can be rolled back safely.</p> <pre>\nflowchart TB\n  A[\"1. Policy definition(constitution / agent rules / CI policies)\"]\n  B[\"2. Enforcement(tool allowlists / protected paths / CI gates)\"]\n  C[\"3. Telemetry + audit capture(structured traces / logs / attribution)\"]\n  D[\"4. Review routing + approvals(risk class / CODEOWNERS / checkpoints)\"]\n  E[\"5. Policy update(incidents / near-misses / new evals)\"]\n\n  A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; A\n</pre> <p>Takeaway: steps 2 and 3 are the \u201chard\u201d parts of governance. Step 2 controls what can happen. Step 3 determines what proof exists afterward, so steps 4 and 5 can block unsafe merges and tighten policy based on observed failures.</p>"},{"location":"book/chapters/06-agent-governance/#concrete-example-1","title":"Concrete Example 1","text":"<p>Protected-path governance.</p> <p>Policy text (agent rules / CI policy):</p> <ul> <li>Rule: forbid edits to security-critical configs without explicit approval.</li> <li>Protected paths (example): <code>infra/</code>, <code>.github/workflows/</code>, <code>config/prod/</code>, <code>secrets/</code>.</li> </ul> <p>Enforcement point (tool layer + CI):</p> <ul> <li>Tool layer: rejects any patch touching a protected path unless an approval token is present in the task context (e.g., <code>APPROVAL=SECURITY-1234</code>) and the approver identity is recorded.</li> <li>CI: re-checks the merged diff; fails the build if protected paths changed without the recorded approval.</li> </ul> <p>Example attempted edit (input and check):</p> <ul> <li>Agent proposes changing <code>.github/workflows/release.yml</code> to add a new step that uploads artifacts.</li> <li>Policy check inspects the diff and matches a protected path.</li> </ul> <p>Expected rejection message (testable outcome):</p> <ul> <li>\u201cRejected: write to protected path <code>.github/workflows/</code> requires approval. Provide approval token + approver identity, or move the change to an approved human-owned PR.\u201d</li> </ul> <p>Remediation path (approval and artifacts produced):</p> <ul> <li>Human reviewer evaluates the intent, then supplies an approval token and identity.</li> <li>Artifact produced: an audit entry containing (a) file paths touched, (b) approval token, (c) approver, (d) timestamps, (e) tool execution trace id.</li> <li>CI verifies the audit entry exists and is linked to the commit/PR before allowing merge.</li> </ul>"},{"location":"book/chapters/06-agent-governance/#concrete-example-2","title":"Concrete Example 2","text":"<p>Incident response for a bad autonomous change.</p> <p>Detection signal:</p> <ul> <li>Trigger: regression detected by the <code>smoke-prod</code> gate post-merge.</li> <li>Example threshold (checkable): <code>smoke-prod</code> fails if HTTP 5xx rate exceeds 1.0% over a 10-minute canary window.</li> <li>Example threshold (checkable): <code>smoke-prod</code> fails if p95 latency regresses by more than 20% versus the previous baseline.</li> <li>Gate semantics (checkable): <code>smoke-prod</code> fails if either condition is met.</li> <li>Gate semantics (checkable): OR semantics.</li> </ul> <p>Immediate containment (rollback):</p> <ul> <li>Action: revert the merge commit (or roll back the deployment) using the pre-defined rollback procedure.</li> <li>Containment SLO (example): rollback initiated within 5 minutes of <code>smoke-prod</code> failure and completed within 15 minutes.</li> <li>Output: a rollback commit/deployment event linked to the original agent run id.</li> </ul> <p>Trace queries (audit-driven investigation):</p> <ul> <li>Use the audit trail to answer: which tools were invoked, which files were changed, which tests were executed, and whether any approvals were used.</li> <li>Minimum required evidence: a searchable trace that reconstructs the diff, command outputs, and decision points (e.g., \u201cskipped test X due to timeout\u201d).</li> <li>SLO evidence: audit record includes timestamps for detection, rollback initiation, and rollback completion, plus a deployment event id that can be cross-referenced.</li> </ul> <p>Root-cause classification (review-driven):</p> <ul> <li>Classify the incident into a governance-relevant bucket, for example:</li> <li>Missing gate: the regression would have been caught by a specific test that did not run.</li> <li>Mis-scoped permissions: the agent changed a file outside the intended scope.</li> <li>Review routing failure: change risk was misclassified and did not receive the required human checkpoint.</li> </ul> <p>Policy change (closing the gap):</p> <ul> <li>Update agent rules/CI policy to prevent recurrence, e.g., \u201cChanges touching <code>config/prod/</code> must run <code>smoke-prod</code> suite,\u201d or \u201cAny change to deployment steps requires CODEOWNER approval.\u201d</li> </ul> <p>New eval/test gate (enforced going forward):</p> <ul> <li>Add or strengthen a regression test/eval that fails deterministically on the known pattern.</li> <li>Wire it into CI so it is mandatory for the relevant risk class, and confirm the audit system records that the gate ran (not just that it exists).</li> </ul>"},{"location":"book/chapters/06-agent-governance/#trade-offs","title":"Trade-offs","text":"<ul> <li>Strong governance reduces risk but can slow iteration; the goal is to pay friction only where risk is high.</li> <li>Overly strict permissions increase human workload via escalations and can create \u201capproval queues.\u201d</li> <li>Excessive auditing can create privacy and compliance burdens.</li> </ul> <p>Operational metrics to make these trade-offs visible:</p> <ul> <li>Friction cost: median time-to-approval for high-risk changes; escalation rate (% of tasks requiring human intervention); rework rate after policy rejections.</li> <li>Prevented incidents: count of blocked protected-path writes; count of merges prevented due to missing required gates; incident rate per 100 changes for governed vs ungoverned paths.</li> <li>Governance effectiveness: time-to-rollback from detection; percentage of changes with complete trace metadata; false-positive rate of risk classification (unnecessary blocks).</li> </ul>"},{"location":"book/chapters/06-agent-governance/#failure-modes","title":"Failure Modes","text":"<ul> <li>Policy drift: rules become outdated and stop reflecting real risks.</li> <li>Shadow autonomy: humans bypass gates \u201cjust this once,\u201d breaking discipline.</li> <li>Governance without enforcement: documents exist but tools/CI do not enforce them.</li> </ul>"},{"location":"book/chapters/06-agent-governance/#research-directions","title":"Research Directions","text":"<ul> <li>Policy-as-code patterns for agent systems. Deliverable: a versioned policy repository with rule tests and a CI job that validates enforcement on every change.</li> <li>Automated risk scoring for diffs to route reviews. Deliverable: a diff classifier with explicit routing rules (which paths/operations require which approvals) and measured false-positive/false-negative rates.</li> <li>Governance metrics: prevented incidents vs friction cost. Deliverable: a dashboard spec that defines the core counters (blocked changes, escalations, incident rate, time-to-rollback) and the data sources for each.</li> </ul>"},{"location":"book/chapters/07-production-ai-infrastructure/","title":"Chapter 07 \u2014 Production AI Infrastructure","text":""},{"location":"book/chapters/07-production-ai-infrastructure/#thesis","title":"Thesis","text":"<p>Production AI-first systems are distributed systems: they require orchestration, isolation, observability, caching, cost control, and reproducible environments.</p> <p>Hypothesis: operational reliability depends more on the tool/runtime plane than on the model prompt. The tool/runtime plane is the execution and control surface around the model. It includes sandboxed execution environments, tool adapters (test runner, browser, repo API), and orchestration policies (queueing, concurrency limits, retries, idempotency). It also includes observability and artifacts for replay and audit. Restated: if you can reliably run tools and record what happened, you can reproduce runs and improve outcomes even when model behavior varies.</p>"},{"location":"book/chapters/07-production-ai-infrastructure/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Without isolation, tool execution becomes a security and reliability risk.</li> <li>Without observability, failures cannot be attributed or fixed systematically.</li> <li>Without cost controls, autonomy can become economically unstable.</li> <li>Operational signals: tool-failure rate, replay success rate, mean tool latency, retry rate, and spend per successful task.</li> </ul> <p>Example targets and alerts (illustrative, not mandates):</p> Metric Signal to watch Illustrative alert Tool-failure rate Tool exits non-zero or returns invalid output Alert if &gt;2% over 1 hour for a repo; or if one tool burns its daily error budget Replay success rate \u201cGreen\u201d runs fail to replay from recorded inputs Alert if &lt;95% on a weekly replay audit sample Mean tool latency Step duration inflation for stable workloads Alert if p95 step duration doubles week-over-week Retry rate Rising retries indicate flakiness or degraded infra Alert if retries exceed 1.2\u00d7 baseline for two consecutive days Spend per successful task Cost-to-merge and wasted tokens trend upward Alert if median cost-to-merge exceeds a cap; or wasted spend exceeds a daily share"},{"location":"book/chapters/07-production-ai-infrastructure/#system-breakdown","title":"System Breakdown","text":"<p>A diagram helps here because the tool/runtime plane has coupled components. It is not a single service. Focus on the contracts between boxes. Each box should emit stable, versioned signals for replay and debugging.</p> <pre><code>flowchart LR\n  M[Model prompt] --&gt; O[Orchestration&lt;br/&gt;Queues \u2022 Concurrency \u2022 Retries \u2022 Idempotency]\n  O --&gt; E[Execution&lt;br/&gt;Sandbox \u2022 Pinned deps \u2022 Timeouts]\n  E --&gt; T[Tool services&lt;br/&gt;Tests \u2022 Build \u2022 Browser \u2022 Repo API]\n  T --&gt; OB[Observability&lt;br/&gt;Traces \u2022 Metrics \u2022 Logs \u2022 Correlation IDs]\n  T --&gt; A[Artifacts&lt;br/&gt;Diffs \u2022 Reports \u2022 Replay bundle]\n  E --&gt; S[Security&lt;br/&gt;Allowlists \u2022 Least privilege \u2022 Secret injection]\n\n  OB --&gt; A\n  S -. governs .-&gt; E\n  S -. governs .-&gt; T\n  O -. tags .-&gt; OB\n  O -. stores run id .-&gt; A\n</code></pre> <p>Takeaway: reliability comes from strict contracts at each boundary. Record the environment and tool versions, constrain execution, and connect every step to a run id. Then you can replay, attribute failures, and control spend.</p> <ul> <li>Execution: sandboxes/containers, dependency pinning, deterministic runners. Contract: identical inputs produce the same tool environment (image hash + lockfile), with a hard wall-clock timeout per step.</li> <li>Tool services: test runners, build systems, browsers, repo APIs. Contract: every tool call is versioned and returns structured output (exit code, stdout/stderr, and a machine-readable summary).</li> <li>Orchestration: queues, concurrency limits, backpressure. Contract: max concurrency is enforced (per repo/org), retries are bounded (count + backoff), and each task carries an idempotency key.</li> <li>Observability: traces, metrics, logs; correlation ids. Contract: every task/run has a run id and spans for each tool call, with outcome and duration recorded.</li> <li>Artifacts: build outputs, diffs, evaluation reports, replay bundles. Contract: store a bundle per run (inputs, tool versions, command lines, logs, diffs, and evaluation result) with a retention policy.</li> <li>Security: secrets handling, network egress controls, least privilege. Contract: allowlists cover tools, filesystem paths, and network egress; secrets are injected only at execution time and never written to artifacts.</li> </ul>"},{"location":"book/chapters/07-production-ai-infrastructure/#concrete-example-1","title":"Concrete Example 1","text":"<p>Sandboxed tool execution for code changes.</p> <ul> <li>Trigger: a proposed patch (diff) plus a task spec (e.g., \u201cfix failing test X\u201d). Include the target branch SHA and a pinned environment (container image + lockfile).</li> <li>Sandbox: start an isolated runner with no ambient credentials. Mount the repo read-write and restrict filesystem + network egress to an allowlist.</li> <li>Tool calls:</li> <li>Run a fixed sequence (format/lint \u2192 unit tests \u2192 build).</li> <li>Enforce step timeouts (e.g., 5m/unit, 15m/build) and bounded retries for flaky steps (e.g., 2 retries with exponential backoff).</li> <li>Artifact bundle (stored per run id):</li> <li>Persist the patch and tool call transcripts (commands, versions, exit codes).</li> <li>Persist logs, test reports (JUnit/JSON), build outputs, and a replay manifest for the same inputs.</li> <li>Evaluation gate:</li> <li>Promote only if required checks pass (e.g., all tests green, no new lints, diff applies cleanly).</li> <li>Require reproducibility: either a replay succeeds at least once, or the environment hash matches a known-good cache entry.</li> <li>On failure, generate a human-facing summary with: run id link, failed step, and top error class.</li> <li>Include a short \u201cwhat to try next\u201d hint (e.g., rerun without cache, or inspect a specific log).</li> </ul>"},{"location":"book/chapters/07-production-ai-infrastructure/#concrete-example-2","title":"Concrete Example 2","text":"<p>Cost-aware autonomy for a batch of maintenance tasks.</p> <ul> <li>Budget: per-task token/cost ceilings (e.g., $0.50 and 20k tokens) plus a batch budget (e.g., $50/day), enforced by the orchestrator.</li> <li>Strategy: fail fast on low-signal tasks (small, repetitive, or high-latency tool loops) and escalate to human review when confidence is low or blast radius is high.</li> <li>Decision policy:</li> <li>Treat a task as \u201clow-signal\u201d when:<ul> <li>(a) there is no progress after N tool steps (e.g., 6),</li> <li>(b) the same error repeats (e.g., the same stack trace twice), or</li> <li>(c) predicted cost-to-complete exceeds remaining budget.</li> </ul> </li> <li>Escalate when the change touches production config or security-sensitive files.</li> <li>Escalate when the diff exceeds a size threshold (e.g., &gt;200 lines changed).</li> <li>If per-task or batch budget is exceeded:<ul> <li>stop further tool calls,</li> <li>write a short spend-and-status summary (last step, last error, run id),</li> <li>escalate for human review.</li> </ul> </li> <li>Measure (throughput): cost per successful task and time-to-merge.</li> <li>Measure (quality): regression rate (e.g., rollback or test failures within 24h) and \u201cwasted spend\u201d (tokens spent on tasks that are abandoned or escalated).</li> </ul>"},{"location":"book/chapters/07-production-ai-infrastructure/#trade-offs","title":"Trade-offs","text":"<ul> <li>Isolation increases safety but adds operational complexity. Default: start with containerized execution + allowlists; revisit if tool latency dominates (e.g., repeated cold starts) and you can prove tighter scoping by repo/path.</li> <li>Strong observability increases insight but raises data retention requirements. Default: use structured logs + traces with short retention for raw logs and longer retention for summaries; revisit if incident analysis regularly needs deeper raw context.</li> <li>Caching and replay improve speed but can mask nondeterminism if misused. Default: cache only deterministic steps (dependency installs keyed by lockfile, build outputs keyed by inputs). Periodically force no-cache replays; revisit if you observe drift or flaky tests that caching hides.</li> </ul>"},{"location":"book/chapters/07-production-ai-infrastructure/#failure-modes","title":"Failure Modes","text":"<ul> <li>Non-reproducible runs: environment drift makes traces hard to replay.</li> <li>Detection: replay fails with different dependency resolutions; tool versions differ from the recorded manifest; repeated \u201cworks on runner A but not runner B\u201d incidents.</li> <li>Mitigation: pin images and dependencies; record tool versions and hashes in the replay manifest; run periodic \u201creplay audits\u201d that re-execute a sample of recent runs.</li> <li>Leaky permissions: tool plane has broader access than intended.</li> <li>Detection: outbound network calls to unexpected domains; tools reading/writing outside approved paths; secrets appearing in logs or artifacts.</li> <li>Mitigation: enforce network egress allowlists; run tools with least-privilege credentials scoped to a repo/task; add secret redaction and artifact scanning before persistence.</li> <li>Noisy observability: too much unstructured logging reduces signal.</li> <li>Detection: high log volume with low queryability; incident timelines require manual grepping; key metrics (duration, retries, error class) missing from dashboards.</li> <li>Mitigation: emit structured events for each tool call; standardize error classes and outcome codes; sample verbose logs while keeping full traces for failed runs only.</li> </ul>"},{"location":"book/chapters/07-production-ai-infrastructure/#research-directions","title":"Research Directions","text":"<ul> <li>Standardized replay bundles for agent runs.</li> <li>Cost/performance models that predict optimal evaluation depth.</li> <li>Secure-by-default tool runtime primitives for autonomy.</li> </ul>"},{"location":"book/chapters/99-future-directions/","title":"Chapter 99 \u2014 Future Directions","text":""},{"location":"book/chapters/99-future-directions/#thesis","title":"Thesis","text":"<p>The frontier is not larger models. It is system-level interfaces and verification.</p> <p>Concretely, that means investing in artifacts that scale across teams and models:</p> <ul> <li>stronger tool contracts</li> <li>better evaluations</li> <li>structured memory</li> <li>governance primitives</li> </ul> <p>Here, \u201cinterfaces\u201d means the concrete artifacts that let components interoperate. In practice, interfaces are what make runs portable. Examples include a tool-call schema that two runtimes both validate, a trace format that two analysis tools both parse, and an eval definition that two teams can both reproduce.</p> <p>\u201cVerification\u201d means methods that detect incorrect behavior reliably. In practice, verification is what makes runs auditable. It lets you show that a tool call matched a contract and that a replay stayed within a declared nondeterminism boundary. It also lets you show that a reported score came from a pinned dataset and scoring implementation.</p> <p>Hypothesis: as autonomy scales, the limiting factor becomes organizational and infrastructural coupling, not raw inference capability. Takeaway: progress comes from making runs portable and checkable across models, tools, and teams.</p>"},{"location":"book/chapters/99-future-directions/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Teams will operate heterogeneous models and tools; interoperability becomes a reliability constraint.</li> <li>Long-horizon autonomy introduces new failure classes (compounded assumptions, policy drift, supply-chain issues).</li> <li>Without standards, every team reinvents trace formats, eval suites, and governance mechanisms.</li> </ul> <p>So what changes for teams? Treat tool schemas, trace formats, and eval definitions as versioned products. Write contracts and test them. Require replayable traces so failures can be audited and compared across models.</p>"},{"location":"book/chapters/99-future-directions/#system-breakdown","title":"System Breakdown","text":"<p>These four areas are coupled. Interfaces create portability, and verification enables auditability. Governance sets the rules for how shared artifacts evolve. The diagram below is useful because it makes the dependency shape explicit. Focus on the arrows: they show what must be versioned and validated before you can compare runs across teams or models.</p> <pre>\nflowchart TB\n  I[Interoperabilitytrace formats \u00b7 tool schemas \u00b7 eval definitions] --&gt; V[Verificationcontract tests \u00b7 replay checks \u00b7 property checks]\n  V --&gt; G[Governance at scalepolicy registry \u00b7 audit workflow \u00b7 incident runbook]\n  I --&gt; R[Ecosystem riskssupply chain \u00b7 dependency security \u00b7 model updates]\n  R --&gt; V\n</pre> <p>Takeaway: you can improve one pillar in isolation, but cross-model portability depends on the whole chain. Interoperability defines what can be exchanged, and verification defines what can be trusted when it is exchanged.</p> <ul> <li>Interoperability: shared trace formats, tool schemas, evaluation definitions.</li> <li>Verification: stronger correctness checks, property-based testing, contract enforcement.</li> <li>Governance at scale: org-level policies, audit workflows, incident response.</li> <li>Ecosystem risks: prompt/tool supply chain, dependency security, model updates.</li> </ul> <p>Artifact map (concrete deliverables):</p> <ul> <li>Interoperability</li> <li>Tool contract schema: JSON Schema for each tool\u2019s inputs/outputs, error types, and retry semantics.</li> <li>Trace interchange spec: required event taxonomy + field requirements so runs can be exported and replayed elsewhere.</li> <li>Eval definition format: task spec + dataset version + scoring code hash so results are reproducible.</li> <li>Verification</li> <li>Contract test suite: tool-level tests (including negative cases) and schema validation on every tool call.</li> <li>Replay protocol: \u201csame inputs, same trace constraints\u201d checks (within defined nondeterminism bounds).</li> <li>Property checks: invariants over traces (e.g., budgets, safety gates, monotonic progress signals).</li> <li>Governance at scale</li> <li>Policy registry: versioned policies with owners, rollout rules, and audit requirements.</li> <li>Audit workflow: sampling rules + trace retention + review checklist for incidents and regressions.</li> <li>Incident runbook: severity levels, rollback procedures, and postmortem templates.</li> <li>Ecosystem risks</li> <li>Supply-chain controls: signed prompt/tool bundles, dependency pinning, and provenance tracking.</li> <li>Model update gates: pre-deploy regression evals + canary rollout criteria.</li> </ul>"},{"location":"book/chapters/99-future-directions/#concrete-example-1","title":"Concrete Example 1","text":"<p>Cross-model portability experiment.</p> <p>Inputs</p> <ul> <li>A fixed harness (same prompts, tool set, budgets, retry policy, and stopping criteria).</li> <li>A fixed eval suite with a versioned dataset and deterministic scoring.</li> <li>A model set (e.g., Model A, Model B, Model C) that is intentionally heterogeneous (different vendors or major versions).</li> </ul> <p>Procedure (minimal)</p> <ul> <li>Run N trials per task per model with identical harness inputs (same seeds where applicable; same tool sandbox state).</li> <li>Record traces using the same trace interchange spec (see next section).</li> <li>Compute:</li> <li>Task success rate (pass/fail as defined by the eval).</li> <li>Tool error rate (by tool + error type).</li> <li>Iteration profile (turn count, tool-call count, timeouts/budget hits).</li> <li>Failure signatures (clusters based on trace event sequences, not just final answers).</li> </ul> <p>Expected outputs</p> <ul> <li>A per-model result table, plus a \u201csignature diff\u201d report. The report shows which failure clusters are model-specific vs shared.</li> <li>A set of \u201cportability blockers\u201d attributed to either:</li> <li>Harness/tool coupling (e.g., a tool contract ambiguity that different models interpret differently), or</li> <li>Model behavior (e.g., consistent violation of a particular tool precondition).</li> </ul> <p>Interpreting disagreements</p> <ul> <li>If multiple models fail in the same way on the same tasks, prioritize harness-level fixes (tool contract clarity, validation, better stop conditions).</li> <li>If one model fails with a distinct trace signature while others pass under identical contracts, treat it as model-dependent and capture it as a regression test.</li> </ul> <p>What would falsify the goal</p> <ul> <li>If variance is dominated by harness nondeterminism (e.g., unstable tool responses or non-versioned datasets), differences cannot be attributed to models. The harness is not portable enough to support the comparison.</li> <li>If success/failure flips under small, contract-preserving changes across models (e.g., harmless schema reordering), the tool contracts are underspecified.</li> </ul>"},{"location":"book/chapters/99-future-directions/#concrete-example-2","title":"Concrete Example 2","text":"<p>Standardized trace interchange.</p> <p>Goal Enable independent auditing and regression analysis by exporting traces from one agent runtime and replaying/analyzing them in another tool.</p> <p>A diagram helps here because trace interchange is a pipeline with explicit checkpoints. As you read it, track three things: where validation occurs, which fields get exported, and what the divergence check is allowed to claim as \u201cverified.\u201d</p> <pre>\nflowchart LR\n  A[Generate run] --&gt; B[Validate tool contracts]\n  B --&gt; C[Emit traceschema_version, run_id, eval_id, model_id]\n  C --&gt; D[Export trace]\n  D --&gt; E[Replay harness]\n  E --&gt; F{Divergence check}\n  F --&gt;|Matches constraints| G[Audit / regression analysis]\n  F --&gt;|Diverges| H[Flag portability failure]\n</pre> <p>Legend: \u201cValidate tool contracts\u201d means schema-checking arguments/results (and negative cases) at runtime before they enter the trace. \u201cDivergence check\u201d means comparing the replayed run to declared constraints, not forcing byte-for-byte identity when nondeterminism is allowed.</p> <p>The point is not to standardize everything. It is to standardize the minimum needed so two independent tools can agree on what happened, and can detect when a run is not reproducible under stated constraints.</p> <p>Minimal trace interchange contract (required fields)</p> <p>Run metadata</p> <ul> <li><code>schema_version</code>: semantic version for the trace spec (e.g., <code>1.2.0</code>).</li> <li><code>run_id</code>: unique identifier for a single run; stable across exports.</li> <li><code>eval_id</code> and <code>eval_version</code>: eval definition and dataset/scoring version.</li> <li><code>harness_id</code> and <code>harness_version</code>: code hash or build identifier.</li> <li><code>model_id</code>: model name/version as reported by the provider.</li> </ul> <p>Event schema</p> <ul> <li><code>events[]</code>: ordered list of events.</li> <li><code>event_id</code>, <code>type</code>, <code>timestamp</code>, <code>parent_event_id</code> (when applicable)</li> <li>Assistant/user text: content plus redaction markers (when redacted)</li> <li>Tool calls: <code>tool_name</code>, validated <code>arguments</code>, <code>result</code> or structured error, <code>duration_ms</code></li> <li>Policy gates: decision, rule id/version, rationale category (not freeform prose)</li> </ul> <p>Versioning rule</p> <ul> <li>Backward-compatible additions increment MINOR.</li> <li>Breaking changes increment MAJOR.</li> <li>A replay tool must refuse to \u201cverify\u201d unsupported MAJOR versions; it may still \u201cview\u201d them.</li> </ul> <p>Replay validity check (what must match)</p> <ul> <li>Under deterministic conditions, the tool-call sequence must match.   Match means tool name plus validated arguments.</li> <li>For deterministic, versioned tools, outcomes must also match.</li> <li>When nondeterminism exists (timeouts, stochastic tools, external APIs), record the boundary.   Examples: tool snapshot id, cached response id, or allowed outcome set.   Replay is valid only if outcomes stay within that boundary.</li> <li>If replay diverges in tool-call sequence under deterministic conditions, flag a portability failure.</li> </ul>"},{"location":"book/chapters/99-future-directions/#trade-offs","title":"Trade-offs","text":"<ul> <li>Standardization improves portability but can slow experimentation.</li> <li>Strong verification increases confidence but can increase compute and engineering effort.</li> <li>More governance improves safety but can reduce developer autonomy.</li> </ul> <p>Decision checklist (operational)</p> <p>Standardize</p> <ul> <li>Standardize when multiple teams depend on the same tools/traces.</li> <li>Standardize when incidents require cross-team auditing.</li> <li>Standardize when model swaps are frequent.</li> <li>Delay standardization when the interface changes weekly and only one team uses it.</li> <li>Minimum viable standardization threshold (example): when a tool or trace schema has 2+ consuming teams and changes less than once per sprint, require semantic versioning, a contract test suite, and a changelog entry for every interface change.</li> </ul> <p>Verify</p> <ul> <li>Use tests/contracts when failures are frequent, expensive, or safety-critical.</li> <li>Prefer lighter checks for experimental, low-impact components.</li> <li>Still enforce schema validation and basic budgets.</li> </ul> <p>Govern</p> <ul> <li>Escalate governance when changes affect shared tool contracts, trace schemas, or eval definitions.</li> <li>Keep governance minimal for isolated experiments that do not affect shared artifacts.</li> <li>Set thresholds explicitly: acceptable tool error rate, maximum budget hits per run, and the severity that triggers an incident workflow.</li> </ul>"},{"location":"book/chapters/99-future-directions/#failure-modes","title":"Failure Modes","text":"<ul> <li>Lock-in: traces and tools become proprietary and non-portable.   Mitigation: adopt the trace interchange spec + semantic versioning, and require export/replay tooling as a release gate for shared runtimes.</li> <li>False comparability: metrics appear comparable across systems but differ in hidden ways.   Mitigation: pin <code>eval_version</code>, record scoring code hashes, and add calibration tasks that must match within tolerance before comparing models.</li> <li>Scale amplification: small policy errors cause large, repeated failures.   Mitigation: treat policies as versioned artifacts, run a policy regression suite on traces, and use canary rollouts with explicit rollback criteria.</li> </ul>"},{"location":"book/chapters/99-future-directions/#research-directions","title":"Research Directions","text":"<ul> <li>Formal methods adapted to agent loops (bounded proofs, verified tool contracts).   Research question: which tool contracts can be specified with pre/post-conditions that are checkable at runtime and useful in practice?   Success signal: a library of contracts where violations predict real failures, plus a measurable reduction in incident rate or replay divergence.</li> <li>Benchmarks for reproducible autonomy (replay success, attribution accuracy).   Research question: what benchmark design makes \u201creplay success\u201d meaningful across runtimes without masking nondeterminism?   Success signal: standardized benchmark suites where independent implementations reach high replay agreement and can localize regressions to a tool, policy, or model change.</li> <li>Org-scale governance patterns and \u201cpolicy drift\u201d detection.   Research question: what signals in traces reliably indicate policy drift (changes in decision boundaries or rule application) before incidents occur?   Success signal: drift detectors with low false positives that catch policy regressions in canaries and prevent broad rollouts.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/","title":"Harness Design Patterns","text":""},{"location":"book/patterns/harness-design-patterns/#context","title":"Context","text":"<p>A model alone is a general-purpose component. Production behavior is shaped by the harness: prompts, tools, memory, budgets, verification, and traceability.</p>"},{"location":"book/patterns/harness-design-patterns/#problem","title":"Problem","text":"<p>How do you design a harness that is reliable, debuggable, and governable without building a fragile tangle of special cases?</p>"},{"location":"book/patterns/harness-design-patterns/#forces","title":"Forces","text":"<ul> <li>Constraints vs. coverage: adding constraints improves safety but can reduce task coverage.</li> <li>Tools vs. surface area: more tools increase capability but enlarge the debug and governance surface.</li> <li>Memory vs. drift/privacy: continuity improves with memory, but so do drift and data handling risks.</li> <li>Automation vs. blast radius: stronger automation improves throughput but increases the impact of failures.</li> <li>Observability vs. cost: better tracing and verification cost time and compute.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#solution","title":"Solution","text":"<p>Use a small set of recurring harness patterns that are easy to audit and compose.</p>"},{"location":"book/patterns/harness-design-patterns/#pattern-1-typed-tool-boundary","title":"Pattern 1: Typed Tool Boundary","text":"<ul> <li>Idea: tools are the only way to cause side effects, and each tool has a typed schema with explicit errors.</li> <li>Why it works: reduces ambiguity and makes traces auditable.</li> <li>Example: <code>create_file(path, content)</code> returns <code>created | updated | no_op</code>, plus a checksum.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#pattern-2-budgeted-control-stepstimecost","title":"Pattern 2: Budgeted Control (Steps/Time/Cost)","text":"<ul> <li>Idea: the kernel enforces budgets; the model cannot override them.</li> <li>Why it works: turns open-ended iteration into a bounded process.</li> <li>Example: max 20 steps or 5 minutes; on exhaustion, stop with a partial report.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#pattern-3-evidence-first-completion","title":"Pattern 3: Evidence-First Completion","text":"<ul> <li>Idea: \u201cdone\u201d requires verifiable evidence (tests run, diffs applied, outputs captured).</li> <li>Why it works: prevents completion based on plausibility alone.</li> <li>Example: stop is rejected unless verification artifacts exist in the trace.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#pattern-4-narrow-context-construction","title":"Pattern 4: Narrow Context Construction","text":"<ul> <li>Idea: select only the files needed for the next action; summarize the rest.</li> <li>Why it works: reduces context bloat and keeps constraints salient.</li> <li>Example: open 2\u20134 files max, keep a rolling summary of prior steps.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#pattern-5-separation-of-duties-plan-vs-execute-vs-verify","title":"Pattern 5: Separation of Duties (Plan vs. Execute vs. Verify)","text":"<ul> <li>Idea: treat these as distinct phases with distinct constraints.</li> <li>Why it works: limits the ability to bypass controls (for example, editing policies during execution).</li> <li>Example: planning cannot call side-effect tools; verification cannot modify code.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#implementation-sketch","title":"Implementation sketch","text":"<p>Minimal harness components:</p> <ul> <li>Kernel: step loop, budgets, cancellation/timeouts, trace append.</li> <li>Tool router: allowlist + argument validation + consistent error model + idempotency support.</li> <li>Context builder: file selection + summarization policy + retrieval policy.</li> <li>Verifier: runs evals/tests and records outputs.</li> <li>Governance layer: approvals for high-risk tools, audit logs, retention/redaction rules.</li> </ul> <p>A small harness configuration can be expressed as a policy document (conceptual):</p> <pre><code>tools:\n  allowlist: [read_file, grep_search, apply_patch, get_errors, run_in_terminal]\nbudgets:\n  max_steps: 20\n  max_minutes: 5\nstop_gate:\n  require_verification: true\n  acceptable_outcomes: [\"verified\", \"blocked\"]\n</code></pre>"},{"location":"book/patterns/harness-design-patterns/#concrete-example","title":"Concrete example","text":"<p>Repo task agent that edits documentation:</p> <ul> <li>Tools: <code>read_file</code>, <code>grep_search</code>, <code>apply_patch</code>, <code>get_errors</code>.</li> <li>Budgets: 15 steps, 3 minutes.</li> <li>Stop gate: markdown checks (or at minimum a syntax/lint pass) must be clean, or the run stops as \u201cblocked\u201d with reproduction steps.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#failure-modes","title":"Failure modes","text":"<ul> <li>Tool sprawl: too many overlapping tools; selection becomes inconsistent and hard to audit.</li> <li>Hidden side effects: tools mutate state without reporting it; traces become misleading.</li> <li>Context bloat: prompts include too much content; constraints and acceptance criteria are diluted.</li> <li>Policy bypass: weak allowlists or validation enable unintended actions.</li> <li>Unmeasured changes: no evals/verification; regressions ship silently.</li> <li>Over-coupling: harness depends on brittle prompt wording instead of enforceable kernel/router constraints.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#when-not-to-use","title":"When not to use","text":"<ul> <li>Single-purpose automation where a deterministic script is simpler.</li> <li>One-off exploratory work where harness engineering overhead dominates.</li> <li>Systems without ownership/ops capacity to maintain tools, budgets, verification, and incident response.</li> </ul>"},{"location":"book/patterns/memory-architectures/","title":"Memory Architectures","text":""},{"location":"book/patterns/memory-architectures/#context","title":"Context","text":"<p>Agents are limited by context window, variability across runs, and the need to operate over long-lived projects. Memory mechanisms can improve continuity, but they also introduce drift, privacy risk, and debugging complexity.</p>"},{"location":"book/patterns/memory-architectures/#problem","title":"Problem","text":"<p>How do you add memory so the system remains reproducible and governable?</p>"},{"location":"book/patterns/memory-architectures/#forces","title":"Forces","text":"<ul> <li>Recall vs. precision: retrieving more increases coverage but adds noise.</li> <li>Freshness vs. stability: updating memory improves relevance but can rewrite history.</li> <li>Privacy vs. utility: storing more can leak sensitive data and expand retention obligations.</li> <li>Debuggability: implicit retrieval is harder to reason about than explicit records.</li> <li>Versioning: memory must evolve with code; unversioned memory becomes a hidden dependency.</li> </ul>"},{"location":"book/patterns/memory-architectures/#solution","title":"Solution","text":"<p>Prefer layered, explicit memory with clear scopes, schemas, and write rules.</p>"},{"location":"book/patterns/memory-architectures/#layer-1-run-local-working-memory-scratch","title":"Layer 1: Run-local working memory (scratch)","text":"<ul> <li>What: transient notes, intermediate calculations, short summaries.</li> <li>Scope: one run.</li> <li>Write rule: always safe to overwrite; never treated as durable truth.</li> <li>Example: \u201cFiles touched: A, B. Hypothesis: failing test caused by null handling.\u201d</li> </ul>"},{"location":"book/patterns/memory-architectures/#layer-2-session-memory-task-state","title":"Layer 2: Session memory (task state)","text":"<ul> <li>What: structured state for a multi-step task (checklists, open questions, next steps).</li> <li>Scope: until task completion.</li> <li>Write rule: update on each step; clear on task close.</li> <li>Example: a JSON task record containing acceptance criteria and verification status.</li> </ul>"},{"location":"book/patterns/memory-architectures/#layer-3-project-memory-durable-facts","title":"Layer 3: Project memory (durable facts)","text":"<ul> <li>What: stable, reviewable records: architecture decisions, interface contracts, runbooks.</li> <li>Scope: long-lived.</li> <li>Write rule: write only after verification passes and with a source-of-truth reference.</li> <li>Example: ADR-style entries with links to code and traces.</li> </ul>"},{"location":"book/patterns/memory-architectures/#layer-4-retrieval-index-searchable-corpus","title":"Layer 4: Retrieval index (searchable corpus)","text":"<ul> <li>What: embeddings or keyword index over docs/issues/traces.</li> <li>Scope: long-lived, but treated as derived data.</li> <li>Write rule: rebuildable; never the only place a critical fact exists.</li> <li>Example: \u201cretrieve top 5 related incidents\u201d feeding short excerpts into context.</li> </ul>"},{"location":"book/patterns/memory-architectures/#implementation-sketch","title":"Implementation sketch","text":"<p>Write rules that keep memory auditable and safe:</p> <ul> <li>Only write durable memory after verification passes.</li> <li>Store sources (file paths, URLs, trace IDs, commit hashes) with each memory item.</li> <li>Separate schemas for different types of memory: facts, decisions, preferences, open questions.</li> <li>Treat retrieval as a hint; require confirmation against sources for critical claims.</li> <li>Version memory alongside the system (or tie it to a release identifier).</li> <li>Support redaction and retention policies (delete by scope, delete by source, delete by time).</li> </ul> <p>Example durable-memory record schema (conceptual):</p> <pre><code>{\n  \"type\": \"decision\",\n  \"title\": \"Prefer golden tests for CLI help output\",\n  \"status\": \"accepted\",\n  \"sources\": [\"docs/cli.md\", \"trace:2026-02-18T10:14Z\"],\n  \"rationale\": \"Help output is user-facing and easy to regress\",\n  \"verified_by\": [\"npm test\", \"snapshot update reviewed\"],\n  \"created_at\": \"2026-02-18\"\n}\n</code></pre>"},{"location":"book/patterns/memory-architectures/#concrete-example","title":"Concrete example","text":"<p>Bugfix agent memory layout:</p> <ul> <li>Run-local: stack trace notes and hypotheses.</li> <li>Session: checklist of reproduction steps + test plan + files changed.</li> <li>Project: \u201cRoot cause and fix\u201d note linked to the failing test and the patch.</li> <li>Retrieval: search prior traces for similar failure signatures.</li> </ul>"},{"location":"book/patterns/memory-architectures/#failure-modes","title":"Failure modes","text":"<ul> <li>Stale memory: outdated assumptions persist after refactors; fixes target the wrong code.</li> <li>Memory poisoning: incorrect entries are stored as facts and bias future actions.</li> <li>Over-retrieval: too many irrelevant items drown the signal and dilute constraints.</li> <li>Silent mutation: memory is updated without review; history is effectively rewritten.</li> <li>Unversioned dependency: behavior depends on memory that is not tied to code/version.</li> <li>Privacy leakage: sensitive content is stored, retrieved, or logged without appropriate handling.</li> </ul>"},{"location":"book/patterns/memory-architectures/#when-not-to-use","title":"When not to use","text":"<ul> <li>Short-lived tasks where the context window is sufficient.</li> <li>High-sensitivity domains without a clear retention/redaction policy.</li> <li>Systems that require strict reproducibility but cannot version memory with code.</li> </ul>"},{"location":"book/patterns/minimal-agent-loop/","title":"Minimal Agent Loop","text":""},{"location":"book/patterns/minimal-agent-loop/#context","title":"Context","text":"<p>You need iterative work (planning + tool use + feedback) inside an engineering environment (repo, CI, tickets, docs). The primary risk is uncontrolled complexity: additional autonomy without corresponding observability and verification.</p>"},{"location":"book/patterns/minimal-agent-loop/#problem","title":"Problem","text":"<p>How do you get useful autonomous work while keeping the control surface small enough to debug, test, and govern?</p>"},{"location":"book/patterns/minimal-agent-loop/#forces","title":"Forces","text":"<ul> <li>Capability vs. determinism: richer loops solve more tasks but increase variance across runs.</li> <li>Observability vs. speed: more logging and checks slow iteration, but missing data makes incidents expensive.</li> <li>Safety vs. throughput: tighter constraints reduce risk but may block progress on ambiguous tasks.</li> <li>Tool side effects: tools mutate state; retries can duplicate actions unless idempotent.</li> <li>Context limits: the loop must decide what to read, summarize, and omit.</li> </ul>"},{"location":"book/patterns/minimal-agent-loop/#solution","title":"Solution","text":"<p>Implement the smallest loop that can:</p> <ol> <li>Load state (workspace snapshot + budgets + any session memory).</li> <li>Produce exactly one next action using a typed schema (tool call or stop).</li> <li>Execute the action with timeouts, error normalization, and side-effect capture.</li> <li>Append a structured event to a trace.</li> <li>Update state and budgets.</li> <li>Stop only on explicit conditions (success, budget exhausted, or blocked).</li> </ol> <p>Keep loop policy simple. Push sophistication into tools (typed boundaries) and verification (tests/evals).</p>"},{"location":"book/patterns/minimal-agent-loop/#implementation-sketch","title":"Implementation sketch","text":"<p>A minimal state machine:</p> <ul> <li>Inputs: goal, repo root, tool allowlist, budgets (steps/time/cost), memory handles.</li> <li>Per-step:</li> <li>Build a narrow context (selected files + short trace summary + constraints).</li> <li>Ask the model for <code>NextAction</code>.</li> <li>If <code>NextAction.type == stop</code>: return a result object (including verification evidence or explicit \u201cblocked\u201d).</li> <li>Otherwise: run the tool, capture outputs and observable side effects.</li> <li>Append the event to the trace.</li> <li>Update budgets; stop if exceeded.</li> </ul> <p>Conceptual <code>NextAction</code> schema:</p> <pre><code>{\n  \"type\": \"tool\" ,\n  \"toolName\": \"apply_patch\",\n  \"args\": { \"input\": \"...\", \"explanation\": \"...\" },\n  \"expectedOutcome\": \"File X updated; markdown lint passes\",\n  \"stopCondition\": \"After lint passes\",\n  \"risk\": \"low\"\n}\n</code></pre> <p>Practical kernel requirements:</p> <ul> <li>Enforce budgets outside the model.</li> <li>Validate tool arguments against schemas.</li> <li>Normalize errors (timeout vs. validation vs. runtime error).</li> <li>Record a trace event with enough data to reproduce the step.</li> <li>For side-effect tools, support an idempotency key (or a safe \u201cdry run\u201d mode) where possible.</li> </ul>"},{"location":"book/patterns/minimal-agent-loop/#concrete-example","title":"Concrete example","text":"<p>Goal: \u201cAdd two glossary terms and ensure formatting.\u201d</p> <ol> <li><code>read_file(book/glossary.md)</code> to learn current format.</li> <li><code>apply_patch</code> to add the terms.</li> <li>Run a markdown check (or at minimum <code>get_errors</code>) and record output.</li> <li>Stop only after the check is clean; otherwise attempt a bounded fix.</li> </ol> <p>An example trace event for step 2:</p> <pre><code>step: 2\naction:\n  type: tool\n  tool: apply_patch\n  expected_outcome: \"Glossary updated with Acceptance criteria and Budget\"\nresult:\n  changed_files:\n    - book/glossary.md\n  tool_exit: success\nbudgets:\n  steps_remaining: 17\n</code></pre>"},{"location":"book/patterns/minimal-agent-loop/#failure-modes","title":"Failure modes","text":"<ul> <li>Missing termination criteria: the loop continues despite completion; budgets are consumed.</li> <li>Ambiguous tool failures: errors are treated as partial success; corrupted state accumulates.</li> <li>Silent side effects: tools mutate state without reporting what changed; traces cannot explain outcomes.</li> <li>Non-idempotent retries: a retry duplicates actions (double-writes, duplicate tickets, repeated API calls).</li> <li>Context bloat: the loop pulls in too much content; constraints are ignored or diluted.</li> <li>Planning without execution: repeated re-planning without tool calls; no measurable progress.</li> </ul>"},{"location":"book/patterns/minimal-agent-loop/#when-not-to-use","title":"When not to use","text":"<ul> <li>The task is deterministic and can be solved with a single script/command.</li> <li>Side effects are unacceptable without human approval (e.g., production mutations).</li> <li>You cannot capture traces or run verification; you will be unable to debug or detect drift.</li> </ul>"},{"location":"book/patterns/self-verification-loop/","title":"Self-Verification Loop","text":""},{"location":"book/patterns/self-verification-loop/#context","title":"Context","text":"<p>Model outputs often look plausible but can be wrong in subtle ways: incorrect assumptions about repo structure, stale APIs, missing edge cases, or incomplete updates across files. In engineering work, \u201csounds right\u201d is not an acceptance criterion.</p>"},{"location":"book/patterns/self-verification-loop/#problem","title":"Problem","text":"<p>How do you force the system to prove work against objective checks before it declares completion?</p>"},{"location":"book/patterns/self-verification-loop/#forces","title":"Forces","text":"<ul> <li>Verification cost must be lower than expected rework cost.</li> <li>Signal alignment: checks must reflect real acceptance criteria, not proxy metrics.</li> <li>Check gaming: if checks are narrow, the system may satisfy them while violating intent.</li> <li>Flakiness: verification tools can fail nondeterministically (network, timing, unstable tests).</li> <li>Side-effect boundaries: verification should not introduce additional risky mutations.</li> </ul>"},{"location":"book/patterns/self-verification-loop/#solution","title":"Solution","text":"<p>Make verification an explicit, mandatory phase with a stop gate:</p> <ol> <li>Define acceptance checks (tests, lint, build, schema validation, golden diffs) and/or a bounded human-review checklist.</li> <li>Require evidence in the trace: commands run, outputs captured, and artifacts produced.</li> <li>Gate completion: the system may only stop when checks pass, or when it produces a bounded \u201cblocked\u201d report with reproduction steps and the smallest viable next action.</li> </ol> <p>The key behavior change is procedural: \u201cdone\u201d becomes a claim that must be backed by artifacts.</p>"},{"location":"book/patterns/self-verification-loop/#implementation-sketch","title":"Implementation sketch","text":"<p>Use a two-part contract per task:</p> <ul> <li>Work: changes made (patches/files) and the intended behavior.</li> <li>Verification: list of checks and their outcomes, including raw outputs or pointers to captured logs.</li> </ul> <p>Suggested verification record shape:</p> <pre><code>verification:\n  - check: \"unit tests\"\n    command: \"npm test\"\n    status: pass\n    evidence: \"stdout excerpt or attached log\"\n  - check: \"lint\"\n    command: \"npm run lint\"\n    status: fail\n    evidence: \"...\"\n    next_action: \"Fix unused import in src/cli.ts\"\n</code></pre> <p>Practical gating logic:</p> <ul> <li>If checks pass: stop.</li> <li>If checks fail with actionable errors: attempt bounded repair (for example, up to 2 iterations).</li> <li>If failures are environmental/flaky: stop with a \u201cblocked\u201d report and clear reproduction steps.</li> </ul>"},{"location":"book/patterns/self-verification-loop/#concrete-example","title":"Concrete example","text":"<p>Task: \u201cAdd a new CLI flag and update docs.\u201d</p> <p>Work:</p> <ul> <li>Implement parsing for <code>--format json</code>.</li> <li>Update <code>README</code> usage section.</li> </ul> <p>Verification:</p> <ul> <li>Run unit tests that cover the new flag.</li> <li>Run a help-text snapshot (golden file) test.</li> <li>Run linter/formatter.</li> </ul> <p>Example evidence-oriented trace snippet:</p> <pre><code>$ mycli --help\n... includes \"--format\" ...\n\n$ npm test\nPASS cli.test.ts (12 tests)\n\n$ npm run lint\n0 problems\n</code></pre>"},{"location":"book/patterns/self-verification-loop/#failure-modes","title":"Failure modes","text":"<ul> <li>Rubber-stamp verification: the system asserts checks passed without running them or without capturing outputs.</li> <li>Proxy mismatch: checks pass but requirements are unmet (acceptance criteria were incomplete or untested).</li> <li>Check gaming: tests are modified to match incorrect behavior; the suite becomes less meaningful.</li> <li>Flaky verification loop: intermittent failures trigger repeated repairs and wasted budget.</li> <li>Over-verification: too many slow checks push verification out of the critical path and encourage skipping.</li> <li>Unsafe verification: verification steps include side-effectful actions (publishing, migrations) without approvals.</li> </ul>"},{"location":"book/patterns/self-verification-loop/#when-not-to-use","title":"When not to use","text":"<ul> <li>Low-impact drafts where verification cost dominates (early outlines, brainstorming, rough notes).</li> <li>Environments where checks cannot run (missing tooling or permissions) and no acceptable substitutes exist.</li> <li>Tasks where human judgment is the primary signal and objective checks are weak (copy tone, early design exploration).</li> </ul>"}]}