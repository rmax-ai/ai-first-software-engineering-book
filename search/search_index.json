{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AI-First Software Engineering Welcome to the public documentation for the AI-first software engineering book. This site publishes the current working drafts, glossaries, patterns, and operational governance that power the book\u2019s ongoing iteration loop. Contents Preface \u2013 foundational motivation and thesis for the AI-first engineering approach. Chapters \u2013 structured research chapters covering paradigm shifts, harness engineering, autonomous kernels, memory systems, evaluation, governance, and production infrastructure. Patterns \u2013 reusable engineering patterns curated from the repository. Glossary \u2013 operational definitions and terminology used throughout the book. Publishing workflow Chapters are drafted in book/chapters/ with deterministic loops enforced by state/kernel.py . The MkDocs configuration references the book files directly so every build reflects the latest chapter revisions. Use mkdocs build to produce the static site or mkdocs serve for a preview server before deployment. Keep governance documents ( CONSTITUTION.md , AGENTS.md ) and evaluation rules ( evals/ ) aligned with the book\u2019s stated principles. Quick commands mkdocs build # Generate the static site mkdocs serve # Start a local preview server mkdocs gh-deploy # Publish to GitHub Pages (configure remote) Project tree mkdocs.yml # Site configuration for the book docs/index.md # Site homepage (this file) book/chapters/*.md # Chapter drafts consumed by MkDocs navigation book/patterns/ # Referenced pattern library book/glossary.md # Detailed glossary definitions CONSTITUTION.md, AGENTS.md # Governance that shapes the book process evals/*.yaml # Evaluation contracts enforced via state/kernel.py","title":"Home"},{"location":"#ai-first-software-engineering","text":"Welcome to the public documentation for the AI-first software engineering book. This site publishes the current working drafts, glossaries, patterns, and operational governance that power the book\u2019s ongoing iteration loop.","title":"AI-First Software Engineering"},{"location":"#contents","text":"Preface \u2013 foundational motivation and thesis for the AI-first engineering approach. Chapters \u2013 structured research chapters covering paradigm shifts, harness engineering, autonomous kernels, memory systems, evaluation, governance, and production infrastructure. Patterns \u2013 reusable engineering patterns curated from the repository. Glossary \u2013 operational definitions and terminology used throughout the book.","title":"Contents"},{"location":"#publishing-workflow","text":"Chapters are drafted in book/chapters/ with deterministic loops enforced by state/kernel.py . The MkDocs configuration references the book files directly so every build reflects the latest chapter revisions. Use mkdocs build to produce the static site or mkdocs serve for a preview server before deployment. Keep governance documents ( CONSTITUTION.md , AGENTS.md ) and evaluation rules ( evals/ ) aligned with the book\u2019s stated principles.","title":"Publishing workflow"},{"location":"#quick-commands","text":"mkdocs build # Generate the static site mkdocs serve # Start a local preview server mkdocs gh-deploy # Publish to GitHub Pages (configure remote)","title":"Quick commands"},{"location":"#project-tree","text":"mkdocs.yml # Site configuration for the book docs/index.md # Site homepage (this file) book/chapters/*.md # Chapter drafts consumed by MkDocs navigation book/patterns/ # Referenced pattern library book/glossary.md # Detailed glossary definitions CONSTITUTION.md, AGENTS.md # Governance that shapes the book process evals/*.yaml # Evaluation contracts enforced via state/kernel.py","title":"Project tree"},{"location":"book/glossary/","text":"Glossary This glossary defines terms as used in this book. Definitions are intentionally operational: they describe what a term does in a system. Acceptance criteria Explicit, testable conditions that define \u201cdone\u201d for a task. In practice, acceptance criteria should map to checks (tests, lint, build, schema validation, golden diffs) or to a bounded human-review checklist. Agent loop A control loop that repeatedly: (1) observes state, (2) chooses a next action (often a tool call), (3) executes, (4) updates state, (5) decides whether to stop. Typical loop variables include a step budget, an allowlist of tools, and a termination condition. Allowlist A set of explicitly permitted actions or tools. Allowlists reduce accidental capability expansion: adding a new tool is a deliberate change that can be reviewed, tested, and governed. Budget A hard limit enforced by the kernel (or orchestration layer) on steps, time, tokens, or cost. Budgets turn \u201ctry until it works\u201d into a bounded process. Context window The maximum amount of text (messages + tool outputs + retrieved memory) that a model can condition on at once. Design implication: systems must choose what to include, summarize, or omit. Determinism The degree to which the system produces the same outcome given the same inputs. In agentic systems, determinism is usually approached through constraints, typed tools, and verification rather than assumed. Drift Unintended change in behavior over time relative to a baseline. Drift can be caused by model updates, prompt edits, tool/API changes, data changes, environment changes, or accumulating memory. Drift is detected by comparing traces or eval results across versions. Evidence Artifacts that support a claim about system behavior. Examples: a test run output, a checksum of an applied patch, a trace segment showing tool inputs/outputs, a golden diff. Eval A repeatable test that measures whether a system meets a target behavior under defined inputs and constraints. Evals can be automated (unit tests, golden files, scripted scenarios) or human-scored, but they must be versioned and runnable. Failure mode A specific way the system can fail, including its trigger and observable symptoms. Failure modes drive targeted mitigations (tool contracts, guardrails, tests) rather than general caution. Golden file A stored \u201cexpected output\u201d used for regression testing. Golden tests are useful for CLI help text, formatted outputs, and traces; they must be reviewed carefully because updating them can mask regressions. Governance Rules, controls, and escalation paths that constrain and audit agent behavior. Governance commonly includes: tool allowlists, budgets, approval gates, logging requirements, data handling policies, incident response, and rollback procedures. Harness The engineered environment around a model that makes behavior reliable and useful in a specific context. A harness typically includes: prompts, tool schemas, routing policies, memory strategy, eval suite, tracing/telemetry, and release discipline. Most production reliability lives in the harness rather than in the model. Idempotency A property of an operation where repeating it produces the same final state as running it once. Idempotency matters for retries: without it, transient failures can cause duplicated side effects. Kernel The minimal execution substrate that runs the agent loop and mediates interaction with the outside world. A kernel is responsible for: step control, tool invocation, persistence boundaries, cancellation/timeouts, and trace logging. It should be small enough to audit. Memory Persisted or semi-persisted state used to condition future behavior. Memory may be transient (within a run), session-scoped, or long-lived. It may be explicit (structured records) or implicit (retrieval index). Memory is a mechanism for reintroducing prior information into context; it is not automatically correct. Non-determinism Variation in outcomes across runs due to sampling, tool timing, nondeterministic tests, external APIs, or changing environments. Managing nondeterminism is a core engineering task in agentic systems. Policy An explicit rule set that constrains behavior. Policies can be encoded in prompts, tool routers, allowlists, budgets, and approval workflows. Effective policies are testable and observable. Prompt The structured input (system/developer/user messages and other context) used to condition the model. Prompting is part of the harness; changes to prompts should be versioned and evaluated like code. Retrieval Selecting external information (documents, traces, code snippets) to include in context for a step. Retrieval must be treated as a hint: critical claims still require confirmation against source-of-truth artifacts. RAG (retrieval-augmented generation) A pattern where the system retrieves relevant documents and includes them in the prompt before generating output. In engineering systems, RAG commonly retrieves code, docs, tickets, and prior traces. Side effect Any operation that changes state outside the model\u2019s messages. Examples: writing files, calling APIs, creating tickets, merging PRs, or modifying databases. Tool interface The contract between the agent and an external capability. A tool interface specifies: inputs, outputs, errors, side effects, idempotency, latency expectations, and permission scope. Good tool interfaces make failure explicit and reduce ambiguity. Trace A structured record of an agent run that is sufficient to reconstruct what happened and why. A trace typically includes: prompts/messages, tool calls and results, intermediate decisions, budgets, timing, and final outputs. Traces are used for debugging, eval attribution, and governance. Verification Objective checks that the system runs (or produces as artifacts) to validate that an output meets acceptance criteria. Verification is stronger than self-review: it produces evidence.","title":"Glossary"},{"location":"book/glossary/#glossary","text":"This glossary defines terms as used in this book. Definitions are intentionally operational: they describe what a term does in a system.","title":"Glossary"},{"location":"book/glossary/#acceptance-criteria","text":"Explicit, testable conditions that define \u201cdone\u201d for a task. In practice, acceptance criteria should map to checks (tests, lint, build, schema validation, golden diffs) or to a bounded human-review checklist.","title":"Acceptance criteria"},{"location":"book/glossary/#agent-loop","text":"A control loop that repeatedly: (1) observes state, (2) chooses a next action (often a tool call), (3) executes, (4) updates state, (5) decides whether to stop. Typical loop variables include a step budget, an allowlist of tools, and a termination condition.","title":"Agent loop"},{"location":"book/glossary/#allowlist","text":"A set of explicitly permitted actions or tools. Allowlists reduce accidental capability expansion: adding a new tool is a deliberate change that can be reviewed, tested, and governed.","title":"Allowlist"},{"location":"book/glossary/#budget","text":"A hard limit enforced by the kernel (or orchestration layer) on steps, time, tokens, or cost. Budgets turn \u201ctry until it works\u201d into a bounded process.","title":"Budget"},{"location":"book/glossary/#context-window","text":"The maximum amount of text (messages + tool outputs + retrieved memory) that a model can condition on at once. Design implication: systems must choose what to include, summarize, or omit.","title":"Context window"},{"location":"book/glossary/#determinism","text":"The degree to which the system produces the same outcome given the same inputs. In agentic systems, determinism is usually approached through constraints, typed tools, and verification rather than assumed.","title":"Determinism"},{"location":"book/glossary/#drift","text":"Unintended change in behavior over time relative to a baseline. Drift can be caused by model updates, prompt edits, tool/API changes, data changes, environment changes, or accumulating memory. Drift is detected by comparing traces or eval results across versions.","title":"Drift"},{"location":"book/glossary/#evidence","text":"Artifacts that support a claim about system behavior. Examples: a test run output, a checksum of an applied patch, a trace segment showing tool inputs/outputs, a golden diff.","title":"Evidence"},{"location":"book/glossary/#eval","text":"A repeatable test that measures whether a system meets a target behavior under defined inputs and constraints. Evals can be automated (unit tests, golden files, scripted scenarios) or human-scored, but they must be versioned and runnable.","title":"Eval"},{"location":"book/glossary/#failure-mode","text":"A specific way the system can fail, including its trigger and observable symptoms. Failure modes drive targeted mitigations (tool contracts, guardrails, tests) rather than general caution.","title":"Failure mode"},{"location":"book/glossary/#golden-file","text":"A stored \u201cexpected output\u201d used for regression testing. Golden tests are useful for CLI help text, formatted outputs, and traces; they must be reviewed carefully because updating them can mask regressions.","title":"Golden file"},{"location":"book/glossary/#governance","text":"Rules, controls, and escalation paths that constrain and audit agent behavior. Governance commonly includes: tool allowlists, budgets, approval gates, logging requirements, data handling policies, incident response, and rollback procedures.","title":"Governance"},{"location":"book/glossary/#harness","text":"The engineered environment around a model that makes behavior reliable and useful in a specific context. A harness typically includes: prompts, tool schemas, routing policies, memory strategy, eval suite, tracing/telemetry, and release discipline. Most production reliability lives in the harness rather than in the model.","title":"Harness"},{"location":"book/glossary/#idempotency","text":"A property of an operation where repeating it produces the same final state as running it once. Idempotency matters for retries: without it, transient failures can cause duplicated side effects.","title":"Idempotency"},{"location":"book/glossary/#kernel","text":"The minimal execution substrate that runs the agent loop and mediates interaction with the outside world. A kernel is responsible for: step control, tool invocation, persistence boundaries, cancellation/timeouts, and trace logging. It should be small enough to audit.","title":"Kernel"},{"location":"book/glossary/#memory","text":"Persisted or semi-persisted state used to condition future behavior. Memory may be transient (within a run), session-scoped, or long-lived. It may be explicit (structured records) or implicit (retrieval index). Memory is a mechanism for reintroducing prior information into context; it is not automatically correct.","title":"Memory"},{"location":"book/glossary/#non-determinism","text":"Variation in outcomes across runs due to sampling, tool timing, nondeterministic tests, external APIs, or changing environments. Managing nondeterminism is a core engineering task in agentic systems.","title":"Non-determinism"},{"location":"book/glossary/#policy","text":"An explicit rule set that constrains behavior. Policies can be encoded in prompts, tool routers, allowlists, budgets, and approval workflows. Effective policies are testable and observable.","title":"Policy"},{"location":"book/glossary/#prompt","text":"The structured input (system/developer/user messages and other context) used to condition the model. Prompting is part of the harness; changes to prompts should be versioned and evaluated like code.","title":"Prompt"},{"location":"book/glossary/#retrieval","text":"Selecting external information (documents, traces, code snippets) to include in context for a step. Retrieval must be treated as a hint: critical claims still require confirmation against source-of-truth artifacts.","title":"Retrieval"},{"location":"book/glossary/#rag-retrieval-augmented-generation","text":"A pattern where the system retrieves relevant documents and includes them in the prompt before generating output. In engineering systems, RAG commonly retrieves code, docs, tickets, and prior traces.","title":"RAG (retrieval-augmented generation)"},{"location":"book/glossary/#side-effect","text":"Any operation that changes state outside the model\u2019s messages. Examples: writing files, calling APIs, creating tickets, merging PRs, or modifying databases.","title":"Side effect"},{"location":"book/glossary/#tool-interface","text":"The contract between the agent and an external capability. A tool interface specifies: inputs, outputs, errors, side effects, idempotency, latency expectations, and permission scope. Good tool interfaces make failure explicit and reduce ambiguity.","title":"Tool interface"},{"location":"book/glossary/#trace","text":"A structured record of an agent run that is sufficient to reconstruct what happened and why. A trace typically includes: prompts/messages, tool calls and results, intermediate decisions, budgets, timing, and final outputs. Traces are used for debugging, eval attribution, and governance.","title":"Trace"},{"location":"book/glossary/#verification","text":"Objective checks that the system runs (or produces as artifacts) to validate that an output meets acceptance criteria. Verification is stronger than self-review: it produces evidence.","title":"Verification"},{"location":"book/preface/","text":"Preface This book treats AI-first software engineering as an engineering discipline rather than a product feature. Scope This book focuses on system design for AI-assisted and agentic development : Harness design: tool contracts, constraints, budgets, evaluation gates, and traces. Operational reliability: reproducibility, attribution, rollback, and incident response. Governance: permissions, protected surfaces, and enforcement via tooling/CI. Memory as an engineered subsystem: provenance, retention, correction, and drift control. This book does not attempt to: Train foundation models or discuss model internals beyond what is necessary to reason about system behavior. Provide a survey of all agent frameworks; patterns are described in terms of interfaces and invariants. Substitute evaluation with plausibility; \u201cdone\u201d requires evidence. Key distinction: model vs harness Model : the reasoning component that proposes plans and edits. Harness : the execution and control environment (tools, policies, evaluation, tracing, state). A recurring hypothesis in the chapters is that many reliability gains in practice are harness-induced: schema design, verification discipline, and traceability change outcomes even when the model is unchanged. What the repository demonstrates The repository is structured to make book development itself a reproducible agent loop: Governance is defined in CONSTITUTION.md and AGENTS.md . Chapter quality, drift signals, and style guardrails are declared in evals/ . Iteration state is recorded in state/ . The intention is to make each chapter a testable unit: clear thesis, system breakdown, concrete examples, trade-offs, failure modes, and research directions. How to read Start with the chapter that matches your immediate constraint (evaluation, governance, infra). Use book/glossary.md to disambiguate terms. Treat pattern documents in book/patterns/ as reusable design primitives.","title":"Preface"},{"location":"book/preface/#preface","text":"This book treats AI-first software engineering as an engineering discipline rather than a product feature.","title":"Preface"},{"location":"book/preface/#scope","text":"This book focuses on system design for AI-assisted and agentic development : Harness design: tool contracts, constraints, budgets, evaluation gates, and traces. Operational reliability: reproducibility, attribution, rollback, and incident response. Governance: permissions, protected surfaces, and enforcement via tooling/CI. Memory as an engineered subsystem: provenance, retention, correction, and drift control. This book does not attempt to: Train foundation models or discuss model internals beyond what is necessary to reason about system behavior. Provide a survey of all agent frameworks; patterns are described in terms of interfaces and invariants. Substitute evaluation with plausibility; \u201cdone\u201d requires evidence.","title":"Scope"},{"location":"book/preface/#key-distinction-model-vs-harness","text":"Model : the reasoning component that proposes plans and edits. Harness : the execution and control environment (tools, policies, evaluation, tracing, state). A recurring hypothesis in the chapters is that many reliability gains in practice are harness-induced: schema design, verification discipline, and traceability change outcomes even when the model is unchanged.","title":"Key distinction: model vs harness"},{"location":"book/preface/#what-the-repository-demonstrates","text":"The repository is structured to make book development itself a reproducible agent loop: Governance is defined in CONSTITUTION.md and AGENTS.md . Chapter quality, drift signals, and style guardrails are declared in evals/ . Iteration state is recorded in state/ . The intention is to make each chapter a testable unit: clear thesis, system breakdown, concrete examples, trade-offs, failure modes, and research directions.","title":"What the repository demonstrates"},{"location":"book/preface/#how-to-read","text":"Start with the chapter that matches your immediate constraint (evaluation, governance, infra). Use book/glossary.md to disambiguate terms. Treat pattern documents in book/patterns/ as reusable design primitives.","title":"How to read"},{"location":"book/chapters/01-paradigm-shift/","text":"Chapter 01 \u2014 Paradigm Shift Thesis AI-first software engineering is an architectural inversion: machine reasoning becomes a primary execution substrate, and the harness (tools, constraints, evaluation, traceability) becomes the primary design surface. The inversion is practical: reliability comes from constraints, evaluations, and traces that turn generated changes into a repeatable, testable loop. A concrete, testable implication: holding the model constant, improving the harness should predictably reduce iteration count, increase pass rates, and improve failure attribution. Operational definition: - Model capability changes when you swap models while holding tools, constraints, and evaluation constant. - Harness capability changes when you keep the model constant but alter tools, policies, evaluation gates, or trace capture. This chapter\u2019s claim is a hypothesis: many observed \u201ccapability\u201d gains in practice are attributable to harness engineering rather than model changes. Why This Matters Without a clear boundary between model capability and harness capability, teams misattribute failures and waste effort. Reliability depends on reproducible loops (plan \u2192 act \u2192 verify) rather than isolated prompts. Production constraints (auditability, security, cost, regression control) require system design, not \u201cprompting.\u201d System Breakdown Actors : human governor, agent loop, tools/runtime, evaluation/CI. Artifacts : specs, plans, diffs, traces, eval results, decision records. Invariants (hypotheses to test): Every non-trivial change is traceable to a plan and verified by checks. The system can attribute regressions to a layer (prompt, tool, code, eval). Autonomy is gated by evaluations and budgets. Measurable signals (to separate model vs harness effects): Iterations-to-pass : number of propose\u2192verify cycles until all required checks pass. Time-to-green : wall-clock time from first attempt to passing evaluation gates. Attribution rate : fraction of failures with a clear primary cause (prompt/spec vs tool/runtime vs code vs eval). Attribution checklist (what evidence makes a failure \u201cbelong\u201d to a layer): Spec/prompt : requirement is ambiguous or contradictory; different reasonable interpretations change expected output; clarifying text resolves the failure without code changes. Tool/runtime : tool errors, timeouts, missing permissions, flaky environment, or nondeterministic command outputs; rerun under identical inputs yields different results. Code : deterministic failing tests or typechecks tied to a specific diff; reverting the diff restores the previous behavior. Eval/CI : mismatch between what is asserted and what is intended; tests are incorrect, overly strict, or missing a required case; fixing the test changes outcomes without changing product behavior. Concrete Example 1 Refactor a small library function using an agent loop. - Inputs: failing unit test + desired behavior specification (e.g., a short \u201cGiven/When/Then\u201d note checked into the repo). - Loop: propose patch \u2192 run tests \u2192 inspect diff \u2192 record trace (commands + outputs) \u2192 stop on pass. - Measured outputs: - Iterations-to-pass and time-to-green. - Diff size (files touched, lines changed) and whether changes are localized to the intended function. - Failure attribution per iteration (spec/prompt vs tool/runtime vs code vs eval) using the checklist above. - Stop rule: - Stop when the originally failing unit test passes, the full unit test suite passes, and the diff is constrained to the intended surface area. - If the loop reaches a fixed budget (e.g., N iterations or T minutes) without progress, stop and escalate to a human with the trace and the smallest reproducible failing case. Concrete Example 2 Ship a minor API change in a production service. - Inputs: API contract + backward-compat constraints + staging environment + a defined rollout/rollback policy. - Loop: generate migration plan \u2192 implement \u2192 run contract tests \u2192 produce trace report (diff + commands + results) \u2192 human approve. - Measured outputs: - Iterations-to-pass and time-to-green (from first migration-plan draft to all required checks passing in staging). - Attribution rate per iteration using the checklist above (spec/prompt vs tool/runtime vs code vs eval). - Backward-compat outcomes: number of contract-test failures introduced, and whether rollback was exercised successfully in staging. - Guardrails: - Protected paths or modules that require explicit human review before edits (e.g., auth, billing, infra). - Required checks (contract tests, integration tests, lint/typecheck, and a staging smoke test). - Rollback plan defined up front (feature flag, config switch, or revert procedure) and verified in staging. - Approval gate: no deploy until a human reviews the migration plan, the diff, and the evaluation results. - Stop rule: - Stop when all required checks pass in staging, the migration plan is consistent with backward-compat constraints, and the trace report can explain every material change. - If any guardrail is violated (protected file touched, required test skipped, rollback unclear), stop immediately and require human intervention. Trade-offs Strong harness constraints reduce freedom (and sometimes speed) but increase reproducibility. More evaluation gates reduce regressions but add compute and latency. Trace-heavy workflows improve debugging but increase storage and privacy considerations. Failure Modes Illusion of capability : improvements credited to the model when they come from better tooling/evals. Unbounded autonomy : loops run without budgets, causing tool thrash and unclear outcomes. Non-attributable failures : missing traces make regressions un-debuggable. Synthesis: treat machine reasoning as an execution substrate, and treat the harness as the primary lever for reliability. Track iterations-to-pass , time-to-green , and attribution rate to separate harness effects from model effects and to make failures actionable. Research Directions Metrics that separate model improvements from harness improvements. Minimal trace schema that supports attribution and replay. Formal definitions of autonomy envelopes and stop conditions.","title":"01. Paradigm Shift"},{"location":"book/chapters/01-paradigm-shift/#chapter-01-paradigm-shift","text":"","title":"Chapter 01 \u2014 Paradigm Shift"},{"location":"book/chapters/01-paradigm-shift/#thesis","text":"AI-first software engineering is an architectural inversion: machine reasoning becomes a primary execution substrate, and the harness (tools, constraints, evaluation, traceability) becomes the primary design surface. The inversion is practical: reliability comes from constraints, evaluations, and traces that turn generated changes into a repeatable, testable loop. A concrete, testable implication: holding the model constant, improving the harness should predictably reduce iteration count, increase pass rates, and improve failure attribution. Operational definition: - Model capability changes when you swap models while holding tools, constraints, and evaluation constant. - Harness capability changes when you keep the model constant but alter tools, policies, evaluation gates, or trace capture. This chapter\u2019s claim is a hypothesis: many observed \u201ccapability\u201d gains in practice are attributable to harness engineering rather than model changes.","title":"Thesis"},{"location":"book/chapters/01-paradigm-shift/#why-this-matters","text":"Without a clear boundary between model capability and harness capability, teams misattribute failures and waste effort. Reliability depends on reproducible loops (plan \u2192 act \u2192 verify) rather than isolated prompts. Production constraints (auditability, security, cost, regression control) require system design, not \u201cprompting.\u201d","title":"Why This Matters"},{"location":"book/chapters/01-paradigm-shift/#system-breakdown","text":"Actors : human governor, agent loop, tools/runtime, evaluation/CI. Artifacts : specs, plans, diffs, traces, eval results, decision records. Invariants (hypotheses to test): Every non-trivial change is traceable to a plan and verified by checks. The system can attribute regressions to a layer (prompt, tool, code, eval). Autonomy is gated by evaluations and budgets. Measurable signals (to separate model vs harness effects): Iterations-to-pass : number of propose\u2192verify cycles until all required checks pass. Time-to-green : wall-clock time from first attempt to passing evaluation gates. Attribution rate : fraction of failures with a clear primary cause (prompt/spec vs tool/runtime vs code vs eval). Attribution checklist (what evidence makes a failure \u201cbelong\u201d to a layer): Spec/prompt : requirement is ambiguous or contradictory; different reasonable interpretations change expected output; clarifying text resolves the failure without code changes. Tool/runtime : tool errors, timeouts, missing permissions, flaky environment, or nondeterministic command outputs; rerun under identical inputs yields different results. Code : deterministic failing tests or typechecks tied to a specific diff; reverting the diff restores the previous behavior. Eval/CI : mismatch between what is asserted and what is intended; tests are incorrect, overly strict, or missing a required case; fixing the test changes outcomes without changing product behavior.","title":"System Breakdown"},{"location":"book/chapters/01-paradigm-shift/#concrete-example-1","text":"Refactor a small library function using an agent loop. - Inputs: failing unit test + desired behavior specification (e.g., a short \u201cGiven/When/Then\u201d note checked into the repo). - Loop: propose patch \u2192 run tests \u2192 inspect diff \u2192 record trace (commands + outputs) \u2192 stop on pass. - Measured outputs: - Iterations-to-pass and time-to-green. - Diff size (files touched, lines changed) and whether changes are localized to the intended function. - Failure attribution per iteration (spec/prompt vs tool/runtime vs code vs eval) using the checklist above. - Stop rule: - Stop when the originally failing unit test passes, the full unit test suite passes, and the diff is constrained to the intended surface area. - If the loop reaches a fixed budget (e.g., N iterations or T minutes) without progress, stop and escalate to a human with the trace and the smallest reproducible failing case.","title":"Concrete Example 1"},{"location":"book/chapters/01-paradigm-shift/#concrete-example-2","text":"Ship a minor API change in a production service. - Inputs: API contract + backward-compat constraints + staging environment + a defined rollout/rollback policy. - Loop: generate migration plan \u2192 implement \u2192 run contract tests \u2192 produce trace report (diff + commands + results) \u2192 human approve. - Measured outputs: - Iterations-to-pass and time-to-green (from first migration-plan draft to all required checks passing in staging). - Attribution rate per iteration using the checklist above (spec/prompt vs tool/runtime vs code vs eval). - Backward-compat outcomes: number of contract-test failures introduced, and whether rollback was exercised successfully in staging. - Guardrails: - Protected paths or modules that require explicit human review before edits (e.g., auth, billing, infra). - Required checks (contract tests, integration tests, lint/typecheck, and a staging smoke test). - Rollback plan defined up front (feature flag, config switch, or revert procedure) and verified in staging. - Approval gate: no deploy until a human reviews the migration plan, the diff, and the evaluation results. - Stop rule: - Stop when all required checks pass in staging, the migration plan is consistent with backward-compat constraints, and the trace report can explain every material change. - If any guardrail is violated (protected file touched, required test skipped, rollback unclear), stop immediately and require human intervention.","title":"Concrete Example 2"},{"location":"book/chapters/01-paradigm-shift/#trade-offs","text":"Strong harness constraints reduce freedom (and sometimes speed) but increase reproducibility. More evaluation gates reduce regressions but add compute and latency. Trace-heavy workflows improve debugging but increase storage and privacy considerations.","title":"Trade-offs"},{"location":"book/chapters/01-paradigm-shift/#failure-modes","text":"Illusion of capability : improvements credited to the model when they come from better tooling/evals. Unbounded autonomy : loops run without budgets, causing tool thrash and unclear outcomes. Non-attributable failures : missing traces make regressions un-debuggable. Synthesis: treat machine reasoning as an execution substrate, and treat the harness as the primary lever for reliability. Track iterations-to-pass , time-to-green , and attribution rate to separate harness effects from model effects and to make failures actionable.","title":"Failure Modes"},{"location":"book/chapters/01-paradigm-shift/#research-directions","text":"Metrics that separate model improvements from harness improvements. Minimal trace schema that supports attribution and replay. Formal definitions of autonomy envelopes and stop conditions.","title":"Research Directions"},{"location":"book/chapters/02-harness-engineering/","text":"Chapter 02 \u2014 Harness Engineering Thesis Harness engineering is the discipline of turning a general-purpose model into a predictable system by defining tool contracts, constraints, loop control, and evaluation gates. In this chapter, \u201cpredictable\u201d means three things. First, the same input context tends to produce the same classes of actions (repeatability). Second, the system\u2019s allowed changes are bounded by explicit constraints (boundedness). Third, outputs can be accepted or rejected by checks rather than judgment calls (verifiability). Hypothesis (falsifiable): for a fixed set of tasks and repositories, tightening the harness (schemas, budgets, gates, and recovery rules) reduces regression rate and rework more than swapping between models of similar capability. Prompts and task definitions are held constant. Why This Matters The same model can behave reliably or unreliably depending on tool schemas, budgets, and verification. Teams can standardize harness practices even when models change. Production safety and auditability primarily live in the harness layer. System Breakdown Control plane : prompts, policies, budgets, stop conditions. Tool plane : filesystem edits, build/test runners, linters, browsers, APIs. Evaluation plane : checks as gates; regression suites; quality rubrics. State plane : task ledger, traces, decisions, artifacts. Interfaces : Tool schemas and error contracts. Patch discipline (diff-only, small changes). Evaluation API (what constitutes pass/fail). A useful way to operationalize the planes is to name what each plane consumes, what it produces, and one metric you can track: Plane Responsibilities Key artifacts (inputs/outputs) One measurable metric Control plane Decide what the agent is allowed to do and when to stop Inputs: task brief, policy, iteration/time budget. Outputs: chosen strategy, stop reason Iterations to first passing gate Tool plane Perform actions with bounded, typed interfaces Inputs: tool calls with validated args. Outputs: patches, command outputs, structured errors Patch locality (changed files/lines per task) Evaluation plane Decide if work is acceptable based on checks Inputs: build/test/lint results, rubrics. Outputs: pass/fail + evidence Gate pass rate (per iteration) State plane Record what happened and enable recovery Inputs: events, diffs, tool outputs. Outputs: trace, ledger, artifacts for replay Reproducibility rate (can rerun and get same pass/fail) The \u201cminimal contract surface\u201d is the set of interfaces that must be stable for predictability. It includes tool schemas (including error codes), patch discipline, and evaluation semantics. Concrete Example 1 Design a tool contract for \u201capply patch\u201d operations. A minimal schema sketch: Required fields: path : absolute or repo-relative file path (must exist unless create=true ) old_str : exact original text to be replaced (must match uniquely) new_str : replacement text context : optional surrounding lines to disambiguate Constraints: Reject edits that change unrelated whitespace outside the old_str region. Reject edits that expand scope beyond the file (no implicit multi-file edits). Reject edits that exceed a size budget (e.g., max changed lines per call). Error contract (examples): NOT_FOUND : path does not exist and create is false NON_UNIQUE_MATCH : old_str matches multiple locations NO_MATCH : old_str matches zero locations (stale context) BUDGET_EXCEEDED : change size violates configured limits POLICY_VIOLATION : edit touches forbidden paths or patterns Happy path flow: 1. Agent proposes a patch using a unique old_str block with minimal scope. 2. Harness validates: file exists, match is unique, diff stays within budgets. 3. Tool applies the patch and returns a structured result: changed line counts, a before/after snippet, and a stable identifier for the diff. Conflict recovery (when the patch fails with NO_MATCH or NON_UNIQUE_MATCH ): 1. The harness returns the error code plus a short \u201cfresh context\u201d snippet around the closest match (or a list of candidate match ranges). 2. The agent re-reads the relevant portion of the file and regenerates a smaller, more specific old_str (or narrows by adding context ). 3. If the second attempt fails, the harness forces a stop condition (\u201cneeds human review\u201d) rather than allowing a whole-file overwrite. Evaluation: measure whether the tool contract is improving outcomes with two task-level metrics: - Revert rate : fraction of tasks where the patch is reverted in the next N commits/PR updates. - Diff locality : median number of files touched and lines changed per task, with an alert threshold for outliers. Concrete Example 2 Add an evaluation gate to an agent loop. A minimal loop in prose looks like: attempt \u2192 gate \u2192 log \u2192 decide. 1. Attempt : the agent makes the smallest change it believes will satisfy the task. 2. Gate : the harness runs required checks (e.g., unit tests + static checks). 3. Log : record the tool outputs, the diff identifier, and the gate result in the task ledger. 4. Decide : - If gate passes: allow \u201cPR-ready\u201d output. - If gate fails: allow another iteration only if budgets remain and the failure is actionable. - If repeated failures occur: stop with a concrete failure summary and evidence. Pass/fail criteria should be explicit: - Pass = all configured checks return success (exit code 0) and no policy violations were triggered. - Fail = any check fails, any policy violation occurs, or budgets are exceeded. Fallback gate policy when tests are missing: - If unit tests are absent or non-runnable, the harness should not silently accept \u201clooks good.\u201d - Replace the unit-test requirement with a narrower, explicit fallback gate such as: - typecheck/lint/build must pass, and - a targeted command or script (documented in the repo) must run successfully, and - the change must be limited by stricter patch budgets (smaller allowed diffs). - The harness should record which gate was used ( full_gate vs fallback_gate ) so reliability metrics stay comparable over time. Trade-offs Richer tool schemas reduce ambiguity but raise integration cost. Strict budgets prevent runaway loops but can truncate legitimate work. Strong gates improve safety but may block progress on tasks lacking tests. Decision checklist (recommended defaults and when to relax): - Tool schemas : - Default: prefer typed, validated arguments plus explicit error codes for recovery. - Relax when: prototyping a new tool where integration speed matters more than repeatability, but only in non-production contexts. - Patch budgets : - Default: small, diff-only edits with per-call and per-task size limits. - Relax when: performing mechanical, reviewable migrations (e.g., dependency rename) where change breadth is intentional and measurable. - Evaluation gates : - Default: require passing tests and static checks (a minimal green CI run) before any final output. - Relax when: the repo cannot run tests in the current environment; use an explicit fallback gate and tighten patch budgets rather than skipping evaluation. - Stop conditions : - Default: stop on repeated failure modes (same check failing twice) with a structured report. - Relax when: failures are due to flaky infrastructure and you can rerun deterministically (record rerun count as part of the trace). Failure Modes Schema underspecification : tools accept ambiguous inputs, producing inconsistent outcomes. How you notice: frequent NO_MATCH / NON_UNIQUE_MATCH -style failures, large diffs for small tasks, and high variance in outcomes across reruns. Harness fix: tighten required fields, add validation (uniqueness checks, size budgets), and return structured error codes plus fresh context to guide recovery. Over-permissive harness : agents change broad parts of the repo with weak verification. How you notice: many files touched per task, drift into unrelated directories, and regressions discovered after \u201ccompletion.\u201d Harness fix: enforce patch locality budgets, add path allowlists/denylists, and require passing gates before accepting final output. Gate bypass : humans accept outputs without running checks, breaking the feedback loop. How you notice: \u201cmerged without green checks,\u201d missing logs/evidence in the ledger, and recurring regressions that gates would have caught. Harness fix: make gates non-optional for PR-ready output (policy), require artifacts (logs, diff id, check results) attached to the task, and surface \u201cstop reason\u201d when evidence is missing. Research Directions Harness quality metrics (iteration efficiency, regression rate, reproducibility). Open question: which small set of metrics best predicts long-term reliability across repos? Evaluation approach: track iterations to first passing gate, revert rate, and reproducibility rate using the plane metrics table, then compare distributions before/after harness changes. Tool error taxonomies that guide automated recovery. Open question: what error codes enable the highest \u201cself-repair\u201d rate without encouraging risky retries? Evaluation approach: measure recovery success rate per error code (e.g., % resolved within 2 retries ) and the resulting diff locality, using structured tool returns. Portable harness templates across languages and repo types. Open question: what parts of the contract surface are truly portable (schemas, budgets, gates) versus language-specific? Evaluation approach: apply a template to multiple repos, then compare reproducibility rate and gate pass rate changes while holding model choice constant.","title":"02. Harness Engineering"},{"location":"book/chapters/02-harness-engineering/#chapter-02-harness-engineering","text":"","title":"Chapter 02 \u2014 Harness Engineering"},{"location":"book/chapters/02-harness-engineering/#thesis","text":"Harness engineering is the discipline of turning a general-purpose model into a predictable system by defining tool contracts, constraints, loop control, and evaluation gates. In this chapter, \u201cpredictable\u201d means three things. First, the same input context tends to produce the same classes of actions (repeatability). Second, the system\u2019s allowed changes are bounded by explicit constraints (boundedness). Third, outputs can be accepted or rejected by checks rather than judgment calls (verifiability). Hypothesis (falsifiable): for a fixed set of tasks and repositories, tightening the harness (schemas, budgets, gates, and recovery rules) reduces regression rate and rework more than swapping between models of similar capability. Prompts and task definitions are held constant.","title":"Thesis"},{"location":"book/chapters/02-harness-engineering/#why-this-matters","text":"The same model can behave reliably or unreliably depending on tool schemas, budgets, and verification. Teams can standardize harness practices even when models change. Production safety and auditability primarily live in the harness layer.","title":"Why This Matters"},{"location":"book/chapters/02-harness-engineering/#system-breakdown","text":"Control plane : prompts, policies, budgets, stop conditions. Tool plane : filesystem edits, build/test runners, linters, browsers, APIs. Evaluation plane : checks as gates; regression suites; quality rubrics. State plane : task ledger, traces, decisions, artifacts. Interfaces : Tool schemas and error contracts. Patch discipline (diff-only, small changes). Evaluation API (what constitutes pass/fail). A useful way to operationalize the planes is to name what each plane consumes, what it produces, and one metric you can track: Plane Responsibilities Key artifacts (inputs/outputs) One measurable metric Control plane Decide what the agent is allowed to do and when to stop Inputs: task brief, policy, iteration/time budget. Outputs: chosen strategy, stop reason Iterations to first passing gate Tool plane Perform actions with bounded, typed interfaces Inputs: tool calls with validated args. Outputs: patches, command outputs, structured errors Patch locality (changed files/lines per task) Evaluation plane Decide if work is acceptable based on checks Inputs: build/test/lint results, rubrics. Outputs: pass/fail + evidence Gate pass rate (per iteration) State plane Record what happened and enable recovery Inputs: events, diffs, tool outputs. Outputs: trace, ledger, artifacts for replay Reproducibility rate (can rerun and get same pass/fail) The \u201cminimal contract surface\u201d is the set of interfaces that must be stable for predictability. It includes tool schemas (including error codes), patch discipline, and evaluation semantics.","title":"System Breakdown"},{"location":"book/chapters/02-harness-engineering/#concrete-example-1","text":"Design a tool contract for \u201capply patch\u201d operations. A minimal schema sketch: Required fields: path : absolute or repo-relative file path (must exist unless create=true ) old_str : exact original text to be replaced (must match uniquely) new_str : replacement text context : optional surrounding lines to disambiguate Constraints: Reject edits that change unrelated whitespace outside the old_str region. Reject edits that expand scope beyond the file (no implicit multi-file edits). Reject edits that exceed a size budget (e.g., max changed lines per call). Error contract (examples): NOT_FOUND : path does not exist and create is false NON_UNIQUE_MATCH : old_str matches multiple locations NO_MATCH : old_str matches zero locations (stale context) BUDGET_EXCEEDED : change size violates configured limits POLICY_VIOLATION : edit touches forbidden paths or patterns Happy path flow: 1. Agent proposes a patch using a unique old_str block with minimal scope. 2. Harness validates: file exists, match is unique, diff stays within budgets. 3. Tool applies the patch and returns a structured result: changed line counts, a before/after snippet, and a stable identifier for the diff. Conflict recovery (when the patch fails with NO_MATCH or NON_UNIQUE_MATCH ): 1. The harness returns the error code plus a short \u201cfresh context\u201d snippet around the closest match (or a list of candidate match ranges). 2. The agent re-reads the relevant portion of the file and regenerates a smaller, more specific old_str (or narrows by adding context ). 3. If the second attempt fails, the harness forces a stop condition (\u201cneeds human review\u201d) rather than allowing a whole-file overwrite. Evaluation: measure whether the tool contract is improving outcomes with two task-level metrics: - Revert rate : fraction of tasks where the patch is reverted in the next N commits/PR updates. - Diff locality : median number of files touched and lines changed per task, with an alert threshold for outliers.","title":"Concrete Example 1"},{"location":"book/chapters/02-harness-engineering/#concrete-example-2","text":"Add an evaluation gate to an agent loop. A minimal loop in prose looks like: attempt \u2192 gate \u2192 log \u2192 decide. 1. Attempt : the agent makes the smallest change it believes will satisfy the task. 2. Gate : the harness runs required checks (e.g., unit tests + static checks). 3. Log : record the tool outputs, the diff identifier, and the gate result in the task ledger. 4. Decide : - If gate passes: allow \u201cPR-ready\u201d output. - If gate fails: allow another iteration only if budgets remain and the failure is actionable. - If repeated failures occur: stop with a concrete failure summary and evidence. Pass/fail criteria should be explicit: - Pass = all configured checks return success (exit code 0) and no policy violations were triggered. - Fail = any check fails, any policy violation occurs, or budgets are exceeded. Fallback gate policy when tests are missing: - If unit tests are absent or non-runnable, the harness should not silently accept \u201clooks good.\u201d - Replace the unit-test requirement with a narrower, explicit fallback gate such as: - typecheck/lint/build must pass, and - a targeted command or script (documented in the repo) must run successfully, and - the change must be limited by stricter patch budgets (smaller allowed diffs). - The harness should record which gate was used ( full_gate vs fallback_gate ) so reliability metrics stay comparable over time.","title":"Concrete Example 2"},{"location":"book/chapters/02-harness-engineering/#trade-offs","text":"Richer tool schemas reduce ambiguity but raise integration cost. Strict budgets prevent runaway loops but can truncate legitimate work. Strong gates improve safety but may block progress on tasks lacking tests. Decision checklist (recommended defaults and when to relax): - Tool schemas : - Default: prefer typed, validated arguments plus explicit error codes for recovery. - Relax when: prototyping a new tool where integration speed matters more than repeatability, but only in non-production contexts. - Patch budgets : - Default: small, diff-only edits with per-call and per-task size limits. - Relax when: performing mechanical, reviewable migrations (e.g., dependency rename) where change breadth is intentional and measurable. - Evaluation gates : - Default: require passing tests and static checks (a minimal green CI run) before any final output. - Relax when: the repo cannot run tests in the current environment; use an explicit fallback gate and tighten patch budgets rather than skipping evaluation. - Stop conditions : - Default: stop on repeated failure modes (same check failing twice) with a structured report. - Relax when: failures are due to flaky infrastructure and you can rerun deterministically (record rerun count as part of the trace).","title":"Trade-offs"},{"location":"book/chapters/02-harness-engineering/#failure-modes","text":"Schema underspecification : tools accept ambiguous inputs, producing inconsistent outcomes. How you notice: frequent NO_MATCH / NON_UNIQUE_MATCH -style failures, large diffs for small tasks, and high variance in outcomes across reruns. Harness fix: tighten required fields, add validation (uniqueness checks, size budgets), and return structured error codes plus fresh context to guide recovery. Over-permissive harness : agents change broad parts of the repo with weak verification. How you notice: many files touched per task, drift into unrelated directories, and regressions discovered after \u201ccompletion.\u201d Harness fix: enforce patch locality budgets, add path allowlists/denylists, and require passing gates before accepting final output. Gate bypass : humans accept outputs without running checks, breaking the feedback loop. How you notice: \u201cmerged without green checks,\u201d missing logs/evidence in the ledger, and recurring regressions that gates would have caught. Harness fix: make gates non-optional for PR-ready output (policy), require artifacts (logs, diff id, check results) attached to the task, and surface \u201cstop reason\u201d when evidence is missing.","title":"Failure Modes"},{"location":"book/chapters/02-harness-engineering/#research-directions","text":"Harness quality metrics (iteration efficiency, regression rate, reproducibility). Open question: which small set of metrics best predicts long-term reliability across repos? Evaluation approach: track iterations to first passing gate, revert rate, and reproducibility rate using the plane metrics table, then compare distributions before/after harness changes. Tool error taxonomies that guide automated recovery. Open question: what error codes enable the highest \u201cself-repair\u201d rate without encouraging risky retries? Evaluation approach: measure recovery success rate per error code (e.g., % resolved within 2 retries ) and the resulting diff locality, using structured tool returns. Portable harness templates across languages and repo types. Open question: what parts of the contract surface are truly portable (schemas, budgets, gates) versus language-specific? Evaluation approach: apply a template to multiple repos, then compare reproducibility rate and gate pass rate changes while holding model choice constant.","title":"Research Directions"},{"location":"book/chapters/03-autonomous-kernels/","text":"Chapter 03 \u2014 Autonomous Kernels Thesis An autonomous kernel is a minimal, well-specified control loop that executes bounded work: plan, apply tool actions, verify, and stop. Its constraints (budgets, permissions, evaluation gates) define a safety envelope that makes outcomes inspectable and repeatable. Definitions: - Autonomous kernel : a control loop with explicit limits and explicit exit criteria; it is not \u201cgeneral autonomy,\u201d long-horizon project management, or open-ended exploration. - Budget : a hard cap on resources (iterations, elapsed time, tool calls, diff size) that prevents runaway behavior and forces escalation when progress stalls. - Evaluation gate : a required check whose result must be recorded and must be satisfied (or explicitly waived with justification) before the kernel can declare success. Hypothesis: small, well-governed autonomous kernels (tight loops with explicit budgets and evaluation gates) outperform broad autonomy in stability and debuggability. Why This Matters Most failures in agentic work are operational: runaway loops, untraceable edits, and unverifiable outcomes. Kernels enable composability: multiple kernels can run with different permissions and evaluation profiles. \u201cKernel-first\u201d design makes autonomy a system property, not a prompt trick. System Breakdown Kernel loop : intent \u2192 plan \u2192 act \u2192 verify \u2192 record trace \u2192 stop/iterate. Budgets : max iterations, time, tool calls, diff size. Permissions : read/write scopes, protected paths, allowed tools. Verification : mandatory checks per action class (e.g., tests for code changes). Persistence : ledger entries, trace logs, artifacts. To make this operational, treat each loop step as a checkpoint with a \u201cmust record\u201d trace payload and a \u201cmust decide\u201d stop condition: - Intent : state the task class and success condition (e.g., \u201ctests pass,\u201d \u201cbuild passes,\u201d \u201crepro no longer fails\u201d). - Plan : enumerate the next 1\u20133 actions only (not the whole project), each tied to a verification gate and a budget slice. - Act : perform the minimal change that addresses the current hypothesis; avoid speculative edits that cannot be evaluated. - Verify : run the smallest evaluation that is credible for the task class. Use unit tests for logic changes and typecheck/build for dependency changes; use end-to-end only when required. Verification can fail in two ways: the check fails, or the check is too narrow to detect the real regression. - Record trace : persist commands executed, files touched, diff stats, and evaluation outputs (or hashes/pointers to them) so a human can replay or audit. - Stop/iterate : stop when the success condition is met, or when a budget is exhausted, or when verification indicates the current plan cannot succeed without broader permissions/scope. Mermaid mapping of stages to controls and outputs: flowchart LR I[Intent] --> P[Plan] --> A[Act] --> V[Verify] --> R[Record trace] --> S{Stop / iterate} B[(Budgets\\niterations/time/tool calls/diff size)] -. constrains .-> P B -. constrains .-> A B -. constrains .-> V Perm[(Permissions\\nread/write scopes\\nprotected paths\\nallowed tools)] -. constrains .-> A Gate[(Evaluation gates\\nby action class)] -. required .-> V Persist[(Persistence\\nledger/trace logs/artifacts)] -. produced .-> R V -->|pass| S V -->|fail| P S -->|iterate| P S -->|stop| End[Exit with summary] A compact \u201cmust capture\u201d checklist (minimum viable trace): | Loop stage | Budget signal | Permission signal | Verification signal | Persistence artifact | |---|---|---|---|---| | intent | remaining iterations/time | required read scope | success criteria defined | intent string + criteria | | plan | tool-call budget allocation | allowed tools list | planned gates named | plan steps + gate mapping | | act | diff size consumed | write scope used | N/A | patch/diff stats | | verify | time/tool calls consumed | execution permissions | gate results (pass/fail) | command + exit code + excerpt | | record trace | N/A | N/A | N/A | ledger entry + trace pointer | | stop/iterate | budget exhausted? | permission insufficient? | gates satisfied? | final summary + next action | Concrete Example 1 Bug-fix kernel for a CLI tool. Input: failing test case: tests/test_parse.py::test_rejects_empty_input reproduction step: python -m mycli parse \"\" returns exit code 0 but should return non-zero budgets: max 3 iterations, max 10 tool calls, max 40 lines changed permissions: read src/ , write src/parser.py , run pytest -k parse Mini-runbook (a single bounded kernel run): 1. Localize failure (evidence-first) - Action: run the smallest check that reproduces the failure. - Command: pytest -k rejects_empty_input - Record: - failing assertion excerpt (placeholder): E assert 0 == 2 - environment notes: OS, Python version, CLI args - Stop/iterate rule: - If the failure does not reproduce, stop and return \u201ccannot reproduce\u201d trace (do not edit). Patch minimal surface (hypothesis-driven) Hypothesis: empty string is being treated as a valid token stream in src/parser.py . Action: make a minimal edit that rejects empty input at the boundary (not across unrelated call sites). Budget check: ensure diff size stays within 40 lines and touches only src/parser.py (or a single adjacent file if necessary). Record: files touched: src/parser.py diff stats: +6 -1 (placeholder) Run verification gate (tight but credible) Gate 1: rerun the failing test. Command: pytest -k rejects_empty_input Gate 2 (cheap regression check): run related unit tests only. Command: pytest -k parse Verification risk handling: If Gate 1 passes but Gate 2 fails, treat as \u201cnot fixed\u201d (the patch likely broke a nearby invariant). Record trace (auditable, replayable) Persist a kernel trace with: budgets consumed: iterations used, tool calls used, diff size commands executed + exit codes final test summary line (placeholder): 2 passed, 0 failed Write a ledger entry summarizing: what changed (one-sentence) why it changed (link to failing assertion) what verified it (gate list) Stop criteria (explicit) Stop success: Gate 1 and Gate 2 pass within budget. Stop failure: tool-call budget exhausted, diff budget exceeded, or verification indicates a broader refactor is required. Stop escalation output: include \u201cnext action for a human\u201d (e.g., \u201cneeds design change in tokenization; requires editing src/lexer.py , which is outside current write scope\u201d). Concrete Example 2 Dependency upgrade kernel. Input: target version: libX 4.2.0 \u2192 4.3.0 constraints: Python >=3.10 , cannot change public API, CI must stay green upgrade guide: notes a breaking rename OldClient \u2192 Client budgets: max 4 iterations, max 15 tool calls, max 120 lines changed permissions: write pyproject.toml and src/ , run python -m compileall and pytest Kernel steps with an explicit remediation branch: 1. Update manifest (narrow scope) - Action: bump version constraint in pyproject.toml . - Record: - old/new constraint strings - diff stats for manifest only - Stop/iterate rule: - If the dependency resolver cannot produce a consistent lock, stop with resolver output (do not attempt ad-hoc pinning unless that is explicitly in scope). Run a fast build/type gate before full tests Gate A (fast): import/type/compile smoke check. Command: python -m compileall src Record: exit code compile summary line (placeholder): Listing 'src'... \u2026 compileall: success (or equivalent) Interpretation: If Gate A fails, this is often a missing symbol or incompatible API that will be faster to remediate than running the full suite. Remediation branch (compile errors vs failing tests) If compile/import fails : Localize: identify first error site (file + symbol). Patch: apply the minimal mechanical fix (e.g., rename OldClient to Client ) in the smallest set of files. Verify: rerun Gate A only, then proceed. Budget guard: If more than 5 files are touched, stop and escalate (\u201crequires broader refactor\u201d). If the cumulative diff exceeds 120 lines changed, stop and escalate (\u201cexceeds change budget for this kernel\u201d). If compile passes but tests fail : Localize: run the single failing test file or test case. Patch: address behavioral change (e.g., new default timeout) with a targeted adjustment and a justification in the trace. Verify: rerun the failing tests, then run the full relevant suite. Run full verification gate (credibility gate) Gate B (full): run the test suite (or the project\u2019s standard verification command). Command: pytest Record: exit code test summary line (placeholder): X passed, 0 failed (or, on failure, X passed, Y failed ) Verification risk handling: Treat a narrowed verification set as a failure mode unless the trace records why it is acceptable (e.g., \u201cno integration tests exist; unit suite is the highest available gate\u201d). Stop criteria and outputs Stop success: Gate A and Gate B pass within budget. Stop failure: repeated failures indicate the upgrade exceeds current permission/scope (e.g., requires API redesign), or budgets are exhausted. Required outputs on stop: change summary: files touched + primary reason verification summary: Gate A command + result and Gate B command + result, including summary lines rollback plan: \u201crevert manifest bump and lockfile\u201d (or equivalent) with the exact files to revert Trade-offs Smaller kernels reduce risk but may require orchestration for multi-step projects. Mitigation: use staged kernels (e.g., \u201cdiagnose-only\u201d kernel \u2192 \u201cpatch\u201d kernel \u2192 \u201crefactor\u201d kernel), each with separate budgets and permissions. Strict permissions reduce blast radius but can prevent necessary refactors. Mitigation: use permission escalation as an explicit step with a justification and a widened verification gate (e.g., requiring a broader test suite when write scope expands). Heavier tracing improves auditability but adds operational overhead. Mitigation: record a minimum viable trace by default (commands, diffs, gate results), and sample/expand traces only on failures or high-risk task classes. Failure Modes Local minima : kernel makes safe micro-edits without addressing root cause. Tool thrash : too many actions with low information gain. False confidence : passing a narrow eval set while violating higher-level requirements. Detection signals (tie these to budgets and evaluation gates, not intuition): - Local minima: - repeated edits in the same small area with no change in verification outcome across iterations - steadily increasing diff size without new evidence (no new failing test localized, no new reproduction) - Tool thrash: - tool-call count rising while the plan does not change (same commands rerun without a new hypothesis) - frequent context switches (many files touched) despite a small, bounded intent - False confidence: - verification gates becoming narrower over time (\u201conly reran one test\u201d) without a recorded justification - \u201cgreen\u201d on fast gates but repeated regressions reported elsewhere (signals the gate set is mis-specified for the task class) - success declared without a trace artifact that includes gate results and the exact commands used Research Directions Kernel composition patterns (delegation, staged permissions, multi-kernel workflows). Automatic stop-condition tuning based on task class. Replayable kernels for deterministic debugging of agent behavior.","title":"03. Autonomous Kernels"},{"location":"book/chapters/03-autonomous-kernels/#chapter-03-autonomous-kernels","text":"","title":"Chapter 03 \u2014 Autonomous Kernels"},{"location":"book/chapters/03-autonomous-kernels/#thesis","text":"An autonomous kernel is a minimal, well-specified control loop that executes bounded work: plan, apply tool actions, verify, and stop. Its constraints (budgets, permissions, evaluation gates) define a safety envelope that makes outcomes inspectable and repeatable. Definitions: - Autonomous kernel : a control loop with explicit limits and explicit exit criteria; it is not \u201cgeneral autonomy,\u201d long-horizon project management, or open-ended exploration. - Budget : a hard cap on resources (iterations, elapsed time, tool calls, diff size) that prevents runaway behavior and forces escalation when progress stalls. - Evaluation gate : a required check whose result must be recorded and must be satisfied (or explicitly waived with justification) before the kernel can declare success. Hypothesis: small, well-governed autonomous kernels (tight loops with explicit budgets and evaluation gates) outperform broad autonomy in stability and debuggability.","title":"Thesis"},{"location":"book/chapters/03-autonomous-kernels/#why-this-matters","text":"Most failures in agentic work are operational: runaway loops, untraceable edits, and unverifiable outcomes. Kernels enable composability: multiple kernels can run with different permissions and evaluation profiles. \u201cKernel-first\u201d design makes autonomy a system property, not a prompt trick.","title":"Why This Matters"},{"location":"book/chapters/03-autonomous-kernels/#system-breakdown","text":"Kernel loop : intent \u2192 plan \u2192 act \u2192 verify \u2192 record trace \u2192 stop/iterate. Budgets : max iterations, time, tool calls, diff size. Permissions : read/write scopes, protected paths, allowed tools. Verification : mandatory checks per action class (e.g., tests for code changes). Persistence : ledger entries, trace logs, artifacts. To make this operational, treat each loop step as a checkpoint with a \u201cmust record\u201d trace payload and a \u201cmust decide\u201d stop condition: - Intent : state the task class and success condition (e.g., \u201ctests pass,\u201d \u201cbuild passes,\u201d \u201crepro no longer fails\u201d). - Plan : enumerate the next 1\u20133 actions only (not the whole project), each tied to a verification gate and a budget slice. - Act : perform the minimal change that addresses the current hypothesis; avoid speculative edits that cannot be evaluated. - Verify : run the smallest evaluation that is credible for the task class. Use unit tests for logic changes and typecheck/build for dependency changes; use end-to-end only when required. Verification can fail in two ways: the check fails, or the check is too narrow to detect the real regression. - Record trace : persist commands executed, files touched, diff stats, and evaluation outputs (or hashes/pointers to them) so a human can replay or audit. - Stop/iterate : stop when the success condition is met, or when a budget is exhausted, or when verification indicates the current plan cannot succeed without broader permissions/scope. Mermaid mapping of stages to controls and outputs: flowchart LR I[Intent] --> P[Plan] --> A[Act] --> V[Verify] --> R[Record trace] --> S{Stop / iterate} B[(Budgets\\niterations/time/tool calls/diff size)] -. constrains .-> P B -. constrains .-> A B -. constrains .-> V Perm[(Permissions\\nread/write scopes\\nprotected paths\\nallowed tools)] -. constrains .-> A Gate[(Evaluation gates\\nby action class)] -. required .-> V Persist[(Persistence\\nledger/trace logs/artifacts)] -. produced .-> R V -->|pass| S V -->|fail| P S -->|iterate| P S -->|stop| End[Exit with summary] A compact \u201cmust capture\u201d checklist (minimum viable trace): | Loop stage | Budget signal | Permission signal | Verification signal | Persistence artifact | |---|---|---|---|---| | intent | remaining iterations/time | required read scope | success criteria defined | intent string + criteria | | plan | tool-call budget allocation | allowed tools list | planned gates named | plan steps + gate mapping | | act | diff size consumed | write scope used | N/A | patch/diff stats | | verify | time/tool calls consumed | execution permissions | gate results (pass/fail) | command + exit code + excerpt | | record trace | N/A | N/A | N/A | ledger entry + trace pointer | | stop/iterate | budget exhausted? | permission insufficient? | gates satisfied? | final summary + next action |","title":"System Breakdown"},{"location":"book/chapters/03-autonomous-kernels/#concrete-example-1","text":"Bug-fix kernel for a CLI tool. Input: failing test case: tests/test_parse.py::test_rejects_empty_input reproduction step: python -m mycli parse \"\" returns exit code 0 but should return non-zero budgets: max 3 iterations, max 10 tool calls, max 40 lines changed permissions: read src/ , write src/parser.py , run pytest -k parse Mini-runbook (a single bounded kernel run): 1. Localize failure (evidence-first) - Action: run the smallest check that reproduces the failure. - Command: pytest -k rejects_empty_input - Record: - failing assertion excerpt (placeholder): E assert 0 == 2 - environment notes: OS, Python version, CLI args - Stop/iterate rule: - If the failure does not reproduce, stop and return \u201ccannot reproduce\u201d trace (do not edit). Patch minimal surface (hypothesis-driven) Hypothesis: empty string is being treated as a valid token stream in src/parser.py . Action: make a minimal edit that rejects empty input at the boundary (not across unrelated call sites). Budget check: ensure diff size stays within 40 lines and touches only src/parser.py (or a single adjacent file if necessary). Record: files touched: src/parser.py diff stats: +6 -1 (placeholder) Run verification gate (tight but credible) Gate 1: rerun the failing test. Command: pytest -k rejects_empty_input Gate 2 (cheap regression check): run related unit tests only. Command: pytest -k parse Verification risk handling: If Gate 1 passes but Gate 2 fails, treat as \u201cnot fixed\u201d (the patch likely broke a nearby invariant). Record trace (auditable, replayable) Persist a kernel trace with: budgets consumed: iterations used, tool calls used, diff size commands executed + exit codes final test summary line (placeholder): 2 passed, 0 failed Write a ledger entry summarizing: what changed (one-sentence) why it changed (link to failing assertion) what verified it (gate list) Stop criteria (explicit) Stop success: Gate 1 and Gate 2 pass within budget. Stop failure: tool-call budget exhausted, diff budget exceeded, or verification indicates a broader refactor is required. Stop escalation output: include \u201cnext action for a human\u201d (e.g., \u201cneeds design change in tokenization; requires editing src/lexer.py , which is outside current write scope\u201d).","title":"Concrete Example 1"},{"location":"book/chapters/03-autonomous-kernels/#concrete-example-2","text":"Dependency upgrade kernel. Input: target version: libX 4.2.0 \u2192 4.3.0 constraints: Python >=3.10 , cannot change public API, CI must stay green upgrade guide: notes a breaking rename OldClient \u2192 Client budgets: max 4 iterations, max 15 tool calls, max 120 lines changed permissions: write pyproject.toml and src/ , run python -m compileall and pytest Kernel steps with an explicit remediation branch: 1. Update manifest (narrow scope) - Action: bump version constraint in pyproject.toml . - Record: - old/new constraint strings - diff stats for manifest only - Stop/iterate rule: - If the dependency resolver cannot produce a consistent lock, stop with resolver output (do not attempt ad-hoc pinning unless that is explicitly in scope). Run a fast build/type gate before full tests Gate A (fast): import/type/compile smoke check. Command: python -m compileall src Record: exit code compile summary line (placeholder): Listing 'src'... \u2026 compileall: success (or equivalent) Interpretation: If Gate A fails, this is often a missing symbol or incompatible API that will be faster to remediate than running the full suite. Remediation branch (compile errors vs failing tests) If compile/import fails : Localize: identify first error site (file + symbol). Patch: apply the minimal mechanical fix (e.g., rename OldClient to Client ) in the smallest set of files. Verify: rerun Gate A only, then proceed. Budget guard: If more than 5 files are touched, stop and escalate (\u201crequires broader refactor\u201d). If the cumulative diff exceeds 120 lines changed, stop and escalate (\u201cexceeds change budget for this kernel\u201d). If compile passes but tests fail : Localize: run the single failing test file or test case. Patch: address behavioral change (e.g., new default timeout) with a targeted adjustment and a justification in the trace. Verify: rerun the failing tests, then run the full relevant suite. Run full verification gate (credibility gate) Gate B (full): run the test suite (or the project\u2019s standard verification command). Command: pytest Record: exit code test summary line (placeholder): X passed, 0 failed (or, on failure, X passed, Y failed ) Verification risk handling: Treat a narrowed verification set as a failure mode unless the trace records why it is acceptable (e.g., \u201cno integration tests exist; unit suite is the highest available gate\u201d). Stop criteria and outputs Stop success: Gate A and Gate B pass within budget. Stop failure: repeated failures indicate the upgrade exceeds current permission/scope (e.g., requires API redesign), or budgets are exhausted. Required outputs on stop: change summary: files touched + primary reason verification summary: Gate A command + result and Gate B command + result, including summary lines rollback plan: \u201crevert manifest bump and lockfile\u201d (or equivalent) with the exact files to revert","title":"Concrete Example 2"},{"location":"book/chapters/03-autonomous-kernels/#trade-offs","text":"Smaller kernels reduce risk but may require orchestration for multi-step projects. Mitigation: use staged kernels (e.g., \u201cdiagnose-only\u201d kernel \u2192 \u201cpatch\u201d kernel \u2192 \u201crefactor\u201d kernel), each with separate budgets and permissions. Strict permissions reduce blast radius but can prevent necessary refactors. Mitigation: use permission escalation as an explicit step with a justification and a widened verification gate (e.g., requiring a broader test suite when write scope expands). Heavier tracing improves auditability but adds operational overhead. Mitigation: record a minimum viable trace by default (commands, diffs, gate results), and sample/expand traces only on failures or high-risk task classes.","title":"Trade-offs"},{"location":"book/chapters/03-autonomous-kernels/#failure-modes","text":"Local minima : kernel makes safe micro-edits without addressing root cause. Tool thrash : too many actions with low information gain. False confidence : passing a narrow eval set while violating higher-level requirements. Detection signals (tie these to budgets and evaluation gates, not intuition): - Local minima: - repeated edits in the same small area with no change in verification outcome across iterations - steadily increasing diff size without new evidence (no new failing test localized, no new reproduction) - Tool thrash: - tool-call count rising while the plan does not change (same commands rerun without a new hypothesis) - frequent context switches (many files touched) despite a small, bounded intent - False confidence: - verification gates becoming narrower over time (\u201conly reran one test\u201d) without a recorded justification - \u201cgreen\u201d on fast gates but repeated regressions reported elsewhere (signals the gate set is mis-specified for the task class) - success declared without a trace artifact that includes gate results and the exact commands used","title":"Failure Modes"},{"location":"book/chapters/03-autonomous-kernels/#research-directions","text":"Kernel composition patterns (delegation, staged permissions, multi-kernel workflows). Automatic stop-condition tuning based on task class. Replayable kernels for deterministic debugging of agent behavior.","title":"Research Directions"},{"location":"book/chapters/04-memory-systems/","text":"Chapter 04 \u2014 Memory Systems Thesis Memory is a system component, not a transcript. It must be structured, queryable, and governed (with provenance) to improve long-horizon work. Hypothesis: uncurated memory increases confidence without increasing correctness, by amplifying earlier mistakes. Why This Matters Long projects exceed context windows; without memory, work becomes repetitive and inconsistent. Without provenance, persistent memory becomes a source of silent drift. Production environments need data minimization and retention policies for stored traces and summaries. System Breakdown Memory classes : Episodic: traces of actions/tool I/O/diffs. Semantic: stable project facts and conventions. Decisions: recorded trade-offs and constraints. State: current plan, progress, open issues. Write policy : what gets stored, when, by whom, and with what validation. Read policy : retrieval filters, ranking, freshness, and provenance checks. Governance : retention, access control, redaction, and correction mechanisms. Concrete Example 1 Decision memory for an architecture choice. - Store: decision record with options, chosen approach, constraints, and rationale. - Enforce: future changes must reference the decision or explicitly supersede it. Concrete Example 2 Trace-indexed memory for debugging. - Store: tool outputs and diffs keyed by task/iteration. - Use: when a regression occurs, retrieve prior similar traces and compare failure signatures. Trade-offs More memory improves continuity but increases risk of stale or incorrect retrieval. Strong provenance improves trust but adds overhead to writing and updating memory. Aggressive retention helps debugging but increases privacy and storage costs. Failure Modes Stale retrieval dominance : old assumptions override new evidence. Summarization loss : key constraints disappear in compression. Memory poisoning : incorrect conclusions become \u201cfacts\u201d through repetition. Research Directions Memory scoring with automated freshness and provenance signals. Mechanisms for correcting memory (retractions, superseding records). Evaluations for memory usefulness (measuring reduced iterations without increased regressions).","title":"04. Memory Systems"},{"location":"book/chapters/04-memory-systems/#chapter-04-memory-systems","text":"","title":"Chapter 04 \u2014 Memory Systems"},{"location":"book/chapters/04-memory-systems/#thesis","text":"Memory is a system component, not a transcript. It must be structured, queryable, and governed (with provenance) to improve long-horizon work. Hypothesis: uncurated memory increases confidence without increasing correctness, by amplifying earlier mistakes.","title":"Thesis"},{"location":"book/chapters/04-memory-systems/#why-this-matters","text":"Long projects exceed context windows; without memory, work becomes repetitive and inconsistent. Without provenance, persistent memory becomes a source of silent drift. Production environments need data minimization and retention policies for stored traces and summaries.","title":"Why This Matters"},{"location":"book/chapters/04-memory-systems/#system-breakdown","text":"Memory classes : Episodic: traces of actions/tool I/O/diffs. Semantic: stable project facts and conventions. Decisions: recorded trade-offs and constraints. State: current plan, progress, open issues. Write policy : what gets stored, when, by whom, and with what validation. Read policy : retrieval filters, ranking, freshness, and provenance checks. Governance : retention, access control, redaction, and correction mechanisms.","title":"System Breakdown"},{"location":"book/chapters/04-memory-systems/#concrete-example-1","text":"Decision memory for an architecture choice. - Store: decision record with options, chosen approach, constraints, and rationale. - Enforce: future changes must reference the decision or explicitly supersede it.","title":"Concrete Example 1"},{"location":"book/chapters/04-memory-systems/#concrete-example-2","text":"Trace-indexed memory for debugging. - Store: tool outputs and diffs keyed by task/iteration. - Use: when a regression occurs, retrieve prior similar traces and compare failure signatures.","title":"Concrete Example 2"},{"location":"book/chapters/04-memory-systems/#trade-offs","text":"More memory improves continuity but increases risk of stale or incorrect retrieval. Strong provenance improves trust but adds overhead to writing and updating memory. Aggressive retention helps debugging but increases privacy and storage costs.","title":"Trade-offs"},{"location":"book/chapters/04-memory-systems/#failure-modes","text":"Stale retrieval dominance : old assumptions override new evidence. Summarization loss : key constraints disappear in compression. Memory poisoning : incorrect conclusions become \u201cfacts\u201d through repetition.","title":"Failure Modes"},{"location":"book/chapters/04-memory-systems/#research-directions","text":"Memory scoring with automated freshness and provenance signals. Mechanisms for correcting memory (retractions, superseding records). Evaluations for memory usefulness (measuring reduced iterations without increased regressions).","title":"Research Directions"},{"location":"book/chapters/05-evaluation-and-traces/","text":"Chapter 05 \u2014 Evaluation and Traces Thesis Evaluation and traceability are the mechanisms that make AI-first engineering reproducible. Traces allow attribution; evaluations enforce correctness gates. Hypothesis: without trace-first design, teams cannot reliably distinguish model errors from tool errors, harness errors, or missing tests. Why This Matters Reproducibility is a prerequisite for iterative improvement. Evaluation gates define the autonomy envelope and prevent silent regressions. Traces enable post-incident analysis and systematic harness refinement. System Breakdown Trace schema (minimum viable): task id, plan, tool calls, outputs, diffs, evaluation results, budgets, stop reason. Evaluation types : correctness: unit/integration/contract tests. safety: permission checks, protected paths, secret scanning. quality: lint, type checks, formatting, doc checks. performance: benchmarks, latency/cost budgets. Gating model : which evaluations are required for which action classes. Concrete Example 1 Tracing a refactor. - Record: each patch + test run + failure signature. - Use: compare traces across runs to identify where the harness improved outcomes. Concrete Example 2 Drift detection for an agent loop. - Maintain: a stable eval suite and a small set of \u201cgolden\u201d tasks. - Detect: changes in iteration counts, regression rate, and stop reasons over time. Trade-offs More evaluation increases confidence but costs time and compute. Rich traces help debugging but create storage and privacy burdens. Overly rigid gates can block progress on codebases with weak test coverage. Failure Modes Eval gaming : optimizing for metrics while harming real-world quality. Blind spots : evaluations do not cover critical behaviors. Un-actionable traces : logs exist but lack structure, making search and attribution hard. Research Directions Standard trace formats for portability across tools and models. Risk-based gating (stricter checks for higher-risk diffs). Low-cost evaluations that correlate with production outcomes.","title":"05. Evaluation and Traces"},{"location":"book/chapters/05-evaluation-and-traces/#chapter-05-evaluation-and-traces","text":"","title":"Chapter 05 \u2014 Evaluation and Traces"},{"location":"book/chapters/05-evaluation-and-traces/#thesis","text":"Evaluation and traceability are the mechanisms that make AI-first engineering reproducible. Traces allow attribution; evaluations enforce correctness gates. Hypothesis: without trace-first design, teams cannot reliably distinguish model errors from tool errors, harness errors, or missing tests.","title":"Thesis"},{"location":"book/chapters/05-evaluation-and-traces/#why-this-matters","text":"Reproducibility is a prerequisite for iterative improvement. Evaluation gates define the autonomy envelope and prevent silent regressions. Traces enable post-incident analysis and systematic harness refinement.","title":"Why This Matters"},{"location":"book/chapters/05-evaluation-and-traces/#system-breakdown","text":"Trace schema (minimum viable): task id, plan, tool calls, outputs, diffs, evaluation results, budgets, stop reason. Evaluation types : correctness: unit/integration/contract tests. safety: permission checks, protected paths, secret scanning. quality: lint, type checks, formatting, doc checks. performance: benchmarks, latency/cost budgets. Gating model : which evaluations are required for which action classes.","title":"System Breakdown"},{"location":"book/chapters/05-evaluation-and-traces/#concrete-example-1","text":"Tracing a refactor. - Record: each patch + test run + failure signature. - Use: compare traces across runs to identify where the harness improved outcomes.","title":"Concrete Example 1"},{"location":"book/chapters/05-evaluation-and-traces/#concrete-example-2","text":"Drift detection for an agent loop. - Maintain: a stable eval suite and a small set of \u201cgolden\u201d tasks. - Detect: changes in iteration counts, regression rate, and stop reasons over time.","title":"Concrete Example 2"},{"location":"book/chapters/05-evaluation-and-traces/#trade-offs","text":"More evaluation increases confidence but costs time and compute. Rich traces help debugging but create storage and privacy burdens. Overly rigid gates can block progress on codebases with weak test coverage.","title":"Trade-offs"},{"location":"book/chapters/05-evaluation-and-traces/#failure-modes","text":"Eval gaming : optimizing for metrics while harming real-world quality. Blind spots : evaluations do not cover critical behaviors. Un-actionable traces : logs exist but lack structure, making search and attribution hard.","title":"Failure Modes"},{"location":"book/chapters/05-evaluation-and-traces/#research-directions","text":"Standard trace formats for portability across tools and models. Risk-based gating (stricter checks for higher-risk diffs). Low-cost evaluations that correlate with production outcomes.","title":"Research Directions"},{"location":"book/chapters/06-agent-governance/","text":"Chapter 06 \u2014 Agent Governance Thesis Governance defines the safe operating envelope for autonomy: permissions, budgets, review policies, auditability, and incident response. Hypothesis: autonomy without enforceable governance increases throughput in the short term but increases defect rate and operational risk over time. Why This Matters Agents can operate faster than human review cycles; governance prevents unsafe acceleration. Production environments require audit trails and controlled access to sensitive operations. Governance makes behavior consistent across models and team members. System Breakdown Policy artifacts : constitution (principles), agent rules (operational constraints), CI policies (enforcement). Permissions : read/write scopes, protected files, tool allowlists. Budgets : time, iterations, tool calls, diff size, cost ceilings. Review : mandatory human checkpoints for specific risk classes. Audit : trace retention, searchable logs, change attribution. Concrete Example 1 Protected-path governance. - Rule: forbid edits to security-critical configs without explicit approval. - Enforcement: tool layer rejects patches; CI verifies policy compliance. Concrete Example 2 Incident response for a bad autonomous change. - Trigger: regression detected by eval gates post-merge. - Response: rollback + trace review + policy update + new regression test. Trade-offs Strong governance reduces risk but can slow iteration. Overly strict permissions increase human workload via escalations. Excessive auditing can create privacy and compliance burdens. Failure Modes Policy drift : rules become outdated and stop reflecting real risks. Shadow autonomy : humans bypass gates \u201cjust this once,\u201d breaking discipline. Governance without enforcement : documents exist but tools/CI do not enforce them. Research Directions Policy-as-code patterns for agent systems. Automated risk scoring for diffs to route reviews. Governance metrics: prevented incidents vs friction cost.","title":"06. Agent Governance"},{"location":"book/chapters/06-agent-governance/#chapter-06-agent-governance","text":"","title":"Chapter 06 \u2014 Agent Governance"},{"location":"book/chapters/06-agent-governance/#thesis","text":"Governance defines the safe operating envelope for autonomy: permissions, budgets, review policies, auditability, and incident response. Hypothesis: autonomy without enforceable governance increases throughput in the short term but increases defect rate and operational risk over time.","title":"Thesis"},{"location":"book/chapters/06-agent-governance/#why-this-matters","text":"Agents can operate faster than human review cycles; governance prevents unsafe acceleration. Production environments require audit trails and controlled access to sensitive operations. Governance makes behavior consistent across models and team members.","title":"Why This Matters"},{"location":"book/chapters/06-agent-governance/#system-breakdown","text":"Policy artifacts : constitution (principles), agent rules (operational constraints), CI policies (enforcement). Permissions : read/write scopes, protected files, tool allowlists. Budgets : time, iterations, tool calls, diff size, cost ceilings. Review : mandatory human checkpoints for specific risk classes. Audit : trace retention, searchable logs, change attribution.","title":"System Breakdown"},{"location":"book/chapters/06-agent-governance/#concrete-example-1","text":"Protected-path governance. - Rule: forbid edits to security-critical configs without explicit approval. - Enforcement: tool layer rejects patches; CI verifies policy compliance.","title":"Concrete Example 1"},{"location":"book/chapters/06-agent-governance/#concrete-example-2","text":"Incident response for a bad autonomous change. - Trigger: regression detected by eval gates post-merge. - Response: rollback + trace review + policy update + new regression test.","title":"Concrete Example 2"},{"location":"book/chapters/06-agent-governance/#trade-offs","text":"Strong governance reduces risk but can slow iteration. Overly strict permissions increase human workload via escalations. Excessive auditing can create privacy and compliance burdens.","title":"Trade-offs"},{"location":"book/chapters/06-agent-governance/#failure-modes","text":"Policy drift : rules become outdated and stop reflecting real risks. Shadow autonomy : humans bypass gates \u201cjust this once,\u201d breaking discipline. Governance without enforcement : documents exist but tools/CI do not enforce them.","title":"Failure Modes"},{"location":"book/chapters/06-agent-governance/#research-directions","text":"Policy-as-code patterns for agent systems. Automated risk scoring for diffs to route reviews. Governance metrics: prevented incidents vs friction cost.","title":"Research Directions"},{"location":"book/chapters/07-production-ai-infrastructure/","text":"Chapter 07 \u2014 Production AI Infrastructure Thesis Production AI-first systems are distributed systems: they require orchestration, isolation, observability, caching, cost control, and reproducible environments. Hypothesis: operational reliability depends more on the tool/runtime plane (sandboxing, retries, replay, artifacts) than on the model prompt. Why This Matters Without isolation, tool execution becomes a security and reliability risk. Without observability, failures cannot be attributed or fixed systematically. Without cost controls, autonomy can become economically unstable. System Breakdown Execution : sandboxes/containers, dependency pinning, deterministic runners. Tool services : test runners, build systems, browsers, repo APIs. Orchestration : queues, concurrency limits, backpressure. Observability : traces, metrics, logs; correlation ids. Artifacts : build outputs, diffs, evaluation reports, replay bundles. Security : secrets handling, network egress controls, least privilege. Concrete Example 1 Sandboxed tool execution for code changes. - Run: tests and builds inside a controlled environment. - Store: artifacts and traces to enable replay. - Gate: promote changes only when evals pass. Concrete Example 2 Cost-aware autonomy for a batch of maintenance tasks. - Budget: per-task token/cost ceilings. - Strategy: fail fast on low-signal tasks; escalate to human review when uncertain. - Measure: cost per successful task and regression rate. Trade-offs Isolation increases safety but adds operational complexity. Strong observability increases insight but raises data retention requirements. Caching and replay improve speed but can mask nondeterminism if misused. Failure Modes Non-reproducible runs : environment drift makes traces hard to replay. Leaky permissions : tool plane has broader access than intended. Noisy observability : too much unstructured logging reduces signal. Research Directions Standardized replay bundles for agent runs. Cost/performance models that predict optimal evaluation depth. Secure-by-default tool runtime primitives for autonomy.","title":"07. Production AI Infrastructure"},{"location":"book/chapters/07-production-ai-infrastructure/#chapter-07-production-ai-infrastructure","text":"","title":"Chapter 07 \u2014 Production AI Infrastructure"},{"location":"book/chapters/07-production-ai-infrastructure/#thesis","text":"Production AI-first systems are distributed systems: they require orchestration, isolation, observability, caching, cost control, and reproducible environments. Hypothesis: operational reliability depends more on the tool/runtime plane (sandboxing, retries, replay, artifacts) than on the model prompt.","title":"Thesis"},{"location":"book/chapters/07-production-ai-infrastructure/#why-this-matters","text":"Without isolation, tool execution becomes a security and reliability risk. Without observability, failures cannot be attributed or fixed systematically. Without cost controls, autonomy can become economically unstable.","title":"Why This Matters"},{"location":"book/chapters/07-production-ai-infrastructure/#system-breakdown","text":"Execution : sandboxes/containers, dependency pinning, deterministic runners. Tool services : test runners, build systems, browsers, repo APIs. Orchestration : queues, concurrency limits, backpressure. Observability : traces, metrics, logs; correlation ids. Artifacts : build outputs, diffs, evaluation reports, replay bundles. Security : secrets handling, network egress controls, least privilege.","title":"System Breakdown"},{"location":"book/chapters/07-production-ai-infrastructure/#concrete-example-1","text":"Sandboxed tool execution for code changes. - Run: tests and builds inside a controlled environment. - Store: artifacts and traces to enable replay. - Gate: promote changes only when evals pass.","title":"Concrete Example 1"},{"location":"book/chapters/07-production-ai-infrastructure/#concrete-example-2","text":"Cost-aware autonomy for a batch of maintenance tasks. - Budget: per-task token/cost ceilings. - Strategy: fail fast on low-signal tasks; escalate to human review when uncertain. - Measure: cost per successful task and regression rate.","title":"Concrete Example 2"},{"location":"book/chapters/07-production-ai-infrastructure/#trade-offs","text":"Isolation increases safety but adds operational complexity. Strong observability increases insight but raises data retention requirements. Caching and replay improve speed but can mask nondeterminism if misused.","title":"Trade-offs"},{"location":"book/chapters/07-production-ai-infrastructure/#failure-modes","text":"Non-reproducible runs : environment drift makes traces hard to replay. Leaky permissions : tool plane has broader access than intended. Noisy observability : too much unstructured logging reduces signal.","title":"Failure Modes"},{"location":"book/chapters/07-production-ai-infrastructure/#research-directions","text":"Standardized replay bundles for agent runs. Cost/performance models that predict optimal evaluation depth. Secure-by-default tool runtime primitives for autonomy.","title":"Research Directions"},{"location":"book/chapters/99-future-directions/","text":"Chapter 99 \u2014 Future Directions Thesis The frontier is system interfaces and verification: stronger tool contracts, better evaluations, structured memory, and governance primitives that scale across teams and models. Hypothesis: as autonomy scales, the limiting factor becomes organizational and infrastructural coupling, not raw inference capability. Why This Matters Teams will operate heterogeneous models and tools; interoperability becomes a reliability constraint. Long-horizon autonomy introduces new failure classes (compounded assumptions, policy drift, supply-chain issues). Without standards, every team reinvents trace formats, eval suites, and governance mechanisms. System Breakdown Interoperability : shared trace formats, tool schemas, evaluation definitions. Verification : stronger correctness checks, property-based testing, contract enforcement. Governance at scale : org-level policies, audit workflows, incident response. Ecosystem risks : prompt/tool supply chain, dependency security, model updates. Concrete Example 1 Cross-model portability experiment. - Run: the same harness + eval suite against multiple models. - Compare: outcomes, iteration profiles, and failure signatures. - Goal: isolate what is harness-dependent vs model-dependent. Concrete Example 2 Standardized trace interchange. - Export: traces from one agent runtime. - Replay/analyze: in a different tool. - Goal: enable independent auditing and regression analysis. Trade-offs Standardization improves portability but can slow experimentation. Strong verification increases confidence but can increase compute and engineering effort. More governance improves safety but can reduce developer autonomy. Failure Modes Lock-in : traces and tools become proprietary and non-portable. False comparability : metrics appear comparable across systems but differ in hidden ways. Scale amplification : small policy errors cause large, repeated failures. Research Directions Formal methods adapted to agent loops (bounded proofs, verified tool contracts). Benchmarks for reproducible autonomy (replay success, attribution accuracy). Org-scale governance patterns and \u201cpolicy drift\u201d detection.","title":"99. Future Directions"},{"location":"book/chapters/99-future-directions/#chapter-99-future-directions","text":"","title":"Chapter 99 \u2014 Future Directions"},{"location":"book/chapters/99-future-directions/#thesis","text":"The frontier is system interfaces and verification: stronger tool contracts, better evaluations, structured memory, and governance primitives that scale across teams and models. Hypothesis: as autonomy scales, the limiting factor becomes organizational and infrastructural coupling, not raw inference capability.","title":"Thesis"},{"location":"book/chapters/99-future-directions/#why-this-matters","text":"Teams will operate heterogeneous models and tools; interoperability becomes a reliability constraint. Long-horizon autonomy introduces new failure classes (compounded assumptions, policy drift, supply-chain issues). Without standards, every team reinvents trace formats, eval suites, and governance mechanisms.","title":"Why This Matters"},{"location":"book/chapters/99-future-directions/#system-breakdown","text":"Interoperability : shared trace formats, tool schemas, evaluation definitions. Verification : stronger correctness checks, property-based testing, contract enforcement. Governance at scale : org-level policies, audit workflows, incident response. Ecosystem risks : prompt/tool supply chain, dependency security, model updates.","title":"System Breakdown"},{"location":"book/chapters/99-future-directions/#concrete-example-1","text":"Cross-model portability experiment. - Run: the same harness + eval suite against multiple models. - Compare: outcomes, iteration profiles, and failure signatures. - Goal: isolate what is harness-dependent vs model-dependent.","title":"Concrete Example 1"},{"location":"book/chapters/99-future-directions/#concrete-example-2","text":"Standardized trace interchange. - Export: traces from one agent runtime. - Replay/analyze: in a different tool. - Goal: enable independent auditing and regression analysis.","title":"Concrete Example 2"},{"location":"book/chapters/99-future-directions/#trade-offs","text":"Standardization improves portability but can slow experimentation. Strong verification increases confidence but can increase compute and engineering effort. More governance improves safety but can reduce developer autonomy.","title":"Trade-offs"},{"location":"book/chapters/99-future-directions/#failure-modes","text":"Lock-in : traces and tools become proprietary and non-portable. False comparability : metrics appear comparable across systems but differ in hidden ways. Scale amplification : small policy errors cause large, repeated failures.","title":"Failure Modes"},{"location":"book/chapters/99-future-directions/#research-directions","text":"Formal methods adapted to agent loops (bounded proofs, verified tool contracts). Benchmarks for reproducible autonomy (replay success, attribution accuracy). Org-scale governance patterns and \u201cpolicy drift\u201d detection.","title":"Research Directions"},{"location":"book/patterns/harness-design-patterns/","text":"Harness Design Patterns Context A model alone is a general-purpose component. Production behavior is shaped by the harness: prompts, tools, memory, budgets, verification, and traceability. Problem How do you design a harness that is reliable, debuggable, and governable without building a fragile tangle of special cases? Forces Constraints vs. coverage : adding constraints improves safety but can reduce task coverage. Tools vs. surface area : more tools increase capability but enlarge the debug and governance surface. Memory vs. drift/privacy : continuity improves with memory, but so do drift and data handling risks. Automation vs. blast radius : stronger automation improves throughput but increases the impact of failures. Observability vs. cost : better tracing and verification cost time and compute. Solution Use a small set of recurring harness patterns that are easy to audit and compose. Pattern 1: Typed Tool Boundary Idea : tools are the only way to cause side effects, and each tool has a typed schema with explicit errors. Why it works : reduces ambiguity and makes traces auditable. Example : create_file(path, content) returns created | updated | no_op , plus a checksum. Pattern 2: Budgeted Control (Steps/Time/Cost) Idea : the kernel enforces budgets; the model cannot override them. Why it works : turns open-ended iteration into a bounded process. Example : max 20 steps or 5 minutes; on exhaustion, stop with a partial report. Pattern 3: Evidence-First Completion Idea : \u201cdone\u201d requires verifiable evidence (tests run, diffs applied, outputs captured). Why it works : prevents completion based on plausibility alone. Example : stop is rejected unless verification artifacts exist in the trace. Pattern 4: Narrow Context Construction Idea : select only the files needed for the next action; summarize the rest. Why it works : reduces context bloat and keeps constraints salient. Example : open 2\u20134 files max, keep a rolling summary of prior steps. Pattern 5: Separation of Duties (Plan vs. Execute vs. Verify) Idea : treat these as distinct phases with distinct constraints. Why it works : limits the ability to bypass controls (for example, editing policies during execution). Example : planning cannot call side-effect tools; verification cannot modify code. Implementation sketch Minimal harness components: Kernel : step loop, budgets, cancellation/timeouts, trace append. Tool router : allowlist + argument validation + consistent error model + idempotency support. Context builder : file selection + summarization policy + retrieval policy. Verifier : runs evals/tests and records outputs. Governance layer : approvals for high-risk tools, audit logs, retention/redaction rules. A small harness configuration can be expressed as a policy document (conceptual): tools: allowlist: [read_file, grep_search, apply_patch, get_errors, run_in_terminal] budgets: max_steps: 20 max_minutes: 5 stop_gate: require_verification: true acceptable_outcomes: [\"verified\", \"blocked\"] Concrete example Repo task agent that edits documentation: Tools: read_file , grep_search , apply_patch , get_errors . Budgets: 15 steps, 3 minutes. Stop gate: markdown checks (or at minimum a syntax/lint pass) must be clean, or the run stops as \u201cblocked\u201d with reproduction steps. Failure modes Tool sprawl : too many overlapping tools; selection becomes inconsistent and hard to audit. Hidden side effects : tools mutate state without reporting it; traces become misleading. Context bloat : prompts include too much content; constraints and acceptance criteria are diluted. Policy bypass : weak allowlists or validation enable unintended actions. Unmeasured changes : no evals/verification; regressions ship silently. Over-coupling : harness depends on brittle prompt wording instead of enforceable kernel/router constraints. When not to use Single-purpose automation where a deterministic script is simpler. One-off exploratory work where harness engineering overhead dominates. Systems without ownership/ops capacity to maintain tools, budgets, verification, and incident response.","title":"Harness Design"},{"location":"book/patterns/harness-design-patterns/#harness-design-patterns","text":"","title":"Harness Design Patterns"},{"location":"book/patterns/harness-design-patterns/#context","text":"A model alone is a general-purpose component. Production behavior is shaped by the harness: prompts, tools, memory, budgets, verification, and traceability.","title":"Context"},{"location":"book/patterns/harness-design-patterns/#problem","text":"How do you design a harness that is reliable, debuggable, and governable without building a fragile tangle of special cases?","title":"Problem"},{"location":"book/patterns/harness-design-patterns/#forces","text":"Constraints vs. coverage : adding constraints improves safety but can reduce task coverage. Tools vs. surface area : more tools increase capability but enlarge the debug and governance surface. Memory vs. drift/privacy : continuity improves with memory, but so do drift and data handling risks. Automation vs. blast radius : stronger automation improves throughput but increases the impact of failures. Observability vs. cost : better tracing and verification cost time and compute.","title":"Forces"},{"location":"book/patterns/harness-design-patterns/#solution","text":"Use a small set of recurring harness patterns that are easy to audit and compose.","title":"Solution"},{"location":"book/patterns/harness-design-patterns/#pattern-1-typed-tool-boundary","text":"Idea : tools are the only way to cause side effects, and each tool has a typed schema with explicit errors. Why it works : reduces ambiguity and makes traces auditable. Example : create_file(path, content) returns created | updated | no_op , plus a checksum.","title":"Pattern 1: Typed Tool Boundary"},{"location":"book/patterns/harness-design-patterns/#pattern-2-budgeted-control-stepstimecost","text":"Idea : the kernel enforces budgets; the model cannot override them. Why it works : turns open-ended iteration into a bounded process. Example : max 20 steps or 5 minutes; on exhaustion, stop with a partial report.","title":"Pattern 2: Budgeted Control (Steps/Time/Cost)"},{"location":"book/patterns/harness-design-patterns/#pattern-3-evidence-first-completion","text":"Idea : \u201cdone\u201d requires verifiable evidence (tests run, diffs applied, outputs captured). Why it works : prevents completion based on plausibility alone. Example : stop is rejected unless verification artifacts exist in the trace.","title":"Pattern 3: Evidence-First Completion"},{"location":"book/patterns/harness-design-patterns/#pattern-4-narrow-context-construction","text":"Idea : select only the files needed for the next action; summarize the rest. Why it works : reduces context bloat and keeps constraints salient. Example : open 2\u20134 files max, keep a rolling summary of prior steps.","title":"Pattern 4: Narrow Context Construction"},{"location":"book/patterns/harness-design-patterns/#pattern-5-separation-of-duties-plan-vs-execute-vs-verify","text":"Idea : treat these as distinct phases with distinct constraints. Why it works : limits the ability to bypass controls (for example, editing policies during execution). Example : planning cannot call side-effect tools; verification cannot modify code.","title":"Pattern 5: Separation of Duties (Plan vs. Execute vs. Verify)"},{"location":"book/patterns/harness-design-patterns/#implementation-sketch","text":"Minimal harness components: Kernel : step loop, budgets, cancellation/timeouts, trace append. Tool router : allowlist + argument validation + consistent error model + idempotency support. Context builder : file selection + summarization policy + retrieval policy. Verifier : runs evals/tests and records outputs. Governance layer : approvals for high-risk tools, audit logs, retention/redaction rules. A small harness configuration can be expressed as a policy document (conceptual): tools: allowlist: [read_file, grep_search, apply_patch, get_errors, run_in_terminal] budgets: max_steps: 20 max_minutes: 5 stop_gate: require_verification: true acceptable_outcomes: [\"verified\", \"blocked\"]","title":"Implementation sketch"},{"location":"book/patterns/harness-design-patterns/#concrete-example","text":"Repo task agent that edits documentation: Tools: read_file , grep_search , apply_patch , get_errors . Budgets: 15 steps, 3 minutes. Stop gate: markdown checks (or at minimum a syntax/lint pass) must be clean, or the run stops as \u201cblocked\u201d with reproduction steps.","title":"Concrete example"},{"location":"book/patterns/harness-design-patterns/#failure-modes","text":"Tool sprawl : too many overlapping tools; selection becomes inconsistent and hard to audit. Hidden side effects : tools mutate state without reporting it; traces become misleading. Context bloat : prompts include too much content; constraints and acceptance criteria are diluted. Policy bypass : weak allowlists or validation enable unintended actions. Unmeasured changes : no evals/verification; regressions ship silently. Over-coupling : harness depends on brittle prompt wording instead of enforceable kernel/router constraints.","title":"Failure modes"},{"location":"book/patterns/harness-design-patterns/#when-not-to-use","text":"Single-purpose automation where a deterministic script is simpler. One-off exploratory work where harness engineering overhead dominates. Systems without ownership/ops capacity to maintain tools, budgets, verification, and incident response.","title":"When not to use"},{"location":"book/patterns/memory-architectures/","text":"Memory Architectures Context Agents are limited by context window, variability across runs, and the need to operate over long-lived projects. Memory mechanisms can improve continuity, but they also introduce drift, privacy risk, and debugging complexity. Problem How do you add memory so the system remains reproducible and governable? Forces Recall vs. precision : retrieving more increases coverage but adds noise. Freshness vs. stability : updating memory improves relevance but can rewrite history. Privacy vs. utility : storing more can leak sensitive data and expand retention obligations. Debuggability : implicit retrieval is harder to reason about than explicit records. Versioning : memory must evolve with code; unversioned memory becomes a hidden dependency. Solution Prefer layered, explicit memory with clear scopes, schemas, and write rules. Layer 1: Run-local working memory (scratch) What : transient notes, intermediate calculations, short summaries. Scope : one run. Write rule : always safe to overwrite; never treated as durable truth. Example : \u201cFiles touched: A, B. Hypothesis: failing test caused by null handling.\u201d Layer 2: Session memory (task state) What : structured state for a multi-step task (checklists, open questions, next steps). Scope : until task completion. Write rule : update on each step; clear on task close. Example : a JSON task record containing acceptance criteria and verification status. Layer 3: Project memory (durable facts) What : stable, reviewable records: architecture decisions, interface contracts, runbooks. Scope : long-lived. Write rule : write only after verification passes and with a source-of-truth reference. Example : ADR-style entries with links to code and traces. Layer 4: Retrieval index (searchable corpus) What : embeddings or keyword index over docs/issues/traces. Scope : long-lived, but treated as derived data. Write rule : rebuildable; never the only place a critical fact exists. Example : \u201cretrieve top 5 related incidents\u201d feeding short excerpts into context. Implementation sketch Write rules that keep memory auditable and safe: Only write durable memory after verification passes. Store sources (file paths, URLs, trace IDs, commit hashes) with each memory item. Separate schemas for different types of memory: facts, decisions, preferences, open questions. Treat retrieval as a hint ; require confirmation against sources for critical claims. Version memory alongside the system (or tie it to a release identifier). Support redaction and retention policies (delete by scope, delete by source, delete by time). Example durable-memory record schema (conceptual): { \"type\": \"decision\", \"title\": \"Prefer golden tests for CLI help output\", \"status\": \"accepted\", \"sources\": [\"docs/cli.md\", \"trace:2026-02-18T10:14Z\"], \"rationale\": \"Help output is user-facing and easy to regress\", \"verified_by\": [\"npm test\", \"snapshot update reviewed\"], \"created_at\": \"2026-02-18\" } Concrete example Bugfix agent memory layout: Run-local : stack trace notes and hypotheses. Session : checklist of reproduction steps + test plan + files changed. Project : \u201cRoot cause and fix\u201d note linked to the failing test and the patch. Retrieval : search prior traces for similar failure signatures. Failure modes Stale memory : outdated assumptions persist after refactors; fixes target the wrong code. Memory poisoning : incorrect entries are stored as facts and bias future actions. Over-retrieval : too many irrelevant items drown the signal and dilute constraints. Silent mutation : memory is updated without review; history is effectively rewritten. Unversioned dependency : behavior depends on memory that is not tied to code/version. Privacy leakage : sensitive content is stored, retrieved, or logged without appropriate handling. When not to use Short-lived tasks where the context window is sufficient. High-sensitivity domains without a clear retention/redaction policy. Systems that require strict reproducibility but cannot version memory with code.","title":"Memory Architectures"},{"location":"book/patterns/memory-architectures/#memory-architectures","text":"","title":"Memory Architectures"},{"location":"book/patterns/memory-architectures/#context","text":"Agents are limited by context window, variability across runs, and the need to operate over long-lived projects. Memory mechanisms can improve continuity, but they also introduce drift, privacy risk, and debugging complexity.","title":"Context"},{"location":"book/patterns/memory-architectures/#problem","text":"How do you add memory so the system remains reproducible and governable?","title":"Problem"},{"location":"book/patterns/memory-architectures/#forces","text":"Recall vs. precision : retrieving more increases coverage but adds noise. Freshness vs. stability : updating memory improves relevance but can rewrite history. Privacy vs. utility : storing more can leak sensitive data and expand retention obligations. Debuggability : implicit retrieval is harder to reason about than explicit records. Versioning : memory must evolve with code; unversioned memory becomes a hidden dependency.","title":"Forces"},{"location":"book/patterns/memory-architectures/#solution","text":"Prefer layered, explicit memory with clear scopes, schemas, and write rules.","title":"Solution"},{"location":"book/patterns/memory-architectures/#layer-1-run-local-working-memory-scratch","text":"What : transient notes, intermediate calculations, short summaries. Scope : one run. Write rule : always safe to overwrite; never treated as durable truth. Example : \u201cFiles touched: A, B. Hypothesis: failing test caused by null handling.\u201d","title":"Layer 1: Run-local working memory (scratch)"},{"location":"book/patterns/memory-architectures/#layer-2-session-memory-task-state","text":"What : structured state for a multi-step task (checklists, open questions, next steps). Scope : until task completion. Write rule : update on each step; clear on task close. Example : a JSON task record containing acceptance criteria and verification status.","title":"Layer 2: Session memory (task state)"},{"location":"book/patterns/memory-architectures/#layer-3-project-memory-durable-facts","text":"What : stable, reviewable records: architecture decisions, interface contracts, runbooks. Scope : long-lived. Write rule : write only after verification passes and with a source-of-truth reference. Example : ADR-style entries with links to code and traces.","title":"Layer 3: Project memory (durable facts)"},{"location":"book/patterns/memory-architectures/#layer-4-retrieval-index-searchable-corpus","text":"What : embeddings or keyword index over docs/issues/traces. Scope : long-lived, but treated as derived data. Write rule : rebuildable; never the only place a critical fact exists. Example : \u201cretrieve top 5 related incidents\u201d feeding short excerpts into context.","title":"Layer 4: Retrieval index (searchable corpus)"},{"location":"book/patterns/memory-architectures/#implementation-sketch","text":"Write rules that keep memory auditable and safe: Only write durable memory after verification passes. Store sources (file paths, URLs, trace IDs, commit hashes) with each memory item. Separate schemas for different types of memory: facts, decisions, preferences, open questions. Treat retrieval as a hint ; require confirmation against sources for critical claims. Version memory alongside the system (or tie it to a release identifier). Support redaction and retention policies (delete by scope, delete by source, delete by time). Example durable-memory record schema (conceptual): { \"type\": \"decision\", \"title\": \"Prefer golden tests for CLI help output\", \"status\": \"accepted\", \"sources\": [\"docs/cli.md\", \"trace:2026-02-18T10:14Z\"], \"rationale\": \"Help output is user-facing and easy to regress\", \"verified_by\": [\"npm test\", \"snapshot update reviewed\"], \"created_at\": \"2026-02-18\" }","title":"Implementation sketch"},{"location":"book/patterns/memory-architectures/#concrete-example","text":"Bugfix agent memory layout: Run-local : stack trace notes and hypotheses. Session : checklist of reproduction steps + test plan + files changed. Project : \u201cRoot cause and fix\u201d note linked to the failing test and the patch. Retrieval : search prior traces for similar failure signatures.","title":"Concrete example"},{"location":"book/patterns/memory-architectures/#failure-modes","text":"Stale memory : outdated assumptions persist after refactors; fixes target the wrong code. Memory poisoning : incorrect entries are stored as facts and bias future actions. Over-retrieval : too many irrelevant items drown the signal and dilute constraints. Silent mutation : memory is updated without review; history is effectively rewritten. Unversioned dependency : behavior depends on memory that is not tied to code/version. Privacy leakage : sensitive content is stored, retrieved, or logged without appropriate handling.","title":"Failure modes"},{"location":"book/patterns/memory-architectures/#when-not-to-use","text":"Short-lived tasks where the context window is sufficient. High-sensitivity domains without a clear retention/redaction policy. Systems that require strict reproducibility but cannot version memory with code.","title":"When not to use"},{"location":"book/patterns/minimal-agent-loop/","text":"Minimal Agent Loop Context You need iterative work (planning + tool use + feedback) inside an engineering environment (repo, CI, tickets, docs). The primary risk is uncontrolled complexity: additional autonomy without corresponding observability and verification. Problem How do you get useful autonomous work while keeping the control surface small enough to debug, test, and govern? Forces Capability vs. determinism : richer loops solve more tasks but increase variance across runs. Observability vs. speed : more logging and checks slow iteration, but missing data makes incidents expensive. Safety vs. throughput : tighter constraints reduce risk but may block progress on ambiguous tasks. Tool side effects : tools mutate state; retries can duplicate actions unless idempotent. Context limits : the loop must decide what to read, summarize, and omit. Solution Implement the smallest loop that can: 1. Load state (workspace snapshot + budgets + any session memory). 2. Produce exactly one next action using a typed schema (tool call or stop). 3. Execute the action with timeouts, error normalization, and side-effect capture. 4. Append a structured event to a trace. 5. Update state and budgets. 6. Stop only on explicit conditions (success, budget exhausted, or blocked). Keep loop policy simple. Push sophistication into tools (typed boundaries) and verification (tests/evals). Implementation sketch A minimal state machine: Inputs : goal, repo root, tool allowlist, budgets (steps/time/cost), memory handles. Per-step : Build a narrow context (selected files + short trace summary + constraints). Ask the model for NextAction . If NextAction.type == stop : return a result object (including verification evidence or explicit \u201cblocked\u201d). Otherwise: run the tool, capture outputs and observable side effects . Append the event to the trace. Update budgets; stop if exceeded. Conceptual NextAction schema: { \"type\": \"tool\" , \"toolName\": \"apply_patch\", \"args\": { \"input\": \"...\", \"explanation\": \"...\" }, \"expectedOutcome\": \"File X updated; markdown lint passes\", \"stopCondition\": \"After lint passes\", \"risk\": \"low\" } Practical kernel requirements: Enforce budgets outside the model. Validate tool arguments against schemas. Normalize errors (timeout vs. validation vs. runtime error). Record a trace event with enough data to reproduce the step. For side-effect tools, support an idempotency key (or a safe \u201cdry run\u201d mode) where possible. Concrete example Goal: \u201cAdd two glossary terms and ensure formatting.\u201d read_file(book/glossary.md) to learn current format. apply_patch to add the terms. Run a markdown check (or at minimum get_errors ) and record output. Stop only after the check is clean; otherwise attempt a bounded fix. An example trace event for step 2: step: 2 action: type: tool tool: apply_patch expected_outcome: \"Glossary updated with Acceptance criteria and Budget\" result: changed_files: - book/glossary.md tool_exit: success budgets: steps_remaining: 17 Failure modes Missing termination criteria : the loop continues despite completion; budgets are consumed. Ambiguous tool failures : errors are treated as partial success; corrupted state accumulates. Silent side effects : tools mutate state without reporting what changed; traces cannot explain outcomes. Non-idempotent retries : a retry duplicates actions (double-writes, duplicate tickets, repeated API calls). Context bloat : the loop pulls in too much content; constraints are ignored or diluted. Planning without execution : repeated re-planning without tool calls; no measurable progress. When not to use The task is deterministic and can be solved with a single script/command. Side effects are unacceptable without human approval (e.g., production mutations). You cannot capture traces or run verification; you will be unable to debug or detect drift.","title":"Minimal Agent Loop"},{"location":"book/patterns/minimal-agent-loop/#minimal-agent-loop","text":"","title":"Minimal Agent Loop"},{"location":"book/patterns/minimal-agent-loop/#context","text":"You need iterative work (planning + tool use + feedback) inside an engineering environment (repo, CI, tickets, docs). The primary risk is uncontrolled complexity: additional autonomy without corresponding observability and verification.","title":"Context"},{"location":"book/patterns/minimal-agent-loop/#problem","text":"How do you get useful autonomous work while keeping the control surface small enough to debug, test, and govern?","title":"Problem"},{"location":"book/patterns/minimal-agent-loop/#forces","text":"Capability vs. determinism : richer loops solve more tasks but increase variance across runs. Observability vs. speed : more logging and checks slow iteration, but missing data makes incidents expensive. Safety vs. throughput : tighter constraints reduce risk but may block progress on ambiguous tasks. Tool side effects : tools mutate state; retries can duplicate actions unless idempotent. Context limits : the loop must decide what to read, summarize, and omit.","title":"Forces"},{"location":"book/patterns/minimal-agent-loop/#solution","text":"Implement the smallest loop that can: 1. Load state (workspace snapshot + budgets + any session memory). 2. Produce exactly one next action using a typed schema (tool call or stop). 3. Execute the action with timeouts, error normalization, and side-effect capture. 4. Append a structured event to a trace. 5. Update state and budgets. 6. Stop only on explicit conditions (success, budget exhausted, or blocked). Keep loop policy simple. Push sophistication into tools (typed boundaries) and verification (tests/evals).","title":"Solution"},{"location":"book/patterns/minimal-agent-loop/#implementation-sketch","text":"A minimal state machine: Inputs : goal, repo root, tool allowlist, budgets (steps/time/cost), memory handles. Per-step : Build a narrow context (selected files + short trace summary + constraints). Ask the model for NextAction . If NextAction.type == stop : return a result object (including verification evidence or explicit \u201cblocked\u201d). Otherwise: run the tool, capture outputs and observable side effects . Append the event to the trace. Update budgets; stop if exceeded. Conceptual NextAction schema: { \"type\": \"tool\" , \"toolName\": \"apply_patch\", \"args\": { \"input\": \"...\", \"explanation\": \"...\" }, \"expectedOutcome\": \"File X updated; markdown lint passes\", \"stopCondition\": \"After lint passes\", \"risk\": \"low\" } Practical kernel requirements: Enforce budgets outside the model. Validate tool arguments against schemas. Normalize errors (timeout vs. validation vs. runtime error). Record a trace event with enough data to reproduce the step. For side-effect tools, support an idempotency key (or a safe \u201cdry run\u201d mode) where possible.","title":"Implementation sketch"},{"location":"book/patterns/minimal-agent-loop/#concrete-example","text":"Goal: \u201cAdd two glossary terms and ensure formatting.\u201d read_file(book/glossary.md) to learn current format. apply_patch to add the terms. Run a markdown check (or at minimum get_errors ) and record output. Stop only after the check is clean; otherwise attempt a bounded fix. An example trace event for step 2: step: 2 action: type: tool tool: apply_patch expected_outcome: \"Glossary updated with Acceptance criteria and Budget\" result: changed_files: - book/glossary.md tool_exit: success budgets: steps_remaining: 17","title":"Concrete example"},{"location":"book/patterns/minimal-agent-loop/#failure-modes","text":"Missing termination criteria : the loop continues despite completion; budgets are consumed. Ambiguous tool failures : errors are treated as partial success; corrupted state accumulates. Silent side effects : tools mutate state without reporting what changed; traces cannot explain outcomes. Non-idempotent retries : a retry duplicates actions (double-writes, duplicate tickets, repeated API calls). Context bloat : the loop pulls in too much content; constraints are ignored or diluted. Planning without execution : repeated re-planning without tool calls; no measurable progress.","title":"Failure modes"},{"location":"book/patterns/minimal-agent-loop/#when-not-to-use","text":"The task is deterministic and can be solved with a single script/command. Side effects are unacceptable without human approval (e.g., production mutations). You cannot capture traces or run verification; you will be unable to debug or detect drift.","title":"When not to use"},{"location":"book/patterns/self-verification-loop/","text":"Self-Verification Loop Context Model outputs often look plausible but can be wrong in subtle ways: incorrect assumptions about repo structure, stale APIs, missing edge cases, or incomplete updates across files. In engineering work, \u201csounds right\u201d is not an acceptance criterion. Problem How do you force the system to prove work against objective checks before it declares completion? Forces Verification cost must be lower than expected rework cost. Signal alignment : checks must reflect real acceptance criteria, not proxy metrics. Check gaming : if checks are narrow, the system may satisfy them while violating intent. Flakiness : verification tools can fail nondeterministically (network, timing, unstable tests). Side-effect boundaries : verification should not introduce additional risky mutations. Solution Make verification an explicit, mandatory phase with a stop gate: Define acceptance checks (tests, lint, build, schema validation, golden diffs) and/or a bounded human-review checklist. Require evidence in the trace: commands run, outputs captured, and artifacts produced. Gate completion : the system may only stop when checks pass, or when it produces a bounded \u201cblocked\u201d report with reproduction steps and the smallest viable next action. The key behavior change is procedural: \u201cdone\u201d becomes a claim that must be backed by artifacts. Implementation sketch Use a two-part contract per task: Work : changes made (patches/files) and the intended behavior. Verification : list of checks and their outcomes, including raw outputs or pointers to captured logs. Suggested verification record shape: verification: - check: \"unit tests\" command: \"npm test\" status: pass evidence: \"stdout excerpt or attached log\" - check: \"lint\" command: \"npm run lint\" status: fail evidence: \"...\" next_action: \"Fix unused import in src/cli.ts\" Practical gating logic: If checks pass: stop. If checks fail with actionable errors: attempt bounded repair (for example, up to 2 iterations). If failures are environmental/flaky: stop with a \u201cblocked\u201d report and clear reproduction steps. Concrete example Task: \u201cAdd a new CLI flag and update docs.\u201d Work: - Implement parsing for --format json . - Update README usage section. Verification: - Run unit tests that cover the new flag. - Run a help-text snapshot (golden file) test. - Run linter/formatter. Example evidence-oriented trace snippet: $ mycli --help ... includes \"--format\" ... $ npm test PASS cli.test.ts (12 tests) $ npm run lint 0 problems Failure modes Rubber-stamp verification : the system asserts checks passed without running them or without capturing outputs. Proxy mismatch : checks pass but requirements are unmet (acceptance criteria were incomplete or untested). Check gaming : tests are modified to match incorrect behavior; the suite becomes less meaningful. Flaky verification loop : intermittent failures trigger repeated repairs and wasted budget. Over-verification : too many slow checks push verification out of the critical path and encourage skipping. Unsafe verification : verification steps include side-effectful actions (publishing, migrations) without approvals. When not to use Low-impact drafts where verification cost dominates (early outlines, brainstorming, rough notes). Environments where checks cannot run (missing tooling or permissions) and no acceptable substitutes exist. Tasks where human judgment is the primary signal and objective checks are weak (copy tone, early design exploration).","title":"Self-Verification Loop"},{"location":"book/patterns/self-verification-loop/#self-verification-loop","text":"","title":"Self-Verification Loop"},{"location":"book/patterns/self-verification-loop/#context","text":"Model outputs often look plausible but can be wrong in subtle ways: incorrect assumptions about repo structure, stale APIs, missing edge cases, or incomplete updates across files. In engineering work, \u201csounds right\u201d is not an acceptance criterion.","title":"Context"},{"location":"book/patterns/self-verification-loop/#problem","text":"How do you force the system to prove work against objective checks before it declares completion?","title":"Problem"},{"location":"book/patterns/self-verification-loop/#forces","text":"Verification cost must be lower than expected rework cost. Signal alignment : checks must reflect real acceptance criteria, not proxy metrics. Check gaming : if checks are narrow, the system may satisfy them while violating intent. Flakiness : verification tools can fail nondeterministically (network, timing, unstable tests). Side-effect boundaries : verification should not introduce additional risky mutations.","title":"Forces"},{"location":"book/patterns/self-verification-loop/#solution","text":"Make verification an explicit, mandatory phase with a stop gate: Define acceptance checks (tests, lint, build, schema validation, golden diffs) and/or a bounded human-review checklist. Require evidence in the trace: commands run, outputs captured, and artifacts produced. Gate completion : the system may only stop when checks pass, or when it produces a bounded \u201cblocked\u201d report with reproduction steps and the smallest viable next action. The key behavior change is procedural: \u201cdone\u201d becomes a claim that must be backed by artifacts.","title":"Solution"},{"location":"book/patterns/self-verification-loop/#implementation-sketch","text":"Use a two-part contract per task: Work : changes made (patches/files) and the intended behavior. Verification : list of checks and their outcomes, including raw outputs or pointers to captured logs. Suggested verification record shape: verification: - check: \"unit tests\" command: \"npm test\" status: pass evidence: \"stdout excerpt or attached log\" - check: \"lint\" command: \"npm run lint\" status: fail evidence: \"...\" next_action: \"Fix unused import in src/cli.ts\" Practical gating logic: If checks pass: stop. If checks fail with actionable errors: attempt bounded repair (for example, up to 2 iterations). If failures are environmental/flaky: stop with a \u201cblocked\u201d report and clear reproduction steps.","title":"Implementation sketch"},{"location":"book/patterns/self-verification-loop/#concrete-example","text":"Task: \u201cAdd a new CLI flag and update docs.\u201d Work: - Implement parsing for --format json . - Update README usage section. Verification: - Run unit tests that cover the new flag. - Run a help-text snapshot (golden file) test. - Run linter/formatter. Example evidence-oriented trace snippet: $ mycli --help ... includes \"--format\" ... $ npm test PASS cli.test.ts (12 tests) $ npm run lint 0 problems","title":"Concrete example"},{"location":"book/patterns/self-verification-loop/#failure-modes","text":"Rubber-stamp verification : the system asserts checks passed without running them or without capturing outputs. Proxy mismatch : checks pass but requirements are unmet (acceptance criteria were incomplete or untested). Check gaming : tests are modified to match incorrect behavior; the suite becomes less meaningful. Flaky verification loop : intermittent failures trigger repeated repairs and wasted budget. Over-verification : too many slow checks push verification out of the critical path and encourage skipping. Unsafe verification : verification steps include side-effectful actions (publishing, migrations) without approvals.","title":"Failure modes"},{"location":"book/patterns/self-verification-loop/#when-not-to-use","text":"Low-impact drafts where verification cost dominates (early outlines, brainstorming, rough notes). Environments where checks cannot run (missing tooling or permissions) and no acceptable substitutes exist. Tasks where human judgment is the primary signal and objective checks are weak (copy tone, early design exploration).","title":"When not to use"}]}