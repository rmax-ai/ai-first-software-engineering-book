{
  "chapter_content": "# Chapter 03 \u2014 Autonomous Kernels\n\n## Thesis\nAn autonomous kernel is a minimal, well-specified control loop that executes bounded work: plan, apply tool actions, verify, and stop. Its constraints (budgets, permissions, evaluation gates) define a safety envelope that makes outcomes inspectable and repeatable.\n\nDefinitions:\n- **Autonomous kernel**: a control loop with explicit limits and explicit exit criteria; it is not \u201cgeneral autonomy,\u201d long-horizon project management, or open-ended exploration.\n- **Budget**: a hard cap on resources (iterations, elapsed time, tool calls, diff size) that prevents runaway behavior and forces escalation when progress stalls.\n- **Evaluation gate**: a required check whose result must be recorded and must be satisfied (or explicitly waived with justification) before the kernel can declare success.\n\nHypothesis: small, well-governed autonomous kernels (tight loops with explicit budgets and evaluation gates) outperform broad autonomy in stability and debuggability.\n\n## Why This Matters\n- Most failures in agentic work are operational: runaway loops, untraceable edits, and unverifiable outcomes.\n- Kernels enable composability: multiple kernels can run with different permissions and evaluation profiles.\n- \u201cKernel-first\u201d design makes autonomy a system property, not a prompt trick.\n\n## System Breakdown\n- **Kernel loop**: intent \u2192 plan \u2192 act \u2192 verify \u2192 record trace \u2192 stop/iterate.\n- **Budgets**: max iterations, time, tool calls, diff size.\n- **Permissions**: read/write scopes, protected paths, allowed tools.\n- **Verification**: mandatory checks per action class (e.g., tests for code changes).\n- **Persistence**: ledger entries, trace logs, artifacts.\n\nTo make this operational, treat each loop step as a checkpoint with a \u201cmust record\u201d trace payload and a \u201cmust decide\u201d stop condition:\n- **Intent**: state the task class and success condition (e.g., \u201ctests pass,\u201d \u201cbuild passes,\u201d \u201crepro no longer fails\u201d).\n- **Plan**: enumerate the next 1\u20133 actions only (not the whole project), each tied to a verification gate and a budget slice.\n- **Act**: perform the minimal change that addresses the current hypothesis; avoid speculative edits that cannot be evaluated.\n- **Verify**: run the smallest evaluation that is credible for the task class. Use unit tests for logic changes and typecheck/build for dependency changes; use end-to-end only when required. Verification can fail in two ways: the check fails, or the check is too narrow to detect the real regression.\n- **Record trace**: persist commands executed, files touched, diff stats, and evaluation outputs (or hashes/pointers to them) so a human can replay or audit.\n- **Stop/iterate**: stop when the success condition is met, or when a budget is exhausted, or when verification indicates the current plan cannot succeed without broader permissions/scope.\n\nMermaid mapping of stages to controls and outputs:\n\n```mermaid\nflowchart LR\n  I[Intent] --> P[Plan] --> A[Act] --> V[Verify] --> R[Record trace] --> S{Stop / iterate}\n\n  B[(Budgets\\niterations/time/tool calls/diff size)] -. constrains .-> P\n  B -. constrains .-> A\n  B -. constrains .-> V\n  Perm[(Permissions\\nread/write scopes\\nprotected paths\\nallowed tools)] -. constrains .-> A\n  Gate[(Evaluation gates\\nby action class)] -. required .-> V\n  Persist[(Persistence\\nledger/trace logs/artifacts)] -. produced .-> R\n\n  V -->|pass| S\n  V -->|fail| P\n  S -->|iterate| P\n  S -->|stop| End[Exit with summary]\n```\n\nA compact \u201cmust capture\u201d checklist (minimum viable trace):\n| Loop stage | Budget signal | Permission signal | Verification signal | Persistence artifact |\n|---|---|---|---|---|\n| intent | remaining iterations/time | required read scope | success criteria defined | intent string + criteria |\n| plan | tool-call budget allocation | allowed tools list | planned gates named | plan steps + gate mapping |\n| act | diff size consumed | write scope used | N/A | patch/diff stats |\n| verify | time/tool calls consumed | execution permissions | gate results (pass/fail) | command + exit code + excerpt |\n| record trace | N/A | N/A | N/A | ledger entry + trace pointer |\n| stop/iterate | budget exhausted? | permission insufficient? | gates satisfied? | final summary + next action |\n\n## Concrete Example 1\nBug-fix kernel for a CLI tool.\n\n- Input:\n  - failing test case: `tests/test_parse.py::test_rejects_empty_input`\n  - reproduction step: `python -m mycli parse \"\"` returns exit code `0` but should return non-zero\n  - budgets: max 3 iterations, max 10 tool calls, max 40 lines changed\n  - permissions: read `src/`, write `src/parser.py`, run `pytest -k parse`\n\nMini-runbook (a single bounded kernel run):\n1. Localize failure (evidence-first)\n   - Action: run the smallest check that reproduces the failure.\n     - Command: `pytest -k rejects_empty_input`\n   - Record:\n     - failing assertion excerpt (placeholder): `E assert 0 == 2`\n     - environment notes: OS, Python version, CLI args\n   - Stop/iterate rule:\n     - If the failure does not reproduce, stop and return \u201ccannot reproduce\u201d trace (do not edit).\n\n2. Patch minimal surface (hypothesis-driven)\n   - Hypothesis: empty string is being treated as a valid token stream in `src/parser.py`.\n   - Action: make a minimal edit that rejects empty input at the boundary (not across unrelated call sites).\n   - Budget check:\n     - ensure diff size stays within 40 lines and touches only `src/parser.py` (or a single adjacent file if necessary).\n   - Record:\n     - files touched: `src/parser.py`\n     - diff stats: `+6 -1` (placeholder)\n\n3. Run verification gate (tight but credible)\n   - Gate 1: rerun the failing test.\n     - Command: `pytest -k rejects_empty_input`\n   - Gate 2 (cheap regression check): run related unit tests only.\n     - Command: `pytest -k parse`\n   - Verification risk handling:\n     - If Gate 1 passes but Gate 2 fails, treat as \u201cnot fixed\u201d (the patch likely broke a nearby invariant).\n\n4. Record trace (auditable, replayable)\n   - Persist a kernel trace with:\n     - budgets consumed: iterations used, tool calls used, diff size\n     - commands executed + exit codes\n     - final test summary line (placeholder): `2 passed, 0 failed`\n   - Write a ledger entry summarizing:\n     - what changed (one-sentence)\n     - why it changed (link to failing assertion)\n     - what verified it (gate list)\n\n5. Stop criteria (explicit)\n   - Stop success: Gate 1 and Gate 2 pass within budget.\n   - Stop failure: tool-call budget exhausted, diff budget exceeded, or verification indicates a broader refactor is required.\n   - Stop escalation output: include \u201cnext action for a human\u201d (e.g., \u201cneeds design change in tokenization; requires editing `src/lexer.py`, which is outside current write scope\u201d).\n\n## Concrete Example 2\nDependency upgrade kernel.\n\n- Input:\n  - target version: `libX 4.2.0 \u2192 4.3.0`\n  - constraints: Python `>=3.10`, cannot change public API, CI must stay green\n  - upgrade guide: notes a breaking rename `OldClient` \u2192 `Client`\n  - budgets: max 4 iterations, max 15 tool calls, max 120 lines changed\n  - permissions: write `pyproject.toml` and `src/`, run `python -m compileall` and `pytest`\n\nKernel steps with an explicit remediation branch:\n1. Update manifest (narrow scope)\n   - Action: bump version constraint in `pyproject.toml`.\n   - Record:\n     - old/new constraint strings\n     - diff stats for manifest only\n   - Stop/iterate rule:\n     - If the dependency resolver cannot produce a consistent lock, stop with resolver output (do not attempt ad-hoc pinning unless that is explicitly in scope).\n\n2. Run a fast build/type gate before full tests\n   - Gate A (fast): import/type/compile smoke check.\n     - Command: `python -m compileall src`\n   - Record:\n     - exit code\n     - compile summary line (placeholder): `Listing 'src'...` \u2026 `compileall: success` (or equivalent)\n   - Interpretation:\n     - If Gate A fails, this is often a missing symbol or incompatible API that will be faster to remediate than running the full suite.\n\n3. Remediation branch (compile errors vs failing tests)\n   - If **compile/import fails**:\n     - Localize: identify first error site (file + symbol).\n     - Patch: apply the minimal mechanical fix (e.g., rename `OldClient` to `Client`) in the smallest set of files.\n     - Verify: rerun Gate A only, then proceed.\n     - Budget guard:\n       - If more than 5 files are touched, stop and escalate (\u201crequires broader refactor\u201d).\n       - If the cumulative diff exceeds 120 lines changed, stop and escalate (\u201cexceeds change budget for this kernel\u201d).\n   - If **compile passes but tests fail**:\n     - Localize: run the single failing test file or test case.\n     - Patch: address behavioral change (e.g., new default timeout) with a targeted adjustment and a justification in the trace.\n     - Verify: rerun the failing tests, then run the full relevant suite.\n\n4. Run full verification gate (credibility gate)\n   - Gate B (full): run the test suite (or the project\u2019s standard verification command).\n     - Command: `pytest`\n   - Record:\n     - exit code\n     - test summary line (placeholder): `X passed, 0 failed` (or, on failure, `X passed, Y failed`)\n   - Verification risk handling:\n     - Treat a narrowed verification set as a failure mode unless the trace records why it is acceptable (e.g., \u201cno integration tests exist; unit suite is the highest available gate\u201d).\n\n5. Stop criteria and outputs\n   - Stop success: Gate A and Gate B pass within budget.\n   - Stop failure: repeated failures indicate the upgrade exceeds current permission/scope (e.g., requires API redesign), or budgets are exhausted.\n   - Required outputs on stop:\n     - change summary: files touched + primary reason\n     - verification summary: Gate A command + result and Gate B command + result, including summary lines\n     - rollback plan: \u201crevert manifest bump and lockfile\u201d (or equivalent) with the exact files to revert\n\n## Trade-offs\n- Smaller kernels reduce risk but may require orchestration for multi-step projects.\n  - Mitigation: use staged kernels (e.g., \u201cdiagnose-only\u201d kernel \u2192 \u201cpatch\u201d kernel \u2192 \u201crefactor\u201d kernel), each with separate budgets and permissions.\n- Strict permissions reduce blast radius but can prevent necessary refactors.\n  - Mitigation: use permission escalation as an explicit step with a justification and a widened verification gate (e.g., requiring a broader test suite when write scope expands).\n- Heavier tracing improves auditability but adds operational overhead.\n  - Mitigation: record a minimum viable trace by default (commands, diffs, gate results), and sample/expand traces only on failures or high-risk task classes.\n\n## Failure Modes\n- **Local minima**: kernel makes safe micro-edits without addressing root cause.\n- **Tool thrash**: too many actions with low information gain.\n- **False confidence**: passing a narrow eval set while violating higher-level requirements.\n\nDetection signals (tie these to budgets and evaluation gates, not intuition):\n- Local minima:\n  - repeated edits in the same small area with no change in verification outcome across iterations\n  - steadily increasing diff size without new evidence (no new failing test localized, no new reproduction)\n- Tool thrash:\n  - tool-call count rising while the plan does not change (same commands rerun without a new hypothesis)\n  - frequent context switches (many files touched) despite a small, bounded intent\n- False confidence:\n  - verification gates becoming narrower over time (\u201conly reran one test\u201d) without a recorded justification\n  - \u201cgreen\u201d on fast gates but repeated regressions reported elsewhere (signals the gate set is mis-specified for the task class)\n  - success declared without a trace artifact that includes gate results and the exact commands used\n\n## Research Directions\n- Kernel composition patterns (delegation, staged permissions, multi-kernel workflows).\n- Automatic stop-condition tuning based on task class.\n- Replayable kernels for deterministic debugging of agent behavior.\n",
  "chapter_hypothesis": "Small, well-governed \u201cautonomous kernels\u201d (tight loops with explicit budgets and evaluation gates) outperform broad autonomy in stability and debuggability.",
  "chapter_id": "03-autonomous-kernels",
  "previous_critic_feedback": {
    "clarity_score": 0.0,
    "decision": "refine",
    "drift_score": 1.0,
    "example_density": 0.0,
    "failure_modes_present": false,
    "structure_score": 0.0,
    "tradeoff_presence": false,
    "violations": [
      "MOCK: not a real evaluation"
    ]
  },
  "quality_metrics": {
    "clarity_score": 0.0,
    "drift_score": 0.005217650566487776,
    "example_density": 0.0,
    "failure_mode_presence": false,
    "structure_score": 0.0,
    "tradeoff_presence": false
  }
}
