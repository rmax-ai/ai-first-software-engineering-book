{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AI-First Software Engineering","text":"<p>Welcome to the public documentation for the AI-first software engineering book. This site publishes the current working drafts, glossaries, patterns, and operational governance that power the book\u2019s ongoing iteration loop.</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Preface \u2013 foundational motivation and thesis for the AI-first engineering approach.</li> <li>Chapters \u2013 structured research chapters covering paradigm shifts, harness engineering, autonomous kernels, memory systems, evaluation, governance, and production infrastructure.</li> <li>Patterns \u2013 reusable engineering patterns curated from the repository.</li> <li>Glossary \u2013 operational definitions and terminology used throughout the book.</li> </ul>"},{"location":"#publishing-workflow","title":"Publishing workflow","text":"<ol> <li>Chapters are drafted in <code>book/chapters/</code> with deterministic loops enforced by <code>state/kernel.py</code>.</li> <li>The MkDocs configuration references the book files directly so every build reflects the latest chapter revisions.</li> <li>Use <code>mkdocs build</code> to produce the static site or <code>mkdocs serve</code> for a preview server before deployment.</li> <li>Keep governance documents (<code>CONSTITUTION.md</code>, <code>AGENTS.md</code>) and evaluation rules (<code>evals/</code>) aligned with the book\u2019s stated principles.</li> </ol>"},{"location":"#quick-commands","title":"Quick commands","text":"<pre><code>mkdocs build          # Generate the static site\nmkdocs serve          # Start a local preview server\nmkdocs gh-deploy      # Publish to GitHub Pages (configure remote)\n</code></pre>"},{"location":"#project-tree","title":"Project tree","text":"<pre><code>mkdocs.yml                    # Site configuration for the book\ndocs/index.md                 # Site homepage (this file)\nbook/chapters/*.md           # Chapter drafts consumed by MkDocs navigation\nbook/patterns/               # Referenced pattern library\nbook/glossary.md             # Detailed glossary definitions\nCONSTITUTION.md, AGENTS.md    # Governance that shapes the book process\nevals/*.yaml                 # Evaluation contracts enforced via state/kernel.py\n</code></pre>"},{"location":"book/glossary/","title":"Glossary","text":"<p>This glossary defines terms as used in this book. Definitions are intentionally operational: they describe what a term does in a system.</p>"},{"location":"book/glossary/#acceptance-criteria","title":"Acceptance criteria","text":"<p>Explicit, testable conditions that define \u201cdone\u201d for a task.</p> <p>In practice, acceptance criteria should map to checks (tests, lint, build, schema validation, golden diffs) or to a bounded human-review checklist.</p>"},{"location":"book/glossary/#agent-loop","title":"Agent loop","text":"<p>A control loop that repeatedly: (1) observes state, (2) chooses a next action (often a tool call), (3) executes, (4) updates state, (5) decides whether to stop.</p> <p>Typical loop variables include a step budget, an allowlist of tools, and a termination condition.</p>"},{"location":"book/glossary/#allowlist","title":"Allowlist","text":"<p>A set of explicitly permitted actions or tools.</p> <p>Allowlists reduce accidental capability expansion: adding a new tool is a deliberate change that can be reviewed, tested, and governed.</p>"},{"location":"book/glossary/#budget","title":"Budget","text":"<p>A hard limit enforced by the kernel (or orchestration layer) on steps, time, tokens, or cost.</p> <p>Budgets turn \u201ctry until it works\u201d into a bounded process.</p>"},{"location":"book/glossary/#context-window","title":"Context window","text":"<p>The maximum amount of text (messages + tool outputs + retrieved memory) that a model can condition on at once.</p> <p>Design implication: systems must choose what to include, summarize, or omit.</p>"},{"location":"book/glossary/#determinism","title":"Determinism","text":"<p>The degree to which the system produces the same outcome given the same inputs.</p> <p>In agentic systems, determinism is usually approached through constraints, typed tools, and verification rather than assumed.</p>"},{"location":"book/glossary/#drift","title":"Drift","text":"<p>Unintended change in behavior over time relative to a baseline.</p> <p>Drift can be caused by model updates, prompt edits, tool/API changes, data changes, environment changes, or accumulating memory. Drift is detected by comparing traces or eval results across versions.</p>"},{"location":"book/glossary/#evidence","title":"Evidence","text":"<p>Artifacts that support a claim about system behavior.</p> <p>Examples: a test run output, a checksum of an applied patch, a trace segment showing tool inputs/outputs, a golden diff.</p>"},{"location":"book/glossary/#eval","title":"Eval","text":"<p>A repeatable test that measures whether a system meets a target behavior under defined inputs and constraints.</p> <p>Evals can be automated (unit tests, golden files, scripted scenarios) or human-scored, but they must be versioned and runnable.</p>"},{"location":"book/glossary/#failure-mode","title":"Failure mode","text":"<p>A specific way the system can fail, including its trigger and observable symptoms.</p> <p>Failure modes drive targeted mitigations (tool contracts, guardrails, tests) rather than general caution.</p>"},{"location":"book/glossary/#golden-file","title":"Golden file","text":"<p>A stored \u201cexpected output\u201d used for regression testing.</p> <p>Golden tests are useful for CLI help text, formatted outputs, and traces; they must be reviewed carefully because updating them can mask regressions.</p>"},{"location":"book/glossary/#governance","title":"Governance","text":"<p>Rules, controls, and escalation paths that constrain and audit agent behavior.</p> <p>Governance commonly includes: tool allowlists, budgets, approval gates, logging requirements, data handling policies, incident response, and rollback procedures.</p>"},{"location":"book/glossary/#harness","title":"Harness","text":"<p>The engineered environment around a model that makes behavior reliable and useful in a specific context.</p> <p>A harness typically includes: prompts, tool schemas, routing policies, memory strategy, eval suite, tracing/telemetry, and release discipline. Most production reliability lives in the harness rather than in the model.</p>"},{"location":"book/glossary/#idempotency","title":"Idempotency","text":"<p>A property of an operation where repeating it produces the same final state as running it once.</p> <p>Idempotency matters for retries: without it, transient failures can cause duplicated side effects.</p>"},{"location":"book/glossary/#kernel","title":"Kernel","text":"<p>The minimal execution substrate that runs the agent loop and mediates interaction with the outside world.</p> <p>A kernel is responsible for: step control, tool invocation, persistence boundaries, cancellation/timeouts, and trace logging. It should be small enough to audit.</p>"},{"location":"book/glossary/#memory","title":"Memory","text":"<p>Persisted or semi-persisted state used to condition future behavior.</p> <p>Memory may be transient (within a run), session-scoped, or long-lived. It may be explicit (structured records) or implicit (retrieval index). Memory is a mechanism for reintroducing prior information into context; it is not automatically correct.</p>"},{"location":"book/glossary/#non-determinism","title":"Non-determinism","text":"<p>Variation in outcomes across runs due to sampling, tool timing, nondeterministic tests, external APIs, or changing environments.</p> <p>Managing nondeterminism is a core engineering task in agentic systems.</p>"},{"location":"book/glossary/#policy","title":"Policy","text":"<p>An explicit rule set that constrains behavior.</p> <p>Policies can be encoded in prompts, tool routers, allowlists, budgets, and approval workflows. Effective policies are testable and observable.</p>"},{"location":"book/glossary/#prompt","title":"Prompt","text":"<p>The structured input (system/developer/user messages and other context) used to condition the model.</p> <p>Prompting is part of the harness; changes to prompts should be versioned and evaluated like code.</p>"},{"location":"book/glossary/#retrieval","title":"Retrieval","text":"<p>Selecting external information (documents, traces, code snippets) to include in context for a step.</p> <p>Retrieval must be treated as a hint: critical claims still require confirmation against source-of-truth artifacts.</p>"},{"location":"book/glossary/#rag-retrieval-augmented-generation","title":"RAG (retrieval-augmented generation)","text":"<p>A pattern where the system retrieves relevant documents and includes them in the prompt before generating output.</p> <p>In engineering systems, RAG commonly retrieves code, docs, tickets, and prior traces.</p>"},{"location":"book/glossary/#side-effect","title":"Side effect","text":"<p>Any operation that changes state outside the model\u2019s messages.</p> <p>Examples: writing files, calling APIs, creating tickets, merging PRs, or modifying databases.</p>"},{"location":"book/glossary/#tool-interface","title":"Tool interface","text":"<p>The contract between the agent and an external capability.</p> <p>A tool interface specifies: inputs, outputs, errors, side effects, idempotency, latency expectations, and permission scope. Good tool interfaces make failure explicit and reduce ambiguity.</p>"},{"location":"book/glossary/#trace","title":"Trace","text":"<p>A structured record of an agent run that is sufficient to reconstruct what happened and why.</p> <p>A trace typically includes: prompts/messages, tool calls and results, intermediate decisions, budgets, timing, and final outputs. Traces are used for debugging, eval attribution, and governance.</p>"},{"location":"book/glossary/#verification","title":"Verification","text":"<p>Objective checks that the system runs (or produces as artifacts) to validate that an output meets acceptance criteria.</p> <p>Verification is stronger than self-review: it produces evidence.</p>"},{"location":"book/preface/","title":"Preface","text":"<p>This book treats AI-first software engineering as an engineering discipline rather than a product feature.</p>"},{"location":"book/preface/#scope","title":"Scope","text":"<p>This book focuses on system design for AI-assisted and agentic development:</p> <ul> <li>Harness design: tool contracts, constraints, budgets, evaluation gates, and traces.</li> <li>Operational reliability: reproducibility, attribution, rollback, and incident response.</li> <li>Governance: permissions, protected surfaces, and enforcement via tooling/CI.</li> <li>Memory as an engineered subsystem: provenance, retention, correction, and drift control.</li> </ul> <p>This book does not attempt to:</p> <ul> <li>Train foundation models or discuss model internals beyond what is necessary to reason about system behavior.</li> <li>Provide a survey of all agent frameworks; patterns are described in terms of interfaces and invariants.</li> <li>Substitute evaluation with plausibility; \u201cdone\u201d requires evidence.</li> </ul>"},{"location":"book/preface/#key-distinction-model-vs-harness","title":"Key distinction: model vs harness","text":"<ul> <li>Model: the reasoning component that proposes plans and edits.</li> <li>Harness: the execution and control environment (tools, policies, evaluation, tracing, state).</li> </ul> <p>A recurring hypothesis in the chapters is that many reliability gains in practice are harness-induced: schema design, verification discipline, and traceability change outcomes even when the model is unchanged.</p>"},{"location":"book/preface/#what-the-repository-demonstrates","title":"What the repository demonstrates","text":"<p>The repository is structured to make book development itself a reproducible agent loop:</p> <ul> <li>Governance is defined in <code>CONSTITUTION.md</code> and <code>AGENTS.md</code>.</li> <li>Chapter quality, drift signals, and style guardrails are declared in <code>evals/</code>.</li> <li>Iteration state is recorded in <code>state/</code>.</li> </ul> <p>The intention is to make each chapter a testable unit: clear thesis, system breakdown, concrete examples, trade-offs, failure modes, and research directions.</p>"},{"location":"book/preface/#how-to-read","title":"How to read","text":"<ol> <li>Start with the chapter that matches your immediate constraint (evaluation, governance, infra).</li> <li>Use <code>book/glossary.md</code> to disambiguate terms.</li> <li>Treat pattern documents in <code>book/patterns/</code> as reusable design primitives.</li> </ol>"},{"location":"book/chapters/01-paradigm-shift/","title":"Chapter 01 \u2014 Paradigm Shift","text":""},{"location":"book/chapters/01-paradigm-shift/#thesis","title":"Thesis","text":"<p>AI-first software engineering is an architectural inversion: machine reasoning becomes a primary execution substrate, and the harness (tools, constraints, evaluation, traceability) becomes the primary design surface.</p> <p>The inversion is practical: reliability comes from constraints, evaluations, and traces that turn generated changes into a repeatable, testable loop.</p> <p>A concrete, testable implication: holding the model constant, improving the harness should predictably reduce iteration count, increase pass rates, and improve failure attribution.</p> <p>Operational definition: - Model capability changes when you swap models while holding tools, constraints, and evaluation constant. - Harness capability changes when you keep the model constant but alter tools, policies, evaluation gates, or trace capture.</p> <p>This chapter\u2019s claim is a hypothesis: many observed \u201ccapability\u201d gains in practice are attributable to harness engineering rather than model changes.</p>"},{"location":"book/chapters/01-paradigm-shift/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Without a clear boundary between model capability and harness capability, teams misattribute failures and waste effort.</li> <li>Reliability depends on reproducible loops (plan \u2192 act \u2192 verify) rather than isolated prompts.</li> <li>Production constraints (auditability, security, cost, regression control) require system design, not \u201cprompting.\u201d</li> </ul>"},{"location":"book/chapters/01-paradigm-shift/#system-breakdown","title":"System Breakdown","text":"<ul> <li>Actors: human governor, agent loop, tools/runtime, evaluation/CI.</li> <li>Artifacts: specs, plans, diffs, traces, eval results, decision records.</li> <li>Invariants (hypotheses to test):</li> <li>Every non-trivial change is traceable to a plan and verified by checks.</li> <li>The system can attribute regressions to a layer (prompt, tool, code, eval).</li> <li>Autonomy is gated by evaluations and budgets.</li> <li>Measurable signals (to separate model vs harness effects):</li> <li>Iterations-to-pass: number of propose\u2192verify cycles until all required checks pass.</li> <li>Time-to-green: wall-clock time from first attempt to passing evaluation gates.</li> <li>Attribution rate: fraction of failures with a clear primary cause (prompt/spec vs tool/runtime vs code vs eval).</li> <li>Attribution checklist (what evidence makes a failure \u201cbelong\u201d to a layer):</li> <li>Spec/prompt: requirement is ambiguous or contradictory; different reasonable interpretations change expected output; clarifying text resolves the failure without code changes.</li> <li>Tool/runtime: tool errors, timeouts, missing permissions, flaky environment, or nondeterministic command outputs; rerun under identical inputs yields different results.</li> <li>Code: deterministic failing tests or typechecks tied to a specific diff; reverting the diff restores the previous behavior.</li> <li>Eval/CI: mismatch between what is asserted and what is intended; tests are incorrect, overly strict, or missing a required case; fixing the test changes outcomes without changing product behavior.</li> </ul>"},{"location":"book/chapters/01-paradigm-shift/#concrete-example-1","title":"Concrete Example 1","text":"<p>Refactor a small library function using an agent loop. - Inputs: failing unit test + desired behavior specification (e.g., a short \u201cGiven/When/Then\u201d note checked into the repo). - Loop: propose patch \u2192 run tests \u2192 inspect diff \u2192 record trace (commands + outputs) \u2192 stop on pass. - Measured outputs:   - Iterations-to-pass and time-to-green.   - Diff size (files touched, lines changed) and whether changes are localized to the intended function.   - Failure attribution per iteration (spec/prompt vs tool/runtime vs code vs eval) using the checklist above. - Stop rule:   - Stop when the originally failing unit test passes, the full unit test suite passes, and the diff is constrained to the intended surface area.   - If the loop reaches a fixed budget (e.g., N iterations or T minutes) without progress, stop and escalate to a human with the trace and the smallest reproducible failing case.</p>"},{"location":"book/chapters/01-paradigm-shift/#concrete-example-2","title":"Concrete Example 2","text":"<p>Ship a minor API change in a production service. - Inputs: API contract + backward-compat constraints + staging environment + a defined rollout/rollback policy. - Loop: generate migration plan \u2192 implement \u2192 run contract tests \u2192 produce trace report (diff + commands + results) \u2192 human approve. - Measured outputs:   - Iterations-to-pass and time-to-green (from first migration-plan draft to all required checks passing in staging).   - Attribution rate per iteration using the checklist above (spec/prompt vs tool/runtime vs code vs eval).   - Backward-compat outcomes: number of contract-test failures introduced, and whether rollback was exercised successfully in staging. - Guardrails:   - Protected paths or modules that require explicit human review before edits (e.g., auth, billing, infra).   - Required checks (contract tests, integration tests, lint/typecheck, and a staging smoke test).   - Rollback plan defined up front (feature flag, config switch, or revert procedure) and verified in staging.   - Approval gate: no deploy until a human reviews the migration plan, the diff, and the evaluation results. - Stop rule:   - Stop when all required checks pass in staging, the migration plan is consistent with backward-compat constraints, and the trace report can explain every material change.   - If any guardrail is violated (protected file touched, required test skipped, rollback unclear), stop immediately and require human intervention.</p>"},{"location":"book/chapters/01-paradigm-shift/#trade-offs","title":"Trade-offs","text":"<ul> <li>Strong harness constraints reduce freedom (and sometimes speed) but increase reproducibility.</li> <li>More evaluation gates reduce regressions but add compute and latency.</li> <li>Trace-heavy workflows improve debugging but increase storage and privacy considerations.</li> </ul>"},{"location":"book/chapters/01-paradigm-shift/#failure-modes","title":"Failure Modes","text":"<ul> <li>Illusion of capability: improvements credited to the model when they come from better tooling/evals.</li> <li>Unbounded autonomy: loops run without budgets, causing tool thrash and unclear outcomes.</li> <li>Non-attributable failures: missing traces make regressions un-debuggable.</li> </ul> <p>Synthesis: treat machine reasoning as an execution substrate, and treat the harness as the primary lever for reliability. Track iterations-to-pass, time-to-green, and attribution rate to separate harness effects from model effects and to make failures actionable.</p>"},{"location":"book/chapters/01-paradigm-shift/#research-directions","title":"Research Directions","text":"<ul> <li>Metrics that separate model improvements from harness improvements.</li> <li>Minimal trace schema that supports attribution and replay.</li> <li>Formal definitions of autonomy envelopes and stop conditions.</li> </ul>"},{"location":"book/chapters/02-harness-engineering/","title":"Chapter 02 \u2014 Harness Engineering","text":""},{"location":"book/chapters/02-harness-engineering/#thesis","title":"Thesis","text":"<p>Harness engineering is the discipline of turning a general-purpose model into a predictable system by defining tool contracts, constraints, loop control, and evaluation gates.</p> <p>In this chapter, \u201cpredictable\u201d means three things. First, the same input context tends to produce the same classes of actions (repeatability). Second, the system\u2019s allowed changes are bounded by explicit constraints (boundedness). Third, outputs can be accepted or rejected by checks rather than judgment calls (verifiability).</p> <p>Hypothesis (falsifiable): for a fixed set of tasks and repositories, tightening the harness (schemas, budgets, gates, and recovery rules) reduces regression rate and rework more than swapping between models of similar capability. Prompts and task definitions are held constant.</p>"},{"location":"book/chapters/02-harness-engineering/#why-this-matters","title":"Why This Matters","text":"<ul> <li>The same model can behave reliably or unreliably depending on tool schemas, budgets, and verification.</li> <li>Teams can standardize harness practices even when models change.</li> <li>Production safety and auditability primarily live in the harness layer.</li> </ul>"},{"location":"book/chapters/02-harness-engineering/#system-breakdown","title":"System Breakdown","text":"<ul> <li>Control plane: prompts, policies, budgets, stop conditions.</li> <li>Tool plane: filesystem edits, build/test runners, linters, browsers, APIs.</li> <li>Evaluation plane: checks as gates; regression suites; quality rubrics.</li> <li>State plane: task ledger, traces, decisions, artifacts.</li> <li>Interfaces:</li> <li>Tool schemas and error contracts.</li> <li>Patch discipline (diff-only, small changes).</li> <li>Evaluation API (what constitutes pass/fail).</li> </ul> <p>A useful way to operationalize the planes is to name what each plane consumes, what it produces, and one metric you can track:</p> Plane Responsibilities Key artifacts (inputs/outputs) One measurable metric Control plane Decide what the agent is allowed to do and when to stop Inputs: task brief, policy, iteration/time budget. Outputs: chosen strategy, stop reason Iterations to first passing gate Tool plane Perform actions with bounded, typed interfaces Inputs: tool calls with validated args. Outputs: patches, command outputs, structured errors Patch locality (changed files/lines per task) Evaluation plane Decide if work is acceptable based on checks Inputs: build/test/lint results, rubrics. Outputs: pass/fail + evidence Gate pass rate (per iteration) State plane Record what happened and enable recovery Inputs: events, diffs, tool outputs. Outputs: trace, ledger, artifacts for replay Reproducibility rate (can rerun and get same pass/fail) <p>The \u201cminimal contract surface\u201d is the set of interfaces that must be stable for predictability. It includes tool schemas (including error codes), patch discipline, and evaluation semantics.</p>"},{"location":"book/chapters/02-harness-engineering/#concrete-example-1","title":"Concrete Example 1","text":"<p>Design a tool contract for \u201capply patch\u201d operations.</p> <p>A minimal schema sketch:</p> <ul> <li>Required fields:</li> <li><code>path</code>: absolute or repo-relative file path (must exist unless <code>create=true</code>)</li> <li><code>old_str</code>: exact original text to be replaced (must match uniquely)</li> <li><code>new_str</code>: replacement text</li> <li><code>context</code>: optional surrounding lines to disambiguate</li> <li>Constraints:</li> <li>Reject edits that change unrelated whitespace outside the <code>old_str</code> region.</li> <li>Reject edits that expand scope beyond the file (no implicit multi-file edits).</li> <li>Reject edits that exceed a size budget (e.g., max changed lines per call).</li> <li>Error contract (examples):</li> <li><code>NOT_FOUND</code>: <code>path</code> does not exist and <code>create</code> is false</li> <li><code>NON_UNIQUE_MATCH</code>: <code>old_str</code> matches multiple locations</li> <li><code>NO_MATCH</code>: <code>old_str</code> matches zero locations (stale context)</li> <li><code>BUDGET_EXCEEDED</code>: change size violates configured limits</li> <li><code>POLICY_VIOLATION</code>: edit touches forbidden paths or patterns</li> </ul> <p>Happy path flow: 1. Agent proposes a patch using a unique <code>old_str</code> block with minimal scope. 2. Harness validates: file exists, match is unique, diff stays within budgets. 3. Tool applies the patch and returns a structured result: changed line counts, a before/after snippet, and a stable identifier for the diff.</p> <p>Conflict recovery (when the patch fails with <code>NO_MATCH</code> or <code>NON_UNIQUE_MATCH</code>): 1. The harness returns the error code plus a short \u201cfresh context\u201d snippet around the closest match (or a list of candidate match ranges). 2. The agent re-reads the relevant portion of the file and regenerates a smaller, more specific <code>old_str</code> (or narrows by adding <code>context</code>). 3. If the second attempt fails, the harness forces a stop condition (\u201cneeds human review\u201d) rather than allowing a whole-file overwrite.</p> <p>Evaluation: measure whether the tool contract is improving outcomes with two task-level metrics: - Revert rate: fraction of tasks where the patch is reverted in the next N commits/PR updates. - Diff locality: median number of files touched and lines changed per task, with an alert threshold for outliers.</p>"},{"location":"book/chapters/02-harness-engineering/#concrete-example-2","title":"Concrete Example 2","text":"<p>Add an evaluation gate to an agent loop.</p> <p>A minimal loop in prose looks like: attempt \u2192 gate \u2192 log \u2192 decide. 1. Attempt: the agent makes the smallest change it believes will satisfy the task. 2. Gate: the harness runs required checks (e.g., unit tests + static checks). 3. Log: record the tool outputs, the diff identifier, and the gate result in the task ledger. 4. Decide:    - If gate passes: allow \u201cPR-ready\u201d output.    - If gate fails: allow another iteration only if budgets remain and the failure is actionable.    - If repeated failures occur: stop with a concrete failure summary and evidence.</p> <p>Pass/fail criteria should be explicit: - Pass = all configured checks return success (exit code 0) and no policy violations were triggered. - Fail = any check fails, any policy violation occurs, or budgets are exceeded.</p> <p>Fallback gate policy when tests are missing: - If unit tests are absent or non-runnable, the harness should not silently accept \u201clooks good.\u201d - Replace the unit-test requirement with a narrower, explicit fallback gate such as:   - typecheck/lint/build must pass, and   - a targeted command or script (documented in the repo) must run successfully, and   - the change must be limited by stricter patch budgets (smaller allowed diffs). - The harness should record which gate was used (<code>full_gate</code> vs <code>fallback_gate</code>) so reliability metrics stay comparable over time.</p>"},{"location":"book/chapters/02-harness-engineering/#trade-offs","title":"Trade-offs","text":"<ul> <li>Richer tool schemas reduce ambiguity but raise integration cost.</li> <li>Strict budgets prevent runaway loops but can truncate legitimate work.</li> <li>Strong gates improve safety but may block progress on tasks lacking tests.</li> </ul> <p>Decision checklist (recommended defaults and when to relax): - Tool schemas:   - Default: prefer typed, validated arguments plus explicit error codes for recovery.   - Relax when: prototyping a new tool where integration speed matters more than repeatability, but only in non-production contexts. - Patch budgets:   - Default: small, diff-only edits with per-call and per-task size limits.   - Relax when: performing mechanical, reviewable migrations (e.g., dependency rename) where change breadth is intentional and measurable. - Evaluation gates:   - Default: require passing tests and static checks (a minimal green CI run) before any final output.   - Relax when: the repo cannot run tests in the current environment; use an explicit fallback gate and tighten patch budgets rather than skipping evaluation. - Stop conditions:   - Default: stop on repeated failure modes (same check failing twice) with a structured report.   - Relax when: failures are due to flaky infrastructure and you can rerun deterministically (record rerun count as part of the trace).</p>"},{"location":"book/chapters/02-harness-engineering/#failure-modes","title":"Failure Modes","text":"<ul> <li>Schema underspecification: tools accept ambiguous inputs, producing inconsistent outcomes.</li> <li>How you notice: frequent <code>NO_MATCH</code>/<code>NON_UNIQUE_MATCH</code>-style failures, large diffs for small tasks, and high variance in outcomes across reruns.</li> <li>Harness fix: tighten required fields, add validation (uniqueness checks, size budgets), and return structured error codes plus fresh context to guide recovery.</li> <li>Over-permissive harness: agents change broad parts of the repo with weak verification.</li> <li>How you notice: many files touched per task, drift into unrelated directories, and regressions discovered after \u201ccompletion.\u201d</li> <li>Harness fix: enforce patch locality budgets, add path allowlists/denylists, and require passing gates before accepting final output.</li> <li>Gate bypass: humans accept outputs without running checks, breaking the feedback loop.</li> <li>How you notice: \u201cmerged without green checks,\u201d missing logs/evidence in the ledger, and recurring regressions that gates would have caught.</li> <li>Harness fix: make gates non-optional for PR-ready output (policy), require artifacts (logs, diff id, check results) attached to the task, and surface \u201cstop reason\u201d when evidence is missing.</li> </ul>"},{"location":"book/chapters/02-harness-engineering/#research-directions","title":"Research Directions","text":"<ul> <li>Harness quality metrics (iteration efficiency, regression rate, reproducibility).</li> <li>Open question: which small set of metrics best predicts long-term reliability across repos?</li> <li>Evaluation approach: track iterations to first passing gate, revert rate, and reproducibility rate using the plane metrics table, then compare distributions before/after harness changes.</li> <li>Tool error taxonomies that guide automated recovery.</li> <li>Open question: what error codes enable the highest \u201cself-repair\u201d rate without encouraging risky retries?</li> <li>Evaluation approach: measure recovery success rate per error code (e.g., <code>% resolved within 2 retries</code>) and the resulting diff locality, using structured tool returns.</li> <li>Portable harness templates across languages and repo types.</li> <li>Open question: what parts of the contract surface are truly portable (schemas, budgets, gates) versus language-specific?</li> <li>Evaluation approach: apply a template to multiple repos, then compare reproducibility rate and gate pass rate changes while holding model choice constant.</li> </ul>"},{"location":"book/chapters/03-autonomous-kernels/","title":"Chapter 03 \u2014 Autonomous Kernels","text":""},{"location":"book/chapters/03-autonomous-kernels/#thesis","title":"Thesis","text":"<p>An autonomous kernel is a minimal, well-specified control loop for bounded work: plan, apply tool actions, verify, and stop. It is designed for short-horizon execution with explicit budgets and explicit exit criteria, so its behavior is inspectable and repeatable. It is not a substitute for long-horizon project management or open-ended exploration.</p> <p>What this is (and is not):</p> <ul> <li>A kernel executes one bounded objective and stops; it does not \u201ckeep going\u201d to find adjacent work.</li> <li>A kernel operates inside a declared safety envelope (budgets + permissions + evaluation gates); it does not expand scope implicitly. Here, \u201csafety envelope\u201d means the explicit combination of budgets, permissions, and evaluation gates. It bounds what actions are allowed and what evidence is required to declare success.</li> <li>A kernel treats verification as mandatory (or explicitly waived with recorded justification); it does not declare success from intent or plausibility.</li> </ul> <p>Definitions:</p> <ul> <li>Autonomous kernel: a control loop with explicit limits and explicit exit criteria; it is not \u201cgeneral autonomy,\u201d long-horizon project management, or open-ended exploration.</li> <li>Budget: a hard cap on resources (iterations, elapsed time, tool calls, diff size) that prevents runaway behavior and forces escalation when progress stalls.</li> <li>Evaluation gate: a required check whose result must be recorded and must be satisfied (or explicitly waived with justification) before the kernel can declare success.</li> </ul> <p>Hypothesis: small, well-governed autonomous kernels (tight loops with explicit budgets and evaluation gates) outperform broad autonomy in stability and debuggability.</p>"},{"location":"book/chapters/03-autonomous-kernels/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Most failures in agentic work are operational: runaway loops, untraceable edits, and unverifiable outcomes.</li> <li>Kernels enable composability: multiple kernels can run with different permissions and evaluation profiles.</li> <li>\u201cKernel-first\u201d design makes autonomy a system property, not a prompt trick.</li> </ul>"},{"location":"book/chapters/03-autonomous-kernels/#system-breakdown","title":"System Breakdown","text":"<ul> <li>Kernel loop: intent \u2192 plan \u2192 act \u2192 verify \u2192 record trace \u2192 stop/iterate.</li> <li>Budgets: max iterations, time, tool calls, diff size.</li> <li>Permissions: read/write scopes, protected paths, allowed tools.</li> <li>Verification: mandatory checks per action class (e.g., tests for code changes).</li> <li>Persistence: ledger entries, trace logs, artifacts.</li> </ul> <p>Treat each loop step as a checkpoint with two requirements: (1) what must be recorded, and (2) what stop condition must be evaluated.</p> <ol> <li>Intent: state the task class and the success condition.</li> <li>Must record: intent string + success criteria (e.g., \u201crepro no longer fails\u201d).</li> <li> <p>Stop condition: if success criteria are ambiguous, stop and request clarification.</p> </li> <li> <p>Plan: enumerate the next 1\u20133 actions only, each tied to a gate and a budget slice.</p> </li> <li>Must record: planned steps + named gates + allocated budget slice.</li> <li> <p>Stop condition: if the plan requires tools/paths outside permissions, stop and request escalation.</p> </li> <li> <p>Act: perform the minimal change that tests the current hypothesis.</p> </li> <li>Must record: files touched + diff stats (and any migration steps taken).</li> <li> <p>Stop condition: if diff budget or file-touch budget is exceeded, stop and escalate.</p> </li> <li> <p>Verify: run the smallest evaluation that is credible for the task class.</p> </li> <li>Must record: exact command(s) + exit codes + summary lines (or pointers/hashes to full logs).</li> <li>Stop condition: if a gate fails, iterate; if no credible gate exists, stop and escalate.</li> <li> <p>Gate waivers: acceptable only when the trace records (a) why the gate is not runnable, (b) what alternative check was run, and (c) what risk remains. A waiver is not a \u201cpass\u201d; it is a recorded exception.</p> </li> <li> <p>Record trace: persist replayable evidence for what happened and why.</p> </li> <li>Must record: commands executed, outputs (or pointers), budgets consumed, permissions exercised.</li> <li> <p>Stop condition: if persistence fails (cannot write trace), stop; do not continue \u201cblind.\u201d</p> </li> <li> <p>Stop/iterate: decide based on evidence and remaining budget.</p> </li> <li>Must record: decision + rationale + the next action (either \u201cdone\u201d or \u201cwhat to try next\u201d).</li> <li>Stop condition: stop on success, or when a budget is exhausted, or when permissions/scope are insufficient.</li> </ol> <p>Mermaid mapping of stages to controls and outputs:</p> <pre><code>flowchart LR\n  I[Intent] --&gt; P[Plan] --&gt; A[Act] --&gt; V[Verify] --&gt; R[Record trace] --&gt; S{Stop / iterate}\n\n  B[(Budgets\\niterations/time/tool calls/diff size)] -. constrains .-&gt; P\n  B -. constrains .-&gt; A\n  B -. constrains .-&gt; V\n  Perm[(Permissions\\nread/write scopes\\nprotected paths\\nallowed tools)] -. constrains .-&gt; A\n  Gate[(Evaluation gates\\nby action class)] -. required .-&gt; V\n  Persist[(Persistence\\nledger/trace logs/artifacts)] -. produced .-&gt; R\n\n  V --&gt;|pass| S\n  V --&gt;|fail| P\n  S --&gt;|iterate| P\n  S --&gt;|stop| End[Exit with summary]\n</code></pre> <p>How to read this diagram:</p> <ul> <li>Solid arrows show the execution order; dotted arrows show constraints that shape what actions are allowed.</li> <li>Gates flow into Verify because \u201cdone\u201d is a verification outcome, not an intent.</li> <li>Persistence flows out of Record trace because the trace is an output, not a side effect.</li> </ul> <p>A compact \u201cmust capture\u201d checklist (minimum viable trace).</p> <p>This is the smallest set of fields that enables replay, audit, and debugging without relying on memory or \u201cit seemed fine.\u201d</p> Loop stage Budget signal Permission signal Verification signal Persistence artifact intent remaining iterations/time required read scope success criteria defined intent string + criteria plan tool-call budget allocation allowed tools list planned gates named plan steps + gate mapping act diff size consumed write scope used N/A patch/diff stats verify time/tool calls consumed execution permissions gate results (pass/fail) command + exit code + excerpt record trace N/A N/A N/A ledger entry + trace pointer stop/iterate budget exhausted? permission insufficient? gates satisfied? final summary + next action"},{"location":"book/chapters/03-autonomous-kernels/#concrete-example-1","title":"Concrete Example 1","text":"<p>Bug-fix kernel for a CLI tool.</p> <ul> <li> <p>Input:</p> </li> <li> <p>failing test case: <code>tests/test_parse.py::test_rejects_empty_input</code></p> </li> <li>reproduction step: <code>python -m mycli parse \"\"</code> returns exit code <code>0</code> but should return non-zero</li> <li>budgets: max 3 iterations, max 10 tool calls, max 40 lines changed</li> <li>permissions: read <code>src/</code>, write <code>src/parser.py</code>, run <code>pytest -k parse</code></li> </ul> <p>Mini-runbook (a single bounded kernel run):</p> <ol> <li>Localize failure (evidence-first)</li> <li>Action: reproduce with the smallest credible check.<ul> <li>Command: <code>pytest -k rejects_empty_input</code></li> </ul> </li> <li>Gate: the reproduction must fail in a controlled way (same assertion, same path).</li> <li>Record:<ul> <li>command + exit code: <code>pytest -k rejects_empty_input</code> (exit code <code>1</code>)</li> <li>failing excerpt (example):</li> <li><code>&gt;       assert parse(\"\") != 0</code></li> <li><code>E       AssertionError: assert 0 != 0</code></li> <li>test summary line (example): <code>1 failed, 42 deselected in 0.31s</code></li> </ul> </li> <li> <p>Stop rule: if the failure does not reproduce, stop and return a \u201ccannot reproduce\u201d trace (no edits).</p> </li> <li> <p>Patch minimal surface (hypothesis-driven)</p> </li> <li>Action: implement the smallest boundary check consistent with the hypothesis.<ul> <li>Hypothesis: empty string is being treated as a valid token stream in <code>src/parser.py</code>.</li> <li>Change: reject empty input at the parse entrypoint (avoid touching downstream call sites).</li> </ul> </li> <li>Gate: diff must remain within 40 lines and touch only <code>src/parser.py</code> (unless escalation is explicitly allowed).</li> <li>Record:<ul> <li>files touched: <code>src/parser.py</code></li> <li>diff stats (example): <code>src/parser.py | 7 ++++++-</code></li> </ul> </li> <li> <p>Stop rule: if a second file is required, stop and escalate (\u201cwrite scope too narrow\u201d).</p> </li> <li> <p>Run verification gate (tight but credible)</p> </li> <li>Action: rerun the failing test, then a small related slice.</li> <li>Gate 1 (target): <code>pytest -k rejects_empty_input</code><ul> <li>Expected summary (example): <code>1 passed, 42 deselected in 0.28s</code></li> </ul> </li> <li>Gate 2 (related): <code>pytest -k parse</code><ul> <li>Expected summary (example): <code>12 passed, 0 failed, 31 deselected in 1.07s</code></li> </ul> </li> <li>Record: both commands, exit codes (<code>0</code>), and the summary lines.</li> <li> <p>Stop rule: if Gate 1 passes but Gate 2 fails, treat as \u201cnot fixed\u201d and iterate (the patch likely broke a nearby invariant).</p> </li> <li> <p>Record trace (auditable, replayable)</p> </li> <li>Action: persist a kernel trace and a ledger entry.</li> <li>Record (minimum):<ul> <li>budgets consumed: <code>iterations=1/3</code>, <code>tool_calls=3/10</code>, <code>diff_lines=7/40</code></li> <li>commands executed + exit codes</li> <li>final test summary lines:</li> <li><code>12 passed, 0 failed, 31 deselected in 1.07s</code></li> </ul> </li> <li> <p>Stop rule: if trace persistence fails, stop; do not continue with further edits.</p> </li> <li> <p>Stop criteria (explicit)</p> </li> <li>Stop success: Gate 1 and Gate 2 pass within budget.</li> <li>Stop failure: tool-call budget exhausted, diff budget exceeded, or verification indicates a broader refactor is required.</li> <li>Stop escalation output: include the next action for a human (e.g., \u201ctokenization treats whitespace-only as empty; requires editing <code>src/lexer.py</code>, outside current write scope\u201d).</li> </ol>"},{"location":"book/chapters/03-autonomous-kernels/#concrete-example-2","title":"Concrete Example 2","text":"<p>Dependency upgrade kernel.</p> <ul> <li> <p>Input:</p> </li> <li> <p>target version: <code>libX 4.2.0 \u2192 4.3.0</code></p> </li> <li>constraints: Python <code>&gt;=3.10</code>, cannot change public API, CI must stay green</li> <li>upgrade guide: notes a breaking rename <code>OldClient</code> \u2192 <code>Client</code></li> <li>budgets: max 4 iterations, max 15 tool calls, max 120 lines changed</li> <li>permissions: write <code>pyproject.toml</code> and <code>src/</code>, run <code>python -m compileall</code> and <code>pytest</code></li> </ul> <p>Kernel steps with an explicit remediation branch:</p> <ol> <li>Update manifest (narrow scope)</li> <li>Action: bump version constraint in <code>pyproject.toml</code>.</li> <li>Record:<ul> <li>old/new constraint strings (example): <code>libX&gt;=4.2,&lt;4.3</code> \u2192 <code>libX&gt;=4.3,&lt;4.4</code></li> <li>diff stats for manifest only (example): <code>pyproject.toml | 1 +-</code></li> </ul> </li> <li> <p>Stop/iterate rule:</p> <ul> <li>If the dependency resolver cannot produce a consistent lock, stop with resolver output (do not attempt ad-hoc pinning unless that is explicitly in scope).</li> </ul> </li> <li> <p>Run a fast build/type gate before full tests</p> </li> <li>Gate A (fast): import/type/compile smoke check.<ul> <li>Command: <code>python -m compileall src</code></li> </ul> </li> <li>Record:<ul> <li>exit code</li> <li>compile summary lines (example):</li> <li><code>Listing 'src'...</code></li> <li><code>Compiling 'src/app.py'...</code></li> <li><code>compileall: success</code></li> </ul> </li> <li> <p>Interpretation:</p> <ul> <li>If Gate A fails, it is often a missing symbol or incompatible API; remediate before running the full suite.</li> </ul> </li> <li> <p>Remediation branch (compile errors vs failing tests)</p> </li> <li> <p>If compile/import fails:</p> <ul> <li>Localize: capture the first error site (file + symbol) from the compile output.</li> <li>Example line: <code>***   File \"src/integrations/libx.py\", line 12</code></li> <li>Example line: <code>ImportError: cannot import name 'OldClient' from 'libx'</code></li> <li>Patch: apply the minimal mechanical fix (e.g., rename <code>OldClient</code> to <code>Client</code>) in the smallest set of files.</li> <li>Verify: rerun Gate A only; proceed only when it returns exit code <code>0</code>.</li> <li>Budget guard:</li> <li>If more than 5 files are touched, stop and escalate (\u201crequires broader refactor\u201d).</li> <li>If cumulative diff exceeds 120 lines, stop and escalate (\u201cexceeds change budget for this kernel\u201d).</li> </ul> </li> <li> <p>If compile passes but tests fail:</p> <ul> <li>Localize: run the smallest failing unit (single file or single test) based on the first failure.</li> <li>Example: <code>pytest tests/test_libx_integration.py -q</code></li> <li>Example summary: <code>1 failed, 18 passed in 4.92s</code></li> <li>Patch: address the behavioral change with a targeted adjustment and record the reason (e.g., \u201clibX now defaults timeout=None; set explicit timeout=5.0 in wrapper\u201d).</li> <li>Verify: rerun the failing tests, then proceed to the full suite gate.</li> </ul> </li> <li> <p>Run full verification gate (credibility gate)</p> </li> <li>Gate B (full): run the test suite (or the project\u2019s standard verification command).<ul> <li>Command: <code>pytest</code></li> </ul> </li> <li>Record:<ul> <li>exit code</li> <li>test summary line (example): <code>219 passed, 0 failed in 38.41s</code></li> </ul> </li> <li> <p>Verification risk handling:</p> <ul> <li>A narrowed verification set is acceptable only as an explicit gate waiver: record why the full gate is unavailable, what alternative gate was run, and what risk remains. Do not label a waiver as \u201cgreen\u201d; label it as \u201cwaived.\u201d</li> </ul> </li> <li> <p>Stop criteria and outputs</p> </li> <li>Stop success: Gate A and Gate B pass within budget.</li> <li>Stop failure: repeated failures indicate the upgrade exceeds current permission/scope (e.g., requires API redesign), or budgets are exhausted.</li> <li>Required outputs on stop:<ul> <li>change summary: files touched + primary reason</li> <li>verification summary: Gate A command + result and Gate B command + result, including summary lines</li> <li>rollback plan: \u201crevert manifest bump and lockfile\u201d (or equivalent) with the exact files to revert</li> </ul> </li> </ol>"},{"location":"book/chapters/03-autonomous-kernels/#trade-offs","title":"Trade-offs","text":"<ul> <li> <p>Smaller kernels reduce risk but may require orchestration for multi-step projects.</p> </li> <li> <p>Mitigation: use staged kernels (e.g., \u201cdiagnose-only\u201d kernel \u2192 \u201cpatch\u201d kernel \u2192 \u201crefactor\u201d kernel), each with separate budgets and permissions.</p> </li> <li> <p>Strict permissions reduce blast radius but can prevent necessary refactors.</p> </li> <li> <p>Mitigation: use permission escalation as an explicit step with a justification and a widened verification gate (e.g., requiring a broader test suite when write scope expands).</p> </li> <li> <p>Heavier tracing improves auditability but adds operational overhead.</p> </li> <li> <p>Mitigation: record a minimum viable trace by default (commands, diffs, gate results), and sample/expand traces only on failures or high-risk task classes.</p> </li> </ul>"},{"location":"book/chapters/03-autonomous-kernels/#failure-modes","title":"Failure Modes","text":"<ul> <li>Local minima: kernel makes safe micro-edits without addressing root cause.</li> <li>Tool thrash: too many actions with low information gain.</li> <li>False confidence: passing a narrow eval set while violating higher-level requirements.</li> </ul> <p>Detection signals (tie these to budgets and evaluation gates, not intuition):</p> <ul> <li>Local minima:</li> <li>same verification failure across iterations with edits confined to the same small area</li> <li>diff size steadily increases without any new localized evidence (no new failing test isolated, no tighter repro)</li> <li> <p>iteration budget is consumed with no change in the selected gate set or its outcome</p> </li> <li> <p>Tool thrash:</p> </li> <li>tool-call count rises while the plan and hypothesis remain unchanged</li> <li>the same command is rerun without any intervening change that could affect its result</li> <li> <p>number of files touched increases despite a small, bounded intent</p> </li> <li> <p>False confidence:</p> </li> <li>gates become narrower over time without a recorded waiver justification and risk note</li> <li>fast gates pass, but the credibility gate (broader suite) is repeatedly deferred without an explicit waiver record</li> <li>success is declared without a trace artifact that includes exact gate commands, exit codes, and summary lines</li> </ul>"},{"location":"book/chapters/03-autonomous-kernels/#research-directions","title":"Research Directions","text":"<ul> <li>Kernel composition patterns (delegation, staged permissions, multi-kernel workflows).</li> <li>Automatic stop-condition tuning based on task class.</li> <li>Replayable kernels for deterministic debugging of agent behavior.</li> </ul>"},{"location":"book/chapters/04-memory-systems/","title":"Chapter 04 \u2014 Memory Systems","text":""},{"location":"book/chapters/04-memory-systems/#thesis","title":"Thesis","text":"<p>Memory is an engineered data store for work artifacts, not a raw log of prior messages. It must be structured, queryable, and governed (with provenance) to improve long-horizon work.</p> <ul> <li>Structured: stored as records with explicit fields (not free-form chat logs), so constraints, sources, and updates are representable.</li> <li>Queryable: retrievable by filters (task, file, timeframe, decision id), with ranking that prefers fresh, high-provenance entries.</li> <li>Governed: subject to retention/redaction/correction rules, so stale or wrong entries can be superseded and sensitive traces can be minimized.</li> </ul> <p>Definition of done for \u201cgood memory\u201d: - Structure: every entry has an id, timestamp, type, and source. - Query: a future agent can retrieve the right entry with a bounded filter (\u201cshow decisions for X\u201d / \u201cshow traces for regression Y\u201d). - Governance: entries can be corrected (superseded) and expire or be redacted under policy.</p> <p>Hypothesis: uncurated memory increases confidence without increasing correctness, by amplifying earlier mistakes.</p>"},{"location":"book/chapters/04-memory-systems/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Long projects exceed context windows; without memory, work becomes repetitive and inconsistent.</li> <li>Without provenance, persistent memory becomes a source of silent drift: older summaries can out-rank newer evidence.</li> <li>Production environments need data minimization and retention policies for stored traces and summaries.</li> <li>Those policies must connect to provenance (what the data came from) and correction (how wrong entries are handled).</li> </ul>"},{"location":"book/chapters/04-memory-systems/#system-breakdown","title":"System Breakdown","text":"<ul> <li>Memory classes:</li> <li>Episodic: traces of actions/tool I/O/diffs.</li> <li>Semantic: stable project facts and conventions.</li> <li>Decisions: recorded trade-offs and constraints.</li> <li>State: current plan, progress, open issues (kept memory-specific: pointers to the latest plan/progress, not a second project management system).</li> <li>Write policy: what gets stored, when, by whom, and with what validation.</li> <li>Read policy: retrieval filters, ranking, freshness, and provenance checks.</li> <li>Governance: retention, access control, redaction, and correction mechanisms.</li> </ul> <p>Minimum memory record schema (applies to all classes): - <code>id</code>: stable identifier (e.g., <code>dec-2026-02-22-authz-approach</code>, <code>trace-&lt;task&gt;-&lt;iter&gt;</code>). - <code>timestamp</code>: when recorded (and optionally <code>valid_through</code> for expiry/refresh). - <code>type</code>: episodic | semantic | decision | state. - <code>source</code>: where it came from (tool output, diff, doc link, human note) and a pointer (path/URL/commit hash). - <code>confidence</code>: coarse signal (e.g., low/medium/high) tied to source quality (tests passing, verified by review, etc.). - <code>supersedes</code>: optional list of prior ids this record replaces (supports correction and avoids \u201cmemory poisoning\u201d via repetition).</p>"},{"location":"book/chapters/04-memory-systems/#concrete-example-1","title":"Concrete Example 1","text":"<p>Decision memory for an architecture choice.</p> <p>Write event (what triggers storage): - After selecting an approach in a design discussion or PR, store a decision record as part of the change (or alongside it) before implementation diverges.</p> <p>Stored record (template; identifiers are illustrative): - <code>id</code>: <code>dec-2026-02-22-memory-store-backend</code> - <code>timestamp</code>: <code>2026-02-22T19:33Z</code> - <code>type</code>: decision - <code>statement</code>: \u201cStore project memory as append-only records with explicit superseding, not as an editable wiki page.\u201d - <code>options_considered</code>:   - \u201cEditable wiki page\u201d   - \u201cAppend-only records + supersedes field\u201d   - \u201cNo persistence; rely on context only\u201d - <code>chosen</code>: \u201cAppend-only records + supersedes field\u201d - <code>constraints</code>: \u201cMust support redaction; must record provenance pointers; must allow correction.\u201d - <code>rationale</code>: \u201cEditable pages hide drift; append-only preserves history and enables explicit correction.\u201d - <code>source</code>: \u201cdocs/decisions/dec-2026-02-22-memory-store-backend.md + commit 0123abcd\u201d - <code>confidence</code>: \u201cmedium (reviewed; not yet load-tested)\u201d - <code>supersedes</code>: <code>[]</code></p> <p>Enforcement rule (how future work must behave): - Any change that contradicts the decision must either (a) reference this <code>id</code> and explain why it still holds, or (b) create a new decision record with <code>supersedes: [\"dec-2026-02-22-memory-store-backend\"]</code>. This prevents silent divergence where repeated but incorrect summaries turn into \u201cfacts\u201d (see Memory poisoning in ## Failure Modes).</p>"},{"location":"book/chapters/04-memory-systems/#concrete-example-2","title":"Concrete Example 2","text":"<p>Trace-indexed memory for debugging.</p> <p>Trace-indexed memory stores enough episodic detail to support \u201cfind similar failure, verify with evidence\u201d workflows.</p> <p>Capture \u2192 index \u2192 retrieve \u2192 verify: 1. Capture: on each iteration, store tool outputs and diffs:    - test command + failing stack trace snippet    - files changed + diff hash    - environment notes that affect reproducibility (OS, dependency lockfile hash) 2. Index: attach keys:    - <code>task_id</code>, <code>iteration</code>, <code>files_touched</code>, <code>error_signature</code> (e.g., exception type + top frame), <code>timestamp</code>, <code>source</code> pointers. 3. Retrieve: when a regression occurs, query with bounded filters in plain language:    - \u201cShow traces where <code>error_signature</code> matches \u2018KeyError: CONFIG\u2019 and <code>files_touched</code> includes <code>settings.py</code> from the last 30 days.\u201d    - Rank by freshness, then by confidence (e.g., traces linked to a commit that later passed CI). 4. Verify (provenance check): before acting on a retrieved trace, confirm provenance and freshness per Read policy:    - Does the trace point to an exact tool output and commit hash?    - Is it superseded by a newer trace for the same signature?    - Do current tests reproduce the signature, or has the failure mode changed?</p> <p>Expected outcome (measurable): - For recurring <code>error_signature</code>s, reduce median iterations-to-fix from 6 to 3. - Keep the post-fix regression rate at or below 2% in the next 20 CI runs affecting the same files.</p> <p>This flow turns memory into a debugging aid rather than a replay of stale context: retrieval is constrained, and action is gated by a provenance check.</p>"},{"location":"book/chapters/04-memory-systems/#trade-offs","title":"Trade-offs","text":"<ul> <li>More memory improves continuity but increases risk of stale or incorrect retrieval.</li> <li>Strong provenance improves trust but adds overhead to writing and updating memory.</li> <li>Aggressive retention helps debugging but increases privacy and storage costs.</li> <li>Governance often trades cost of being wrong (acting on stale memory, causing regressions) against cost of being slow (extra steps to write, verify, and supersede records).</li> </ul>"},{"location":"book/chapters/04-memory-systems/#failure-modes","title":"Failure Modes","text":"<ul> <li>Stale retrieval dominance: old assumptions override new evidence. Detection/mitigation: include freshness signals in Read policy (time windows, \u201cprefer not-superseded\u201d), and require verification against current tests before applying a past fix.</li> <li>Summarization loss: key constraints disappear in compression. Detection/mitigation: enforce Write policy that decision/semantic records keep explicit constraint fields, and link summaries back to their <code>source</code> pointers so missing details are recoverable.</li> <li>Memory poisoning: incorrect conclusions become \u201cfacts\u201d through repetition. Detection/mitigation: require corrections via Governance using <code>supersedes</code> (no silent edits), and down-rank low-provenance entries in Read policy so repetition does not outweigh evidence.</li> </ul>"},{"location":"book/chapters/04-memory-systems/#research-directions","title":"Research Directions","text":"<ul> <li>Memory scoring with automated freshness and provenance signals.</li> <li>Mechanisms for correcting memory (retractions, superseding records).</li> <li>Evaluations for memory usefulness (measuring reduced iterations without increased regressions), e.g., for trace-indexed debugging: reduce median \u201citerations-to-fix\u201d for recurring <code>error_signature</code>s while holding the post-fix regression rate constant (or lower) across subsequent CI runs.</li> </ul>"},{"location":"book/chapters/05-evaluation-and-traces/","title":"Chapter 05 \u2014 Evaluation and Traces","text":""},{"location":"book/chapters/05-evaluation-and-traces/#thesis","title":"Thesis","text":"<p>Evaluation and traceability are the mechanisms that make AI-first engineering reproducible. Traces provide the evidence needed to attribute outcomes to the correct layer (model, tool, harness, or missing tests); evaluations turn that evidence into gates that prevent incorrect changes from being accepted.</p> <p>Hypothesis: without trace-first design, teams cannot reliably distinguish model errors (wrong edit suggestion) from tool errors (command failed), harness errors (applied the wrong diff or wrong working directory), or missing tests (a real regression that was not checked).</p>"},{"location":"book/chapters/05-evaluation-and-traces/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Reproducibility is a prerequisite for iterative improvement.</li> <li>Evaluation gates define the autonomy envelope and prevent silent regressions.</li> <li>Traces enable post-incident analysis and systematic harness refinement.</li> </ul>"},{"location":"book/chapters/05-evaluation-and-traces/#system-breakdown","title":"System Breakdown","text":"<ul> <li>Trace schema (minimum viable):</li> <li>task id</li> <li>plan (the intended steps and stop conditions)</li> <li>tool calls (tool name, arguments, start/end timestamps, exit status)</li> <li>tool outputs (stdout/stderr excerpts or pointers to stored artifacts)</li> <li>diffs (patches applied, file paths touched)</li> <li>evaluation results (which checks ran, pass/fail, key failure signatures)</li> <li>budgets (time, iteration count, token/cost limits where applicable)</li> <li>stop reason (completed, blocked by gate, permission denied, budget exceeded)</li> <li>redaction policy (what is removed or hashed: secrets, tokens, PII, proprietary paths)</li> <li>environment fingerprint (repo URL if applicable, commit SHA, branch, tool versions, OS/arch)</li> <li>retry/iteration counters (attempt number per step, total loop count)</li> <li>When used for incident review, the trace must support queries like:<ul> <li>\u201cshow all tasks that modified <code>pyproject.toml</code> and failed secret scanning\u201d</li> <li>\u201clist failures where <code>tests</code> failed but a patch was still applied\u201d</li> <li>\u201cgroup stop reasons by action class over the last N runs\u201d</li> </ul> </li> <li>Evaluation types:</li> <li>correctness: unit/integration/contract tests.</li> <li>safety: permission checks, protected paths, secret scanning.</li> <li>quality: lint, type checks, formatting, doc checks.</li> <li>performance: benchmarks, latency/cost budgets.</li> <li>Gating model: which evaluations are required for which action classes.</li> <li>Minimal gating matrix (example, stated as rules):<ul> <li>read-only (grep/view/list):</li> <li>safety: required permission check for the path(s) being accessed</li> <li>quality: skipped; record <code>skipped_reason: \"read_only_action\"</code></li> <li>correctness: skipped; record <code>skipped_reason: \"read_only_action\"</code></li> <li>performance: skipped; record <code>skipped_reason: \"read_only_action\"</code></li> <li>patch edit (apply diff to code/docs):</li> <li>safety: required protected-path check for all touched files; if any are protected, stop with <code>stop_reason: \"permission_denied\"</code></li> <li>quality: required if repo has lint/typecheck config; otherwise record <code>skipped_reason: \"no_config\"</code></li> <li>correctness: required targeted tests covering touched modules; if no mapping exists, select a test command deterministically:<ul> <li>if the plan declares <code>test_command</code>, run it</li> <li>else if the repo declares a default test command, run it (e.g., in <code>Makefile</code>, <code>package.json</code>, or a CI config) and record the source as <code>selection_reason</code></li> <li>else run one fixed fallback based on detected stack and record <code>selection_reason</code>:</li> <li>Python: <code>pytest -q</code></li> <li>Node: <code>npm test</code></li> <li>Go: <code>go test ./...</code></li> <li>else record <code>skipped_reason: \"no_test_runner_detected\"</code> and stop with <code>stop_reason: \"blocked_by_correctness_gate\"</code></li> </ul> </li> <li>performance: required only if the diff touches paths under <code>deploy/</code> or <code>prod/</code>, or if a <code>latency_budget_ms</code>/<code>cost_budget_usd</code> is present in the plan; otherwise record <code>skipped_reason: \"not_applicable\"</code></li> <li>dependency install (lockfile changes, package adds):</li> <li>safety: required secret scan of the diff and updated lockfile(s); required protected-path check</li> <li>quality: required if repo has lint/typecheck config; otherwise record <code>skipped_reason: \"no_config\"</code></li> <li>correctness: required test subset that imports the changed dependency; if unknown, select a test command using the same deterministic rule as patch edit and record <code>selection_reason</code></li> <li>performance: required only if the change affects runtime images or deploy manifests (e.g., <code>Dockerfile</code>, <code>deploy/**</code>); otherwise record <code>skipped_reason: \"not_applicable\"</code></li> <li>deploy/release (publish, migrate, prod config):</li> <li>safety: required explicit permission grant + secret scan; stop on any forbidden hit</li> <li>quality: required lint/typecheck if configured; otherwise record <code>skipped_reason: \"no_config\"</code></li> <li>correctness: required full suite or contract tests defined for release; record suite name and command</li> <li>performance: required if a budget is declared in the plan; stop if budget exceeded and record measured values</li> </ul> </li> </ul>"},{"location":"book/chapters/05-evaluation-and-traces/#concrete-example-1","title":"Concrete Example 1","text":"<p>Tracing a refactor.</p> <ul> <li>Record: each patch + test run + failure signature, but also the environment fingerprint and the exact tool invocations so a reviewer can replay the same sequence.</li> </ul> <p>Pseudo-trace excerpt (illustrative):</p> <ul> <li>task_id: <code>refactor-auth-2026-02-22-001</code></li> <li>environment:</li> <li>repo_sha: <code>a1b2c3d</code></li> <li>tool_versions: <code>{ \"pytest\": \"8.1.1\", \"python\": \"3.12.1\" }</code></li> <li>plan: \u201crename <code>AuthClient</code> \u2192 <code>CopilotClient</code>; update imports; run targeted tests; stop if tests fail.\u201d</li> <li>tool_calls:   1) <code>{ \"tool\": \"apply_patch\", \"files\": [\"src/auth/client.py\"], \"exit_status\": 0 }</code>   2) <code>{ \"tool\": \"bash\", \"cmd\": \"pytest -q tests/test_auth_client.py::test_retry\", \"exit_status\": 1, \"failure_signature\": \"ImportError: cannot import name 'AuthClient'\" }</code></li> <li>diffs:</li> <li><code>src/auth/client.py</code>: renamed symbol</li> <li><code>tests/test_auth_client.py</code>: unchanged</li> <li>evaluation_results:</li> <li>correctness: <code>pytest -q ...</code> \u2192 FAIL (ImportError)</li> <li>stop_reason: \u201cblocked by correctness gate\u201d</li> </ul> <p>Query step using structured fields (one possible workflow): - Query: <code>repo_sha == \"a1b2c3d\" AND failure_signature CONTAINS \"ImportError: cannot import name 'AuthClient'\" AND diffs.paths CONTAINS \"src/auth/client.py\"</code> - Result (summary):   - matching_tasks: 3   - last_success_task_id: <code>refactor-auth-2026-02-20-007</code>   - last_success_repo_sha: <code>a1b2c3d</code>   - common_missing_patch_pattern: diffs do not include any files under <code>src/auth/__init__.py</code> or <code>src/auth/*</code> besides <code>client.py</code></p> <p>Decision rule using the query result: - If <code>last_success_repo_sha</code> matches the current <code>repo_sha</code> and <code>common_missing_patch_pattern</code> shows only <code>client.py</code> was changed, then expand patch scope to import sites (e.g., update <code>src/auth/__init__.py</code> exports and any <code>from src.auth import AuthClient</code> usage) and rerun the same correctness command (<code>pytest -q tests/test_auth_client.py::test_retry</code>). - If <code>last_success_repo_sha</code> differs, first rebase/reset to the pinned <code>repo_sha</code> before applying the expanded patch; then rerun the same correctness command.</p> <p>Attribution using the trace: - Not a model vs tool ambiguity: the patch tool succeeded and the test runner executed; the failure signature is a stable ImportError. - Not a harness misapplication: matching_tasks share the same repo_sha and the diffs confirm the same single-file touch pattern. - Likely root cause: refactor incompleteness (imports/usages not updated outside <code>client.py</code>); the next action is to expand the diff scope and re-run the same correctness gate.</p>"},{"location":"book/chapters/05-evaluation-and-traces/#concrete-example-2","title":"Concrete Example 2","text":"<p>Drift detection for an agent loop.</p> <ul> <li>Maintain: a stable eval suite and a small set of \u201cgolden\u201d tasks.</li> <li>A \u201cgolden task\u201d should include:<ul> <li>fixed input prompt and any fixed attachments</li> <li>fixed repo state (commit SHA) or a pinned fixture repository</li> <li>expected outcome constraints (e.g., \u201ctouch only <code>src/foo.py</code>\u201d, \u201cno network\u201d, \u201cno new dependencies\u201d)</li> <li>expected evaluations (which checks must pass)</li> <li>budgets (max iterations, max wall time, max tool calls)</li> </ul> </li> <li>Detect: changes in iteration counts, regression rate, and stop reasons over time.</li> <li>Define a baseline as a pinned reference run-set, such as \u201cthe last green run-set on release tag <code>vX.Y</code>\u201d for the same golden task and the same repo_sha (or the same pinned fixture).</li> <li>Example drift signals with thresholds (current window vs baseline window):<ul> <li>iteration drift: median loop iterations in the most recent 50 runs increases by \u2265 30% compared to the baseline\u2019s 50-run median</li> <li>regression rate: correctness-gate failure rate in the most recent 50 runs increases by \u2265 5 percentage points compared to the baseline\u2019s 50-run rate</li> <li>stop-reason shift: in the most recent 50 runs, \u201cbudget exceeded\u201d becomes the most frequent stop_reason while the baseline window had \u201ccompleted\u201d as the most frequent stop_reason</li> </ul> </li> </ul> <p>The point is not to predict every future failure. The point is to detect that something changed (model, tool, harness, or repo) and to have enough trace evidence to localize the change.</p>"},{"location":"book/chapters/05-evaluation-and-traces/#trade-offs","title":"Trade-offs","text":"<ul> <li>More evaluation increases confidence but costs time and compute.</li> <li>Rich traces help debugging but create storage and privacy burdens.</li> <li>Overly rigid gates can block progress on codebases with weak test coverage.</li> </ul>"},{"location":"book/chapters/05-evaluation-and-traces/#failure-modes","title":"Failure Modes","text":"<ul> <li>Eval gaming: optimizing for metrics while harming real-world quality.</li> <li>Detect: compare \u201cpasses gates\u201d with downstream signals in traces (reverts, follow-up bugfix tasks, repeated failures on adjacent golden tasks); watch for large diffs with minimal test coverage.</li> <li>Respond: add adversarial or regression tests, strengthen gate definitions (e.g., require touched-module tests), and record coverage/selection rationale in the trace.</li> <li>Blind spots: evaluations do not cover critical behaviors.</li> <li>Detect: incidents where traces show \u201call gates passed\u201d but production-like behavior fails; repeated similar failures without corresponding eval signatures.</li> <li>Respond: add contract tests or invariants to the eval suite; introduce risk-based gating (e.g., stricter checks for auth/payment paths) and make the gating decision explicit in the trace.</li> <li>Un-actionable traces: logs exist but lack structure, making search and attribution hard.</li> <li>Detect: inability to answer basic questions (\u201cwhat changed?\u201d, \u201cwhat ran?\u201d, \u201cwhy did it stop?\u201d) without reading raw logs; missing environment fingerprint or missing diff records.</li> <li>Respond: enforce a minimal trace schema, store normalized fields for queries, and treat \u201cmissing trace fields\u201d as an evaluation failure for non-trivial action classes.</li> </ul>"},{"location":"book/chapters/05-evaluation-and-traces/#research-directions","title":"Research Directions","text":"<ul> <li>Standard trace formats for portability across tools and models.</li> <li>Risk-based gating (stricter checks for higher-risk diffs).</li> <li>Low-cost evaluations that correlate with production outcomes.</li> </ul>"},{"location":"book/chapters/06-agent-governance/","title":"Chapter 06 \u2014 Agent Governance","text":""},{"location":"book/chapters/06-agent-governance/#thesis","title":"Thesis","text":"<p>Governance defines the safe operating envelope for autonomy: permissions, budgets, review policies, auditability, and incident response.</p> <p>Hypothesis: autonomy without enforceable governance increases throughput in the short term but increases defect rate and operational risk over time.</p>"},{"location":"book/chapters/06-agent-governance/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Agents can operate faster than human review cycles; governance prevents unsafe acceleration.</li> <li>Production environments require audit trails and controlled access to sensitive operations.</li> <li>Governance makes behavior consistent across models and team members.</li> </ul>"},{"location":"book/chapters/06-agent-governance/#system-breakdown","title":"System Breakdown","text":"<ul> <li>Policy artifacts: constitution (principles), agent rules (operational constraints), CI policies (enforcement).</li> <li>Permissions: read/write scopes, protected files, tool allowlists.</li> <li>Budgets: time, iterations, tool calls, diff size, cost ceilings.</li> <li>Review: mandatory human checkpoints for specific risk classes.</li> <li>Audit: trace retention, searchable logs, change attribution.</li> </ul>"},{"location":"book/chapters/06-agent-governance/#concrete-example-1","title":"Concrete Example 1","text":"<p>Protected-path governance. - Rule: forbid edits to security-critical configs without explicit approval. - Enforcement: tool layer rejects patches; CI verifies policy compliance.</p>"},{"location":"book/chapters/06-agent-governance/#concrete-example-2","title":"Concrete Example 2","text":"<p>Incident response for a bad autonomous change. - Trigger: regression detected by eval gates post-merge. - Response: rollback + trace review + policy update + new regression test.</p>"},{"location":"book/chapters/06-agent-governance/#trade-offs","title":"Trade-offs","text":"<ul> <li>Strong governance reduces risk but can slow iteration.</li> <li>Overly strict permissions increase human workload via escalations.</li> <li>Excessive auditing can create privacy and compliance burdens.</li> </ul>"},{"location":"book/chapters/06-agent-governance/#failure-modes","title":"Failure Modes","text":"<ul> <li>Policy drift: rules become outdated and stop reflecting real risks.</li> <li>Shadow autonomy: humans bypass gates \u201cjust this once,\u201d breaking discipline.</li> <li>Governance without enforcement: documents exist but tools/CI do not enforce them.</li> </ul>"},{"location":"book/chapters/06-agent-governance/#research-directions","title":"Research Directions","text":"<ul> <li>Policy-as-code patterns for agent systems.</li> <li>Automated risk scoring for diffs to route reviews.</li> <li>Governance metrics: prevented incidents vs friction cost.</li> </ul>"},{"location":"book/chapters/07-production-ai-infrastructure/","title":"Chapter 07 \u2014 Production AI Infrastructure","text":""},{"location":"book/chapters/07-production-ai-infrastructure/#thesis","title":"Thesis","text":"<p>Production AI-first systems are distributed systems: they require orchestration, isolation, observability, caching, cost control, and reproducible environments.</p> <p>Hypothesis: operational reliability depends more on the tool/runtime plane (sandboxing, retries, replay, artifacts) than on the model prompt.</p>"},{"location":"book/chapters/07-production-ai-infrastructure/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Without isolation, tool execution becomes a security and reliability risk.</li> <li>Without observability, failures cannot be attributed or fixed systematically.</li> <li>Without cost controls, autonomy can become economically unstable.</li> </ul>"},{"location":"book/chapters/07-production-ai-infrastructure/#system-breakdown","title":"System Breakdown","text":"<ul> <li>Execution: sandboxes/containers, dependency pinning, deterministic runners.</li> <li>Tool services: test runners, build systems, browsers, repo APIs.</li> <li>Orchestration: queues, concurrency limits, backpressure.</li> <li>Observability: traces, metrics, logs; correlation ids.</li> <li>Artifacts: build outputs, diffs, evaluation reports, replay bundles.</li> <li>Security: secrets handling, network egress controls, least privilege.</li> </ul>"},{"location":"book/chapters/07-production-ai-infrastructure/#concrete-example-1","title":"Concrete Example 1","text":"<p>Sandboxed tool execution for code changes. - Run: tests and builds inside a controlled environment. - Store: artifacts and traces to enable replay. - Gate: promote changes only when evals pass.</p>"},{"location":"book/chapters/07-production-ai-infrastructure/#concrete-example-2","title":"Concrete Example 2","text":"<p>Cost-aware autonomy for a batch of maintenance tasks. - Budget: per-task token/cost ceilings. - Strategy: fail fast on low-signal tasks; escalate to human review when uncertain. - Measure: cost per successful task and regression rate.</p>"},{"location":"book/chapters/07-production-ai-infrastructure/#trade-offs","title":"Trade-offs","text":"<ul> <li>Isolation increases safety but adds operational complexity.</li> <li>Strong observability increases insight but raises data retention requirements.</li> <li>Caching and replay improve speed but can mask nondeterminism if misused.</li> </ul>"},{"location":"book/chapters/07-production-ai-infrastructure/#failure-modes","title":"Failure Modes","text":"<ul> <li>Non-reproducible runs: environment drift makes traces hard to replay.</li> <li>Leaky permissions: tool plane has broader access than intended.</li> <li>Noisy observability: too much unstructured logging reduces signal.</li> </ul>"},{"location":"book/chapters/07-production-ai-infrastructure/#research-directions","title":"Research Directions","text":"<ul> <li>Standardized replay bundles for agent runs.</li> <li>Cost/performance models that predict optimal evaluation depth.</li> <li>Secure-by-default tool runtime primitives for autonomy.</li> </ul>"},{"location":"book/chapters/99-future-directions/","title":"Chapter 99 \u2014 Future Directions","text":""},{"location":"book/chapters/99-future-directions/#thesis","title":"Thesis","text":"<p>The frontier is system interfaces and verification: stronger tool contracts, better evaluations, structured memory, and governance primitives that scale across teams and models.</p> <p>Hypothesis: as autonomy scales, the limiting factor becomes organizational and infrastructural coupling, not raw inference capability.</p>"},{"location":"book/chapters/99-future-directions/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Teams will operate heterogeneous models and tools; interoperability becomes a reliability constraint.</li> <li>Long-horizon autonomy introduces new failure classes (compounded assumptions, policy drift, supply-chain issues).</li> <li>Without standards, every team reinvents trace formats, eval suites, and governance mechanisms.</li> </ul>"},{"location":"book/chapters/99-future-directions/#system-breakdown","title":"System Breakdown","text":"<ul> <li>Interoperability: shared trace formats, tool schemas, evaluation definitions.</li> <li>Verification: stronger correctness checks, property-based testing, contract enforcement.</li> <li>Governance at scale: org-level policies, audit workflows, incident response.</li> <li>Ecosystem risks: prompt/tool supply chain, dependency security, model updates.</li> </ul>"},{"location":"book/chapters/99-future-directions/#concrete-example-1","title":"Concrete Example 1","text":"<p>Cross-model portability experiment. - Run: the same harness + eval suite against multiple models. - Compare: outcomes, iteration profiles, and failure signatures. - Goal: isolate what is harness-dependent vs model-dependent.</p>"},{"location":"book/chapters/99-future-directions/#concrete-example-2","title":"Concrete Example 2","text":"<p>Standardized trace interchange. - Export: traces from one agent runtime. - Replay/analyze: in a different tool. - Goal: enable independent auditing and regression analysis.</p>"},{"location":"book/chapters/99-future-directions/#trade-offs","title":"Trade-offs","text":"<ul> <li>Standardization improves portability but can slow experimentation.</li> <li>Strong verification increases confidence but can increase compute and engineering effort.</li> <li>More governance improves safety but can reduce developer autonomy.</li> </ul>"},{"location":"book/chapters/99-future-directions/#failure-modes","title":"Failure Modes","text":"<ul> <li>Lock-in: traces and tools become proprietary and non-portable.</li> <li>False comparability: metrics appear comparable across systems but differ in hidden ways.</li> <li>Scale amplification: small policy errors cause large, repeated failures.</li> </ul>"},{"location":"book/chapters/99-future-directions/#research-directions","title":"Research Directions","text":"<ul> <li>Formal methods adapted to agent loops (bounded proofs, verified tool contracts).</li> <li>Benchmarks for reproducible autonomy (replay success, attribution accuracy).</li> <li>Org-scale governance patterns and \u201cpolicy drift\u201d detection.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/","title":"Harness Design Patterns","text":""},{"location":"book/patterns/harness-design-patterns/#context","title":"Context","text":"<p>A model alone is a general-purpose component. Production behavior is shaped by the harness: prompts, tools, memory, budgets, verification, and traceability.</p>"},{"location":"book/patterns/harness-design-patterns/#problem","title":"Problem","text":"<p>How do you design a harness that is reliable, debuggable, and governable without building a fragile tangle of special cases?</p>"},{"location":"book/patterns/harness-design-patterns/#forces","title":"Forces","text":"<ul> <li>Constraints vs. coverage: adding constraints improves safety but can reduce task coverage.</li> <li>Tools vs. surface area: more tools increase capability but enlarge the debug and governance surface.</li> <li>Memory vs. drift/privacy: continuity improves with memory, but so do drift and data handling risks.</li> <li>Automation vs. blast radius: stronger automation improves throughput but increases the impact of failures.</li> <li>Observability vs. cost: better tracing and verification cost time and compute.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#solution","title":"Solution","text":"<p>Use a small set of recurring harness patterns that are easy to audit and compose.</p>"},{"location":"book/patterns/harness-design-patterns/#pattern-1-typed-tool-boundary","title":"Pattern 1: Typed Tool Boundary","text":"<ul> <li>Idea: tools are the only way to cause side effects, and each tool has a typed schema with explicit errors.</li> <li>Why it works: reduces ambiguity and makes traces auditable.</li> <li>Example: <code>create_file(path, content)</code> returns <code>created | updated | no_op</code>, plus a checksum.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#pattern-2-budgeted-control-stepstimecost","title":"Pattern 2: Budgeted Control (Steps/Time/Cost)","text":"<ul> <li>Idea: the kernel enforces budgets; the model cannot override them.</li> <li>Why it works: turns open-ended iteration into a bounded process.</li> <li>Example: max 20 steps or 5 minutes; on exhaustion, stop with a partial report.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#pattern-3-evidence-first-completion","title":"Pattern 3: Evidence-First Completion","text":"<ul> <li>Idea: \u201cdone\u201d requires verifiable evidence (tests run, diffs applied, outputs captured).</li> <li>Why it works: prevents completion based on plausibility alone.</li> <li>Example: stop is rejected unless verification artifacts exist in the trace.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#pattern-4-narrow-context-construction","title":"Pattern 4: Narrow Context Construction","text":"<ul> <li>Idea: select only the files needed for the next action; summarize the rest.</li> <li>Why it works: reduces context bloat and keeps constraints salient.</li> <li>Example: open 2\u20134 files max, keep a rolling summary of prior steps.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#pattern-5-separation-of-duties-plan-vs-execute-vs-verify","title":"Pattern 5: Separation of Duties (Plan vs. Execute vs. Verify)","text":"<ul> <li>Idea: treat these as distinct phases with distinct constraints.</li> <li>Why it works: limits the ability to bypass controls (for example, editing policies during execution).</li> <li>Example: planning cannot call side-effect tools; verification cannot modify code.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#implementation-sketch","title":"Implementation sketch","text":"<p>Minimal harness components:</p> <ul> <li>Kernel: step loop, budgets, cancellation/timeouts, trace append.</li> <li>Tool router: allowlist + argument validation + consistent error model + idempotency support.</li> <li>Context builder: file selection + summarization policy + retrieval policy.</li> <li>Verifier: runs evals/tests and records outputs.</li> <li>Governance layer: approvals for high-risk tools, audit logs, retention/redaction rules.</li> </ul> <p>A small harness configuration can be expressed as a policy document (conceptual):</p> <pre><code>tools:\n  allowlist: [read_file, grep_search, apply_patch, get_errors, run_in_terminal]\nbudgets:\n  max_steps: 20\n  max_minutes: 5\nstop_gate:\n  require_verification: true\n  acceptable_outcomes: [\"verified\", \"blocked\"]\n</code></pre>"},{"location":"book/patterns/harness-design-patterns/#concrete-example","title":"Concrete example","text":"<p>Repo task agent that edits documentation:</p> <ul> <li>Tools: <code>read_file</code>, <code>grep_search</code>, <code>apply_patch</code>, <code>get_errors</code>.</li> <li>Budgets: 15 steps, 3 minutes.</li> <li>Stop gate: markdown checks (or at minimum a syntax/lint pass) must be clean, or the run stops as \u201cblocked\u201d with reproduction steps.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#failure-modes","title":"Failure modes","text":"<ul> <li>Tool sprawl: too many overlapping tools; selection becomes inconsistent and hard to audit.</li> <li>Hidden side effects: tools mutate state without reporting it; traces become misleading.</li> <li>Context bloat: prompts include too much content; constraints and acceptance criteria are diluted.</li> <li>Policy bypass: weak allowlists or validation enable unintended actions.</li> <li>Unmeasured changes: no evals/verification; regressions ship silently.</li> <li>Over-coupling: harness depends on brittle prompt wording instead of enforceable kernel/router constraints.</li> </ul>"},{"location":"book/patterns/harness-design-patterns/#when-not-to-use","title":"When not to use","text":"<ul> <li>Single-purpose automation where a deterministic script is simpler.</li> <li>One-off exploratory work where harness engineering overhead dominates.</li> <li>Systems without ownership/ops capacity to maintain tools, budgets, verification, and incident response.</li> </ul>"},{"location":"book/patterns/memory-architectures/","title":"Memory Architectures","text":""},{"location":"book/patterns/memory-architectures/#context","title":"Context","text":"<p>Agents are limited by context window, variability across runs, and the need to operate over long-lived projects. Memory mechanisms can improve continuity, but they also introduce drift, privacy risk, and debugging complexity.</p>"},{"location":"book/patterns/memory-architectures/#problem","title":"Problem","text":"<p>How do you add memory so the system remains reproducible and governable?</p>"},{"location":"book/patterns/memory-architectures/#forces","title":"Forces","text":"<ul> <li>Recall vs. precision: retrieving more increases coverage but adds noise.</li> <li>Freshness vs. stability: updating memory improves relevance but can rewrite history.</li> <li>Privacy vs. utility: storing more can leak sensitive data and expand retention obligations.</li> <li>Debuggability: implicit retrieval is harder to reason about than explicit records.</li> <li>Versioning: memory must evolve with code; unversioned memory becomes a hidden dependency.</li> </ul>"},{"location":"book/patterns/memory-architectures/#solution","title":"Solution","text":"<p>Prefer layered, explicit memory with clear scopes, schemas, and write rules.</p>"},{"location":"book/patterns/memory-architectures/#layer-1-run-local-working-memory-scratch","title":"Layer 1: Run-local working memory (scratch)","text":"<ul> <li>What: transient notes, intermediate calculations, short summaries.</li> <li>Scope: one run.</li> <li>Write rule: always safe to overwrite; never treated as durable truth.</li> <li>Example: \u201cFiles touched: A, B. Hypothesis: failing test caused by null handling.\u201d</li> </ul>"},{"location":"book/patterns/memory-architectures/#layer-2-session-memory-task-state","title":"Layer 2: Session memory (task state)","text":"<ul> <li>What: structured state for a multi-step task (checklists, open questions, next steps).</li> <li>Scope: until task completion.</li> <li>Write rule: update on each step; clear on task close.</li> <li>Example: a JSON task record containing acceptance criteria and verification status.</li> </ul>"},{"location":"book/patterns/memory-architectures/#layer-3-project-memory-durable-facts","title":"Layer 3: Project memory (durable facts)","text":"<ul> <li>What: stable, reviewable records: architecture decisions, interface contracts, runbooks.</li> <li>Scope: long-lived.</li> <li>Write rule: write only after verification passes and with a source-of-truth reference.</li> <li>Example: ADR-style entries with links to code and traces.</li> </ul>"},{"location":"book/patterns/memory-architectures/#layer-4-retrieval-index-searchable-corpus","title":"Layer 4: Retrieval index (searchable corpus)","text":"<ul> <li>What: embeddings or keyword index over docs/issues/traces.</li> <li>Scope: long-lived, but treated as derived data.</li> <li>Write rule: rebuildable; never the only place a critical fact exists.</li> <li>Example: \u201cretrieve top 5 related incidents\u201d feeding short excerpts into context.</li> </ul>"},{"location":"book/patterns/memory-architectures/#implementation-sketch","title":"Implementation sketch","text":"<p>Write rules that keep memory auditable and safe:</p> <ul> <li>Only write durable memory after verification passes.</li> <li>Store sources (file paths, URLs, trace IDs, commit hashes) with each memory item.</li> <li>Separate schemas for different types of memory: facts, decisions, preferences, open questions.</li> <li>Treat retrieval as a hint; require confirmation against sources for critical claims.</li> <li>Version memory alongside the system (or tie it to a release identifier).</li> <li>Support redaction and retention policies (delete by scope, delete by source, delete by time).</li> </ul> <p>Example durable-memory record schema (conceptual):</p> <pre><code>{\n  \"type\": \"decision\",\n  \"title\": \"Prefer golden tests for CLI help output\",\n  \"status\": \"accepted\",\n  \"sources\": [\"docs/cli.md\", \"trace:2026-02-18T10:14Z\"],\n  \"rationale\": \"Help output is user-facing and easy to regress\",\n  \"verified_by\": [\"npm test\", \"snapshot update reviewed\"],\n  \"created_at\": \"2026-02-18\"\n}\n</code></pre>"},{"location":"book/patterns/memory-architectures/#concrete-example","title":"Concrete example","text":"<p>Bugfix agent memory layout:</p> <ul> <li>Run-local: stack trace notes and hypotheses.</li> <li>Session: checklist of reproduction steps + test plan + files changed.</li> <li>Project: \u201cRoot cause and fix\u201d note linked to the failing test and the patch.</li> <li>Retrieval: search prior traces for similar failure signatures.</li> </ul>"},{"location":"book/patterns/memory-architectures/#failure-modes","title":"Failure modes","text":"<ul> <li>Stale memory: outdated assumptions persist after refactors; fixes target the wrong code.</li> <li>Memory poisoning: incorrect entries are stored as facts and bias future actions.</li> <li>Over-retrieval: too many irrelevant items drown the signal and dilute constraints.</li> <li>Silent mutation: memory is updated without review; history is effectively rewritten.</li> <li>Unversioned dependency: behavior depends on memory that is not tied to code/version.</li> <li>Privacy leakage: sensitive content is stored, retrieved, or logged without appropriate handling.</li> </ul>"},{"location":"book/patterns/memory-architectures/#when-not-to-use","title":"When not to use","text":"<ul> <li>Short-lived tasks where the context window is sufficient.</li> <li>High-sensitivity domains without a clear retention/redaction policy.</li> <li>Systems that require strict reproducibility but cannot version memory with code.</li> </ul>"},{"location":"book/patterns/minimal-agent-loop/","title":"Minimal Agent Loop","text":""},{"location":"book/patterns/minimal-agent-loop/#context","title":"Context","text":"<p>You need iterative work (planning + tool use + feedback) inside an engineering environment (repo, CI, tickets, docs). The primary risk is uncontrolled complexity: additional autonomy without corresponding observability and verification.</p>"},{"location":"book/patterns/minimal-agent-loop/#problem","title":"Problem","text":"<p>How do you get useful autonomous work while keeping the control surface small enough to debug, test, and govern?</p>"},{"location":"book/patterns/minimal-agent-loop/#forces","title":"Forces","text":"<ul> <li>Capability vs. determinism: richer loops solve more tasks but increase variance across runs.</li> <li>Observability vs. speed: more logging and checks slow iteration, but missing data makes incidents expensive.</li> <li>Safety vs. throughput: tighter constraints reduce risk but may block progress on ambiguous tasks.</li> <li>Tool side effects: tools mutate state; retries can duplicate actions unless idempotent.</li> <li>Context limits: the loop must decide what to read, summarize, and omit.</li> </ul>"},{"location":"book/patterns/minimal-agent-loop/#solution","title":"Solution","text":"<p>Implement the smallest loop that can: 1. Load state (workspace snapshot + budgets + any session memory). 2. Produce exactly one next action using a typed schema (tool call or stop). 3. Execute the action with timeouts, error normalization, and side-effect capture. 4. Append a structured event to a trace. 5. Update state and budgets. 6. Stop only on explicit conditions (success, budget exhausted, or blocked).</p> <p>Keep loop policy simple. Push sophistication into tools (typed boundaries) and verification (tests/evals).</p>"},{"location":"book/patterns/minimal-agent-loop/#implementation-sketch","title":"Implementation sketch","text":"<p>A minimal state machine:</p> <ul> <li>Inputs: goal, repo root, tool allowlist, budgets (steps/time/cost), memory handles.</li> <li>Per-step:</li> <li>Build a narrow context (selected files + short trace summary + constraints).</li> <li>Ask the model for <code>NextAction</code>.</li> <li>If <code>NextAction.type == stop</code>: return a result object (including verification evidence or explicit \u201cblocked\u201d).</li> <li>Otherwise: run the tool, capture outputs and observable side effects.</li> <li>Append the event to the trace.</li> <li>Update budgets; stop if exceeded.</li> </ul> <p>Conceptual <code>NextAction</code> schema:</p> <pre><code>{\n  \"type\": \"tool\" ,\n  \"toolName\": \"apply_patch\",\n  \"args\": { \"input\": \"...\", \"explanation\": \"...\" },\n  \"expectedOutcome\": \"File X updated; markdown lint passes\",\n  \"stopCondition\": \"After lint passes\",\n  \"risk\": \"low\"\n}\n</code></pre> <p>Practical kernel requirements:</p> <ul> <li>Enforce budgets outside the model.</li> <li>Validate tool arguments against schemas.</li> <li>Normalize errors (timeout vs. validation vs. runtime error).</li> <li>Record a trace event with enough data to reproduce the step.</li> <li>For side-effect tools, support an idempotency key (or a safe \u201cdry run\u201d mode) where possible.</li> </ul>"},{"location":"book/patterns/minimal-agent-loop/#concrete-example","title":"Concrete example","text":"<p>Goal: \u201cAdd two glossary terms and ensure formatting.\u201d</p> <ol> <li><code>read_file(book/glossary.md)</code> to learn current format.</li> <li><code>apply_patch</code> to add the terms.</li> <li>Run a markdown check (or at minimum <code>get_errors</code>) and record output.</li> <li>Stop only after the check is clean; otherwise attempt a bounded fix.</li> </ol> <p>An example trace event for step 2:</p> <pre><code>step: 2\naction:\n  type: tool\n  tool: apply_patch\n  expected_outcome: \"Glossary updated with Acceptance criteria and Budget\"\nresult:\n  changed_files:\n    - book/glossary.md\n  tool_exit: success\nbudgets:\n  steps_remaining: 17\n</code></pre>"},{"location":"book/patterns/minimal-agent-loop/#failure-modes","title":"Failure modes","text":"<ul> <li>Missing termination criteria: the loop continues despite completion; budgets are consumed.</li> <li>Ambiguous tool failures: errors are treated as partial success; corrupted state accumulates.</li> <li>Silent side effects: tools mutate state without reporting what changed; traces cannot explain outcomes.</li> <li>Non-idempotent retries: a retry duplicates actions (double-writes, duplicate tickets, repeated API calls).</li> <li>Context bloat: the loop pulls in too much content; constraints are ignored or diluted.</li> <li>Planning without execution: repeated re-planning without tool calls; no measurable progress.</li> </ul>"},{"location":"book/patterns/minimal-agent-loop/#when-not-to-use","title":"When not to use","text":"<ul> <li>The task is deterministic and can be solved with a single script/command.</li> <li>Side effects are unacceptable without human approval (e.g., production mutations).</li> <li>You cannot capture traces or run verification; you will be unable to debug or detect drift.</li> </ul>"},{"location":"book/patterns/self-verification-loop/","title":"Self-Verification Loop","text":""},{"location":"book/patterns/self-verification-loop/#context","title":"Context","text":"<p>Model outputs often look plausible but can be wrong in subtle ways: incorrect assumptions about repo structure, stale APIs, missing edge cases, or incomplete updates across files. In engineering work, \u201csounds right\u201d is not an acceptance criterion.</p>"},{"location":"book/patterns/self-verification-loop/#problem","title":"Problem","text":"<p>How do you force the system to prove work against objective checks before it declares completion?</p>"},{"location":"book/patterns/self-verification-loop/#forces","title":"Forces","text":"<ul> <li>Verification cost must be lower than expected rework cost.</li> <li>Signal alignment: checks must reflect real acceptance criteria, not proxy metrics.</li> <li>Check gaming: if checks are narrow, the system may satisfy them while violating intent.</li> <li>Flakiness: verification tools can fail nondeterministically (network, timing, unstable tests).</li> <li>Side-effect boundaries: verification should not introduce additional risky mutations.</li> </ul>"},{"location":"book/patterns/self-verification-loop/#solution","title":"Solution","text":"<p>Make verification an explicit, mandatory phase with a stop gate:</p> <ol> <li>Define acceptance checks (tests, lint, build, schema validation, golden diffs) and/or a bounded human-review checklist.</li> <li>Require evidence in the trace: commands run, outputs captured, and artifacts produced.</li> <li>Gate completion: the system may only stop when checks pass, or when it produces a bounded \u201cblocked\u201d report with reproduction steps and the smallest viable next action.</li> </ol> <p>The key behavior change is procedural: \u201cdone\u201d becomes a claim that must be backed by artifacts.</p>"},{"location":"book/patterns/self-verification-loop/#implementation-sketch","title":"Implementation sketch","text":"<p>Use a two-part contract per task:</p> <ul> <li>Work: changes made (patches/files) and the intended behavior.</li> <li>Verification: list of checks and their outcomes, including raw outputs or pointers to captured logs.</li> </ul> <p>Suggested verification record shape:</p> <pre><code>verification:\n  - check: \"unit tests\"\n    command: \"npm test\"\n    status: pass\n    evidence: \"stdout excerpt or attached log\"\n  - check: \"lint\"\n    command: \"npm run lint\"\n    status: fail\n    evidence: \"...\"\n    next_action: \"Fix unused import in src/cli.ts\"\n</code></pre> <p>Practical gating logic:</p> <ul> <li>If checks pass: stop.</li> <li>If checks fail with actionable errors: attempt bounded repair (for example, up to 2 iterations).</li> <li>If failures are environmental/flaky: stop with a \u201cblocked\u201d report and clear reproduction steps.</li> </ul>"},{"location":"book/patterns/self-verification-loop/#concrete-example","title":"Concrete example","text":"<p>Task: \u201cAdd a new CLI flag and update docs.\u201d</p> <p>Work: - Implement parsing for <code>--format json</code>. - Update <code>README</code> usage section.</p> <p>Verification: - Run unit tests that cover the new flag. - Run a help-text snapshot (golden file) test. - Run linter/formatter.</p> <p>Example evidence-oriented trace snippet:</p> <pre><code>$ mycli --help\n... includes \"--format\" ...\n\n$ npm test\nPASS cli.test.ts (12 tests)\n\n$ npm run lint\n0 problems\n</code></pre>"},{"location":"book/patterns/self-verification-loop/#failure-modes","title":"Failure modes","text":"<ul> <li>Rubber-stamp verification: the system asserts checks passed without running them or without capturing outputs.</li> <li>Proxy mismatch: checks pass but requirements are unmet (acceptance criteria were incomplete or untested).</li> <li>Check gaming: tests are modified to match incorrect behavior; the suite becomes less meaningful.</li> <li>Flaky verification loop: intermittent failures trigger repeated repairs and wasted budget.</li> <li>Over-verification: too many slow checks push verification out of the critical path and encourage skipping.</li> <li>Unsafe verification: verification steps include side-effectful actions (publishing, migrations) without approvals.</li> </ul>"},{"location":"book/patterns/self-verification-loop/#when-not-to-use","title":"When not to use","text":"<ul> <li>Low-impact drafts where verification cost dominates (early outlines, brainstorming, rough notes).</li> <li>Environments where checks cannot run (missing tooling or permissions) and no acceptable substitutes exist.</li> <li>Tasks where human judgment is the primary signal and objective checks are weak (copy tone, early design exploration).</li> </ul>"}]}